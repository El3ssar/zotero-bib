@article{abarbanelAnalysisObservedChaotic1993,
  title = {The Analysis of Observed Chaotic Data in Physical Systems},
  author = {Abarbanel, Henry D. I. and Brown, Reggie and Sidorowich, John J. and Tsimring, Lev Sh.},
  year = {1993},
  month = oct,
  journal = {Reviews of Modern Physics},
  volume = {65},
  number = {4},
  pages = {1331--1392},
  issn = {0034-6861, 1539-0756},
  doi = {10.1103/RevModPhys.65.1331},
  urldate = {2025-08-12},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/UCHDNHJV/Abarbanel et al. - 1993 - The analysis of observed chaotic data in physical systems.pdf}
}

@inproceedings{abasoloNonlinearAnalysisIntracranial2007,
  title = {Non-Linear {{Analysis}} of {{Intracranial Electroencephalogram Recordings}} with {{Approximate Entropy}} and {{Lempel-Ziv Complexity}} for {{Epileptic Seizure Detection}}},
  booktitle = {2007 29th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}}},
  author = {Abasolo, Daniel and James, Christopher J. and Hornero, Roberto},
  year = {2007},
  month = aug,
  pages = {1953--1956},
  publisher = {IEEE},
  address = {Lyon, France},
  issn = {1557-170X},
  doi = {10.1109/iembs.2007.4352700},
  urldate = {2025-07-15},
  abstract = {Epileptic seizures are generated by an abnormal synchronization of neurons unforeseeable for the patients. In this study we analyzed invasive electroencephalogram (EEG) recordings in patients suffering from medically intractable focal epilepsy with two non-linear methods, Approximate Entropy (ApEn) and Lempel-Ziv (LZ) complexity. ApEn and LZ complexity quantify the regularity and complexity of a time series, respectively, and are well suited to the analysis of nonstationary biomedical signals of short length. Our results show an increase in ApEn and LZ complexity values during seizures at the focal electrodes. These changes could also be seen at some extra focal electrodes. After the seizure ends, the values of both non-linear metrics return to values lower than those before the seizure. Moreover, we quantified the changes in LZ complexity, showing the complexity increase during the seizure and its notable decrease after its end. Our results suggest that these techniques are useful to detect changes due to epileptic seizures in the EEG.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/WUKFCQ2V/Abasolo et al. - 2007 - Non-linear Analysis of Intracranial Electroencephalogram Recordings with Approximate Entropy and Lem.pdf}
}

@article{abeHighlyintegrableAnalogueReservoir2024,
  title = {Highly-Integrable Analogue Reservoir Circuits Based on a Simple Cycle Architecture},
  author = {Abe, Yuki and Nakada, Kazuki and Hagiwara, Naruki and Suzuki, Eiji and Suda, Keita and Mochizuki, Shin-ichiro and Terasaki, Yukio and Sasaki, Tomoyuki and Asai, Tetsuya},
  year = {2024},
  month = may,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {10966},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-61880-z},
  urldate = {2025-08-12},
  abstract = {Abstract             Physical reservoir computing is a promising solution for accelerating artificial intelligence (AI) computations. Various physical systems that exhibit nonlinear and fading-memory properties have been proposed as physical reservoirs. Highly-integrable physical reservoirs, particularly for edge AI computing, has a strong demand. However, realizing a practical physical reservoir with high performance and integrability remains challenging. Herein, we present an analogue circuit reservoir with a simple cycle architecture suitable for complementary metal-oxide-semiconductor (CMOS) chip integration. In several benchmarks and demonstrations using synthetic and real-world data, our developed hardware prototype and its simulator exhibit a high prediction performance and sufficient memory capacity for practical applications, showing promise for future applications in highly integrated AI accelerators.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/D3H6CVAE/Abe et al. - 2024 - Highly-integrable analogue reservoir circuits based on a simple cycle architecture.pdf}
}

@article{aboyInterpretationLempelZivComplexity2006,
  title = {Interpretation of the {{Lempel-Ziv Complexity Measure}} in the {{Context}} of {{Biomedical Signal Analysis}}},
  author = {Aboy, M. and Hornero, R. and Abasolo, D. and Alvarez, D.},
  year = {2006},
  month = nov,
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {53},
  number = {11},
  pages = {2282--2288},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9294, 1558-2531},
  doi = {10.1109/tbme.2006.883696},
  urldate = {2025-07-15},
  abstract = {Lempel-Ziv complexity (LZ) and derived LZ algorithms have been extensively used to solve information theoretic problems such as coding and lossless data compression. In recent years, LZ has been widely used in biomedical applications to estimate the complexity of discrete-time signals. Despite its popularity as a complexity measure for biosignal analysis, the question of LZ interpretability and its relationship to other signal parameters and to other metrics has not been previously addressed. We have carried out an investigation aimed at gaining a better understanding of the LZ complexity itself, especially regarding its interpretability as a biomedical signal analysis technique. Our results indicate that LZ is particularly useful as a scalar metric to estimate the bandwidth of random processes and the harmonic variability in quasi-periodic signals.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/NMA64EF8/Aboy et al. - 2006 - Interpretation of the Lempel-Ziv Complexity Measure in the Context of Biomedical Signal Analysis.pdf}
}

@article{abramsChimeraStatesCoupled2004,
  title = {Chimera {{States}} for {{Coupled Oscillators}}},
  author = {Abrams, Daniel M. and Strogatz, Steven H.},
  year = {2004},
  month = oct,
  journal = {Physical Review Letters},
  volume = {93},
  number = {17},
  eprint = {nlin/0407045},
  pages = {174102},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.93.174102},
  urldate = {2025-07-28},
  abstract = {Arrays of identical oscillators can display a remarkable spatiotemporal pattern in which phase-locked oscillators coexist with drifting ones. Discovered two years ago, such "chimera states" are believed to be impossible for locally or globally coupled systems; they are peculiar to the intermediate case of nonlocal coupling. Here we present an exact solution for this state, for a ring of phase oscillators coupled by a cosine kernel. We show that the stable chimera state bifurcates from a spatially modulated drift state, and dies in a saddle-node bifurcation with an unstable chimera.},
  archiveprefix = {arXiv},
  keywords = {Nonlinear Sciences - Pattern Formation and Solitons},
  file = {/home/elessar/Zotero/storage/SZP8L85R/Abrams and Strogatz - 2004 - Chimera States for Coupled Oscillators.pdf;/home/elessar/Zotero/storage/2ZQ2VTJG/0407045.html}
}

@misc{aghamohammadiAmbiguitySimplicity2016,
  title = {The {{Ambiguity}} of {{Simplicity}}},
  author = {Aghamohammadi, Cina and Mahoney, John R. and Crutchfield, James P.},
  year = {2016},
  month = feb,
  number = {arXiv:1602.08646},
  eprint = {1602.08646},
  primaryclass = {quant-ph},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1602.08646},
  urldate = {2025-07-15},
  abstract = {A system's apparent simplicity depends on whether it is represented classically or quantally. This is not so surprising, as classical and quantum physics are descriptive frameworks built on different assumptions that capture, emphasize, and express different properties and mechanisms. What is surprising is that, as we demonstrate, simplicity is ambiguous: the relative simplicity between two systems can change sign when moving between classical and quantum descriptions. Thus, notions of absolute physical simplicity---minimal structure or memory---at best form a partial, not a total, order. This suggests that appeals to principles of physical simplicity, via Ockham's Razor or to the ``elegance'' of competing theories, may be fundamentally subjective, perhaps even beyond the purview of physics itself. It also raises challenging questions in model selection between classical and quantum descriptions. Fortunately, experiments are now beginning to probe measures of simplicity, creating the potential to directly test for ambiguity.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Condensed Matter - Statistical Mechanics,Mathematics - Information Theory,Quantum Physics},
  file = {/home/elessar/Zotero/storage/RACCUV44/Aghamohammadi et al. - 2016 - The Ambiguity of Simplicity.pdf}
}

@inproceedings{agradocastanoDetectionDeterminismNeuronal2012,
  title = {{Detection of determinism in neuronal activity background from microelectrodes recording signals}},
  booktitle = {{2012 XVII Symposium of Image, Signal Processing, and Artificial Vision (STSIVA)}},
  author = {Agrado Castano, Sebastian and Guarin, Diego L. and Orozco, Alvaro A.},
  year = {2012},
  month = sep,
  pages = {227--230},
  publisher = {IEEE},
  address = {Medellin, Antioquia, Colombia},
  doi = {10.1109/stsiva.2012.6340587},
  urldate = {2025-07-15},
  abstract = {In recent years the location of brain structures as the basal ganglia has shown to be a useful tool in surgery of patients suffering from neurodegenerative diseases like Parkinson's disease, where it is needed the implantation of a neurostimulator in the subthalamic nucleus (STN). Despite the different existing approaches that seek to address this problem, it is still possible to improve classification rates because during the stage of preprocessing of the signals from microelectrodes (which capture the neuronal activity of the brain), usually filtering a known signal portion: background neuronal activity. This preprocessing is done under the assumption that this part of the signal is not relevant for the classification process. However, in no event processing is performed to validate this assumption. This paper develops a methodology for the detection of determinism in background neuronal activity in the microelectrode recording signals (MER): application to Parkinson's disease. The proposal is based on the method of surrogate data, nonlinear tool has now been widely used for time series analysis, especially in physiological series1.},
  langid = {spanish},
  file = {/home/elessar/Zotero/storage/83I4IRUH/Agrado Castano et al. - 2012 - Detection of determinism in neuronal activity background from microelectrodes recording signals.pdf}
}

@misc{ahmedTransformersTimeseriesAnalysis2022,
  title = {Transformers in {{Time-series Analysis}}: {{A Tutorial}}},
  shorttitle = {Transformers in {{Time-series Analysis}}},
  author = {Ahmed, Sabeen and Nielsen, Ian E. and Tripathi, Aakash and Siddiqui, Shamoon and Rasool, Ghulam and Ramachandran, Ravi P.},
  year = {2022},
  month = apr,
  number = {arXiv:2205.01138},
  eprint = {2205.01138},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-30},
  abstract = {Transformer architecture has widespread applications, particularly in Natural Language Processing and computer vision. Recently Transformers have been employed in various aspects of time-series analysis. This tutorial provides an overview of the Transformer architecture, its applications, and a collection of examples from recent research papers in time-series analysis. We delve into an explanation of the core components of the Transformer, including the self-attention mechanism, positional encoding, multi-head, and encoder/decoder. Several enhancements to the initial, Transformer architecture are highlighted to tackle time-series tasks. The tutorial also provides best practices and techniques to overcome the challenge of effectively training Transformers for time-series analysis.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  annotation = {GSCC: 0000111},
  file = {/home/elessar/Zotero/storage/XGFF6X63/Ahmed et al. - 2022 - Transformers in Time-series Analysis A Tutorial.pdf}
}

@article{airoldiEntropyApproachDisclosure2011,
  title = {An Entropy Approach to Disclosure Risk Assessment: {{Lessons}} from Real Applications and Simulated Domains},
  shorttitle = {An Entropy Approach to Disclosure Risk Assessment},
  author = {Airoldi, Edoardo M. and Bai, Xue and Malin, Bradley A.},
  year = {2011},
  month = apr,
  journal = {Decision Support Systems},
  volume = {51},
  number = {1},
  pages = {10--20},
  publisher = {Elsevier BV},
  issn = {0167-9236},
  doi = {10.1016/j.dss.2010.11.014},
  urldate = {2025-07-25},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/UM523762/Airoldi et al. - 2011 - An entropy approach to disclosure risk assessment Lessons from real applications and simulated doma.pdf}
}

@inproceedings{akiyamaAnalysisCharacteristicsMultiStep2019,
  title = {Analysis on {{Characteristics}} of {{Multi-Step Learning Echo State Networks}} for {{Nonlinear Time Series Prediction}}},
  booktitle = {2019 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Akiyama, Takanori and Tanaka, Gouhei},
  year = {2019},
  month = jul,
  pages = {1--8},
  publisher = {IEEE},
  address = {Budapest, Hungary},
  doi = {10.1109/IJCNN.2019.8851876},
  urldate = {2023-05-30},
  abstract = {Reservoir Computing (RC) is a framework based on recurrent neural networks for high-speed learning and has attracted much attention. RC has been applied to a variety of temporal recognition tasks. Especially, Jaeger showed that the Echo State Network (ESN), which is one of the RC models, was effective for chaotic time series prediction tasks. However, there are two inevitable problems in nonlinear time series prediction using the standard ESN. One is that its prediction ability reaches a saturation point as the reservoir size is increased. The other is that its prediction ability depends heavily on hyperparameter values. In this paper, we propose a multi-step learning ESN to solve these problems. The proposed system has multiple reservoirs and the prediction error of one ESN-based predictor is corrected by another subsequent predictor. We demonstrate the effectiveness of the proposed method in two nonlinear time series prediction tasks. Another experiment using Lyapunov exponents suggests that the performance of the proposed method is robust against changes in hyperparameter values. In addition, we clarify the characteristic of the proposed method with regard to nonlinearity and memory using simple function approximation tasks.},
  isbn = {978-1-7281-1985-4},
  langid = {english},
  annotation = {GSCC: 0000017},
  file = {/home/elessar/Zotero/storage/GA8IUC4D/Akiyama and Tanaka - 2019 - Analysis on Characteristics of Multi-Step Learning.pdf}
}

@article{akyarSchurDiagonalStability2010,
  title = {On {{Schur Diagonal Stability}}},
  author = {Akyar, Handan},
  year = {2010},
  month = dec,
  journal = {Journal of Advanced Research in Applied Mathematics},
  volume = {2},
  number = {4},
  pages = {1--13},
  issn = {19429649},
  doi = {10.5373/jaram.412.041910},
  urldate = {2023-05-30},
  abstract = {In this paper, a necessary and sufficient condition which ensures the existence of diagonal solution of the Stein inequality for a 2{\texttimes}2 matrix is given. Then, this result is extended to common diagonal solution of finite number of 2 {\texttimes} 2 matrices. Schur diagonal stability of segment of matrices is also considered. Furthermore, a sufficient condition is obtained for Schur diagonal stability of 3 {\texttimes} 3 matrices. Some explanatory examples are presented in the text.},
  langid = {english},
  annotation = {GSCC: 0000501},
  file = {/home/elessar/Zotero/storage/PV4CM6UY/Akyar - 2010 - On Schur Diagonal Stability.pdf}
}

@inproceedings{allenderDerandomizationDistinguishingComplexity,
  title = {Derandomization and Distinguishing Complexity},
  booktitle = {18th {{IEEE Annual Conference}} on {{Computational Complexity}}, 2003. {{Proceedings}}.},
  author = {Allender, E. and Koucky, M. and Ronneburger, D. and {Sambuddha Roy}},
  pages = {209--220},
  publisher = {IEEE Comput. Soc},
  address = {Aarhus, Denmark},
  doi = {10.1109/ccc.2003.1214421},
  urldate = {2025-07-25},
  abstract = {We continue an investigation of resource-bounded Kolmogorov complexity and derandomization techniques begun in [2, 3].},
  langid = {english},
  file = {/home/elessar/Zotero/storage/4BMDIA7S/Allender et al. - Derandomization and distinguishing complexity.pdf}
}

@article{allenderNLprintableSetsNondeterministic2006,
  title = {{{NL-printable}} Sets and Nondeterministic {{Kolmogorov}} Complexity},
  author = {Allender, Eric},
  year = {2006},
  month = apr,
  journal = {Theoretical Computer Science},
  volume = {355},
  number = {2},
  pages = {127--138},
  publisher = {Elsevier BV},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2006.01.005},
  urldate = {2025-07-25},
  abstract = {P-printable sets were defined by Hartmanis and Yesha and have been investigated by several researchers. The analogous notion of L-printable sets was defined by Fortnow et al.; both P-printability and L-printability were shown to be related to notions of resourcebounded Kolmogorov complexity. Nondeterministic logspace (NL)-printability was defined by Jenner and Kirsig, but some basic questions regarding this notion were left open. In this paper we answer a question of Jenner and Kirsig by providing a machine-based characterization of the NL-printable sets.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/2C5YRQ5W/Allender - 2006 - NL-printable sets and nondeterministic Kolmogorov complexity.pdf}
}

@article{allenderWhatCanBe2006,
  title = {What Can Be Efficiently Reduced to the {{Kolmogorov-random}} Strings?},
  author = {Allender, Eric and Buhrman, Harry and Kouck{\'y}, Michal},
  year = {2006},
  month = mar,
  journal = {Annals of Pure and Applied Logic},
  volume = {138},
  number = {1-3},
  pages = {2--19},
  publisher = {Elsevier BV},
  issn = {0168-0072},
  doi = {10.1016/j.apal.2005.06.003},
  urldate = {2025-07-25},
  abstract = {We investigate the question of whether one can characterize complexity classes (such as PSPACE or NEXP) in terms of efficient reducibility to the set of Kolmogorov-random strings RC. This question arises because PSPACE {$\subseteq$} PRC and NEXP {$\subseteq$} NPRC , and no larger complexity classes are known to be reducible to RC in this way. We show that this question cannot be posed without explicitly dealing with issues raised by the choice of universal machine in the definition of Kolmogorov complexity. What follows is a list of some of our main results.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/CXURLGWN/Allender et al. - 2006 - What can be efficiently reduced to the Kolmogorov-random strings.pdf}
}

@article{alstro/mScalingRelationsCritical1986,
  title = {Scaling Relations at the Critical Line and the Period-Doubling Route for the Sine Map and the Driven Damped Pendulum},
  author = {Alstro/m, P. and Christiansen, B. and Hyldgaard, P. and Levinsen, M. T. and Rasmussen, R.},
  year = {1986},
  month = sep,
  journal = {Physical Review A},
  volume = {34},
  number = {3},
  pages = {2220--2233},
  issn = {0556-2791},
  doi = {10.1103/PhysRevA.34.2220},
  urldate = {2025-07-29},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/L9J25RSE/Alstrom et al. - 1986 - Scaling relations at the critical line and the period-doubling route for the sine map and the driven.pdf}
}

@article{altSpectralRadiusRandom2021,
  title = {Spectral Radius of Random Matrices with Independent Entries},
  author = {Alt, Johannes and Erdos, Laszlo and Kr{\"u}ger, Torben},
  year = {2021},
  month = may,
  journal = {Probability and Mathematical Physics},
  volume = {2},
  number = {2},
  eprint = {1907.13631},
  primaryclass = {math-ph},
  pages = {221--280},
  issn = {2690-1005, 2690-0998},
  doi = {10.2140/pmp.2021.2.221},
  urldate = {2023-05-30},
  abstract = {We consider random \$n{\textbackslash}times n\$ matrices \$X\$ with independent and centered entries and a general variance profile. We show that the spectral radius of \$X\$ converges with very high probability to the square root of the spectral radius of the variance matrix of \$X\$ when \$n\$ tends to infinity. We also establish the optimal rate of convergence, that is a new result even for general i.i.d. matrices beyond the explicitly solvable Gaussian cases. The main ingredient is the proof of the local inhomogeneous circular law [arXiv:1612.07776] at the spectral edge.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {60B20 15B52 46Txx,Mathematical Physics,Mathematics - Functional Analysis,Mathematics - Probability},
  annotation = {GSCC: 0000039},
  file = {/home/elessar/Zotero/storage/C3LRXTPE/Alt et al. - 2021 - Spectral radius of random matrices with independen.pdf;/home/elessar/Zotero/storage/RRBQ387A/Alt et al. - 2021 - Spectral radius of random matrices with independent entries.pdf}
}

@article{antunaMetodosMatematicosFisica,
  title = {{M{\'e}todos Matem{\'a}ticos de la F{\'i}sica}},
  author = {Antuna, Jose Mar{\i}n},
  langid = {spanish},
  file = {/home/elessar/Zotero/storage/KKFCFM6B/Antuna - Me´todos Matema´ticos de la F´ısica.pdf}
}

@article{antunesComputationalDepthConcept2006,
  title = {Computational Depth: {{Concept}} and Applications},
  shorttitle = {Computational Depth},
  author = {Antunes, Luis and Fortnow, Lance and Van Melkebeek, Dieter and Vinodchandran, N.V.},
  year = {2006},
  month = apr,
  journal = {Theoretical Computer Science},
  volume = {354},
  number = {3},
  pages = {391--404},
  publisher = {Elsevier BV},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2005.11.033},
  urldate = {2025-07-25},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/ICPR3H2H/Antunes et al. - 2006 - Computational depth Concept and applications.pdf}
}

@article{appeltantInformationProcessingUsing2011,
  title = {Information Processing Using a Single Dynamical Node as Complex System},
  author = {Appeltant, L. and Soriano, M.C. and Van Der Sande, G. and Danckaert, J. and Massar, S. and Dambre, J. and Schrauwen, B. and Mirasso, C.R. and Fischer, I.},
  year = {2011},
  month = sep,
  journal = {Nature Communications},
  volume = {2},
  number = {1},
  pages = {468},
  issn = {2041-1723},
  doi = {10.1038/ncomms1476},
  urldate = {2023-05-30},
  langid = {english},
  annotation = {GSCC: 0001615},
  file = {/home/elessar/Zotero/storage/YGC6IZ4K/Appeltant et al. - 2011 - Information processing using a single dynamical no.pdf}
}

@article{armenioOptimalTrainingEcho2020,
  title = {Optimal {{Training}} of {{Echo State Networks}} via {{Scenario Optimization}}},
  author = {Armenio, Luca Bugliari and Fagiano, Lorenzo and Terzi, Enrico and Farina, Marcello and Scattolini, Riccardo},
  year = {2020},
  journal = {IFAC-PapersOnLine},
  volume = {53},
  number = {2},
  pages = {5183--5188},
  issn = {24058963},
  doi = {10.1016/j.ifacol.2020.12.1187},
  urldate = {2025-03-12},
  abstract = {Echo State Networks (ESNs) are widely-used Recurrent Neural Networks. They are dynamical systems including, in state-space form, a nonlinear state equation and a linear output transformation. The common procedure to train ESNs is to randomly select the parameters of the state equation, and then to estimate those of the output equation via a standard least squares problem. Such a procedure is repeated for different instances of the random parameters characterizing the state equation, until satisfactory results are achieved. However, this trial-anderror procedure is not systematic and does not provide any guarantee about the optimality of the identification results. To solve this problem, we propose to complement the identification procedure of ESNs by applying results in scenario optimization. The resulting training procedure is theoretically sound and allows one to link precisely the number of identification instances to a guaranteed optimality bound on relevant performance indexes, such as the Root Mean Square error and the FIT index of the estimated model evaluated over a validation data-set. The proposed procedure is finally applied to the simulated model of a pH neutralization process: the obtained results confirm the validity of the approach.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/8IVWITGA/Armenio et al. - 2020 - Optimal Training of Echo State Networks via Scenario Optimization.pdf}
}

@book{arnoldRandomDynamicalSystems1998,
  title = {Random {{Dynamical Systems}}},
  author = {Arnold, Ludwig},
  year = {1998},
  series = {Springer {{Monographs}} in {{Mathematics}}},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-12878-7},
  urldate = {2025-07-28},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-642-08355-6 978-3-662-12878-7},
  langid = {english},
  file = {/home/elessar/Zotero/storage/I9NJXFBA/Arnold - 1998 - Random Dynamical Systems.pdf}
}

@article{arslanovElementaryComputabilityTheoreticProperties2001,
  title = {On {{Elementary Computability-Theoretic Properties}} of {{Algorithmic Randomness}}},
  author = {Arslanov, Asat},
  year = {2001},
  month = jan,
  journal = {Electronic Notes in Theoretical Computer Science},
  volume = {42},
  pages = {41--51},
  publisher = {Elsevier BV},
  issn = {1571-0661},
  doi = {10.1016/s1571-0661(04)80877-6},
  urldate = {2025-07-25},
  abstract = {In this paper we apply some elementary computability-theoretic notions to algorithmic complexity theory with the aim of understanding the role and extent of computability techniques for algorithmic complexity theory. We study some computability-theoretic properties of two different notions of randomness for finite strings: randomness based on the blank-endmarker complexity measure and Chaitin's randomness based on the self-delimiting complexity measure. We introduce the notion of complex infinite sequence of finite strings, which we call K-bounded sequences.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/ZAC9I3B7/Arslanov - 2001 - On Elementary Computability-Theoretic Properties of Algorithmic Randomness.pdf}
}

@article{atiyaNewResultsRecurrent2000,
  title = {New Results on Recurrent Network Training: Unifying the Algorithms and Accelerating Convergence},
  shorttitle = {New Results on Recurrent Network Training},
  author = {Atiya, A.F. and Parlos, A.G.},
  year = {2000},
  month = may,
  journal = {IEEE Transactions on Neural Networks},
  volume = {11},
  number = {3},
  pages = {697--709},
  issn = {10459227},
  doi = {10.1109/72.846741},
  urldate = {2023-05-30},
  abstract = {How to efficiently train recurrent networks remains a challenging and active research topic. Most of the proposed training approaches are based on computational ways to efficiently obtain the gradient of the error function, and can be generally grouped into five major groups. In this study we present a derivation that unifies these approaches. We demonstrate that the approaches are only five different ways of solving a particular matrix equation. The second goal of this paper is develop a new algorithm based on the insights gained from the novel formulation. The new algorithm, which is based on approximating the error gradient, has lower computational complexity in computing the weight update than the competing techniques for most typical problems. In addition, it reaches the error minimum in a much smaller number of iterations. A desirable characteristic of recurrent network training algorithms is to be able to update the weights in an on-line fashion. We have also developed an on-line version of the proposed algorithm, that is based on updating the error gradient approximation in a recursive manner.},
  langid = {english},
  annotation = {GSCC: 0000619},
  file = {/home/elessar/Zotero/storage/ZV7QJX2A/Atiya and Parlos - 2000 - New results on recurrent network training unifyin.pdf}
}

@article{attardInformationContentSignals1997,
  title = {Information Content of Signals Using Correlation Function Expansions of the Entropy},
  author = {Attard, Phil and Jepps, Owen G. and Mar{\v c}elja, Stjepan},
  year = {1997},
  month = oct,
  journal = {Physical Review E},
  volume = {56},
  number = {4},
  pages = {4052--4067},
  publisher = {American Physical Society (APS)},
  issn = {1063-651X, 1095-3787},
  doi = {10.1103/physreve.56.4052},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/WKYVUKNF/Attard et al. - 1997 - Information content of signals using correlation function expansions of the entropy.pdf}
}

@article{ayGeometricApproachComplexity2011,
  title = {A Geometric Approach to Complexity},
  author = {Ay, Nihat and Olbrich, Eckehard and Bertschinger, Nils and Jost, J{\"u}rgen},
  year = {2011},
  month = sep,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {21},
  number = {3},
  publisher = {AIP Publishing},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.3638446},
  urldate = {2025-07-15},
  abstract = {We develop a geometric approach to complexity based on the principle that complexity requires interactions at different scales of description. Complex systems are more than the sum of their parts of any size and not just more than the sum of their elements. Using information geometry, we therefore analyze the decomposition of a system in terms of an interaction hierarchy. In mathematical terms, we present a theory of complexity measures for finite random fields using the geometric framework of hierarchies of exponential families. Within our framework, previously proposed complexity measures find their natural place and gain a new interpretation.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/27D6GRNP/Ay et al. - 2011 - A geometric approach to complexity.pdf}
}

@article{ayUnifyingFrameworkComplexity,
  title = {A {{Unifying Framework}} for {{Complexity Measures}} of {{Finite Systems}}},
  author = {Ay, Nihat and Olbrich, Eckehard and Bertschinger, Nils and Jost, Jurgen},
  abstract = {We develop a unifying approach for complexity measures, based on the principle that complexity requires interactions at different scales of description. Complex systems are more than the sum of their parts of any size, and not just more than the sum of their elements. We therefore analyze the decomposition of a system in terms of an interaction hierarchy. In mathematical terms, we present a theory of complexity measures for finite random fields using the geometric framework of hierarchies of exponential families. Within our framework, previously proposed complexity measures find their natural place and gain a new interpretation.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/6QZD7LF9/Ay et al. - A Unifying Framework for Complexity Measures of Finite Systems.pdf}
}

@article{baileyRandomCharacterFundamental2001,
  title = {On the {{Random Character}} of {{Fundamental Constant Expansions}}},
  author = {Bailey, David H. and Crandall, Richard E.},
  year = {2001},
  month = jan,
  journal = {Experimental Mathematics},
  volume = {10},
  number = {2},
  pages = {175--190},
  publisher = {Informa UK Limited},
  issn = {1058-6458, 1944-950X},
  doi = {10.1080/10586458.2001.10504441},
  urldate = {2025-07-25},
  langid = {english},
  file = {/home/elessar/Zotero/storage/UTJ9CIBD/Bailey and Crandall - 2001 - On the Random Character of Fundamental Constant Expansions.pdf}
}

@article{bakModelockingTransitionChaos,
  title = {Mode-Locking and the {{Transition}} to {{Chaos In Dissipative Syste}}},
  author = {Bak, Per},
  abstract = {Dissipative systems with two competing frequencies exhibit transitions to chaos. We have investigated the transition through a study of discrete maps of the circle onto itself, and by constructing and analyzing return maps of differential equations representing some physical systems. The transition is caused by interaction and overlap of mode-locked resonances and takes place at a critical line where the map looses invertibility. At this line the mode-locked intervals trace up a complete Devil's Staircase whose complementary set is a Cantor set with universal fractal dimension D - 0.87. Below criticality there is room for quasiperiodic orbits, whose measure is given by an exponent B " 0.34 which can be related to D through a scaling relation, just as for second order phase transitions. The Lebesgue measure serves as an order parameter for the transition to chaos.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/R24QRJ8Q/Bak - Mode-locking and the Transition to Chaos In Dissipative Syste.pdf}
}

@article{baldwinCalculatingTopologicalEntropy1997,
  title = {Calculating {{Topological Entropy}}},
  author = {Baldwin, Stewart L. and Slaminka, Edward E.},
  year = {1997},
  month = dec,
  journal = {Journal of Statistical Physics},
  volume = {89},
  number = {5-6},
  pages = {1017--1033},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {0022-4715, 1572-9613},
  doi = {10.1007/bf02764219},
  urldate = {2025-07-25},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/home/elessar/Zotero/storage/SFT3FXCG/Baldwin and Slaminka - 1997 - Calculating Topological Entropy.pdf}
}

@article{banerjeaDevilsStaircaseOnedimensional1984,
  title = {Devil's Staircase in a One-Dimensional Model},
  author = {Banerjea, Amitava and Taylor, P. L.},
  year = {1984},
  month = dec,
  journal = {Physical Review B},
  volume = {30},
  number = {11},
  pages = {6489--6497},
  issn = {0163-1829},
  doi = {10.1103/PhysRevB.30.6489},
  urldate = {2025-07-29},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/XYNY3DPX/Banerjea and Taylor - 1984 - Devil's staircase in a one-dimensional model.pdf}
}

@article{barkleyNearcriticalBehaviorOneparameter1988,
  title = {Near-Critical Behavior for One-Parameter Families of Circle Maps},
  author = {Barkley, Dwight},
  year = {1988},
  month = may,
  journal = {Physics Letters A},
  volume = {129},
  number = {4},
  pages = {219--222},
  issn = {03759601},
  doi = {10.1016/0375-9601(88)90353-2},
  urldate = {2025-07-29},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/LQTR7MPE/Barkley - 1988 - Near-critical behavior for one-parameter families of circle maps.pdf}
}

@article{barnumEntropyInformationCausality2010,
  title = {Entropy and Information Causality in General Probabilistic Theories},
  author = {Barnum, Howard and Barrett, Jonathan and Clark, Lisa Orloff and Leifer, Matthew and Spekkens, Robert and Stepanik, Nicholas and Wilce, Alex and Wilke, Robin},
  year = {2010},
  month = mar,
  journal = {New Journal of Physics},
  volume = {12},
  number = {3},
  pages = {033024},
  publisher = {IOP Publishing},
  issn = {1367-2630},
  doi = {10.1088/1367-2630/12/3/033024},
  urldate = {2025-07-25},
  abstract = {We investigate the concept of entropy in probabilistic theories more general than quantum mechanics, with particular reference to the notion of information causality (IC) recently proposed by Pawlowski et al (2009 arXiv:0905.2292). We consider two entropic quantities, which we term measurement and mixing entropy. In the context of classical and quantum theory, these coincide, being given by the Shannon and von Neumann entropies, respectively; in general, however, they are very different. In particular, while measurement entropy is easily seen to be concave, mixing entropy need not be. In fact, as we show, mixing entropy is not concave whenever the state space is a non-simplicial polytope. Thus, the condition that measurement and mixing entropies coincide is a strong constraint on possible theories. We call theories with this property monoentropic.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/WYFSKUFR/Barnum et al. - 2010 - Entropy and information causality in general probabilistic theories.pdf}
}

@book{barretoIntuitiveUnderstandingKalman2020,
  title = {"{{Intuitive}} Understanding of {{Kalman}} Filtering with {{MATLAB}}"},
  author = {Barreto, Armando and Adjouadi, Malek and Ortega, Francisco R. and {O-larnnithipong}, Nonnarit},
  year = {2020},
  edition = {First edition},
  publisher = {CRC Press},
  address = {Boca Raton, FL},
  abstract = {"The emergence of affordable micro sensors, such as MEMS Inertial Measurement Systems, which are being applied in embedded systems and Internet-of-Things devices, has brought techniques such as Kalman Filtering, capable of combining information from multiple sensors or sources, to the interest of students and hobbyists. This will book will develop just the necessary background concepts, helping a much wider audience of readers develop an understanding and intuition that will enable them to follow the explanation for the Kalman Filtering algorithm"--},
  isbn = {978-0-367-19135-1 978-0-367-19133-7},
  langid = {english},
  lccn = {QA402.3 .B3535 2020},
  keywords = {Kalman filtering,MATLAB},
  file = {/home/elessar/Zotero/storage/R877SF7M/Barreto et al. - 2020 - Intuitive understanding of Kalman filtering with MATLAB.pdf}
}

@inproceedings{baruchFilteringPredictionIndividual,
  title = {Filtering and Prediction of Individual Sequences Corrupted by Noise Using the {{Lempel-Ziv}} Algorithm},
  booktitle = {2000 {{IEEE International Symposium}} on {{Information Theory}} ({{Cat}}. {{No}}.{{00CH37060}})},
  author = {Baruch, A. and Merhav, N.},
  pages = {99},
  publisher = {IEEE},
  address = {Sorrento, Italy},
  doi = {10.1109/isit.2000.866389},
  urldate = {2025-07-15},
  abstract = {We address the problem of filtering and prediction of an individual binary sequence based on its noisy past, as an extension to [l]. The performance criterion investigated is the expected fraction of errors. We propose algorithms and compare their performance to that of the best finite state machine (FSM).We improve on previous results [l]by showing that optimum performance can be achieved by Lempel-Ziv-based estimation algorithms.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/U4E3SPVY/Baruch and Merhav - Filtering and prediction of individual sequences corrupted by noise using the Lempel-Ziv algorithm.pdf}
}

@misc{basterrechEmpiricalAnalysisNecessary2017,
  title = {Empirical {{Analysis}} of the {{Necessary}} and {{Sufficient Conditions}} of the {{Echo State Property}}},
  author = {Basterrech, Sebasti{\'a}n},
  year = {2017},
  month = mar,
  number = {arXiv:1703.06664},
  eprint = {1703.06664},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-21},
  abstract = {The Echo State Network (ESN) is a specific recurrent network, which has gained popularity during the last years. The model has a recurrent network named reservoir, that is fixed during the learning process. The reservoir is used for transforming the input space in a larger space. A fundamental property that provokes an impact on the model accuracy is the Echo State Property (ESP). There are two main theoretical results related to the ESP. First, a sufficient condition for the ESP existence that involves the singular values of the reservoir matrix. Second, a necessary condition for the ESP. The ESP can be violated according to the spectral radius value of the reservoir matrix. There is a theoretical gap between these necessary and sufficient conditions. This article presents an empirical analysis of the accuracy and the projections of reservoirs that satisfy this theoretical gap. It gives some insights about the generation of the reservoir matrix. From previous works, it is already known that the optimal accuracy is obtained near to the border of stability control of the dynamics. Then, according to our empirical results, we can see that this border seems to be closer to the sufficient conditions than to the necessary conditions of the ESP.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/elessar/Zotero/storage/RXE3AYB8/Basterrech - 2017 - Empirical Analysis of the Necessary and Sufficient Conditions of the Echo State Property.pdf}
}

@inbook{baykasoluArtificialBeeColony2007,
  title = {Artificial {{Bee Colony Algorithm}} and {{Its Application}} to {{Generalized Assignment Problem}}},
  booktitle = {Swarm {{Intelligence}}, {{Focus}} on {{Ant}} and {{Particle Swarm Optimization}}},
  year = {2007},
  month = dec,
  publisher = {{I-Tech Education and Publishing}},
  doi = {10.5772/5101},
  urldate = {2025-07-15},
  collaborator = {Baykasolu, Adil and Oumlzbakr, Lale and Tapk, Pnar},
  isbn = {978-3-902613-09-7},
  langid = {english},
  file = {/home/elessar/Zotero/storage/5ADSZVS6/2007 - Artificial Bee Colony Algorithm and Its Application to Generalized Assignment Problem.pdf}
}

@article{becherLinearlyComputableMeasure2012,
  title = {A Linearly Computable Measure of String Complexity},
  author = {Becher, Ver{\'o}nica and Heiber, Pablo Ariel},
  year = {2012},
  month = jun,
  journal = {Theoretical Computer Science},
  volume = {438},
  pages = {62--73},
  publisher = {Elsevier BV},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2012.03.007},
  urldate = {2025-07-15},
  abstract = {We present a measure of string complexity, called I-complexity, computable in linear time and space. It counts the number of different substrings in a given string. The least complex strings are the runs of a single symbol, the most complex are the de Bruijn strings. Although the I-complexity of a string is not the length of any minimal description of the string, it satisfies many basic properties of classical description complexity. In particular, the number of strings with I-complexity up to a given value is bounded, and most strings of each length have high I-complexity.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/5M5PJVKD/Becher and Heiber - 2012 - A linearly computable measure of string complexity.pdf}
}

@article{beichlMetropolisAlgorithmHas,
  title = {The {{Metropolis Algorithm}} Has Been the Most Successful and Influential of All the Members of the Computational Species That Used to Be Called the ``{{Monte Carlo Method}}.'' {{Today}}, Topics Related to This Algorithm Constitute an Entire Field of Computational Science Supported by a Deep Theory and Having Applications Ranging from Physical Simulations to the Foundations of Computational Complexity.},
  author = {Beichl, Isabel and Sullivan, Francis},
  langid = {english},
  file = {/home/elessar/Zotero/storage/7PJKT2DG/Beichl and Sullivan - The Metropolis Algorithm has been the most successful and inﬂuential of all the members of the compu.PDF}
}

@article{bennettLogicalReversibilityComputation1973,
  title = {Logical {{Reversibility}} of {{Computation}}},
  author = {Bennett, C. H.},
  year = {1973},
  month = nov,
  journal = {IBM Journal of Research and Development},
  volume = {17},
  number = {6},
  pages = {525--532},
  publisher = {IBM},
  issn = {0018-8646, 0018-8646},
  doi = {10.1147/rd.176.0525},
  urldate = {2025-07-25},
  abstract = {The usual general-purpose computing automaton (e.g.. a Turing machine) is logically irreversible- its transition function lacks a single-valued inverse. Here it is shown that such machines may he made logically reversible at every step, while retainillg their simplicity and their ability to do general computations. This result is of great physical interest because it makes plausible the existence of thermodynamically reversible computers which could perform useful computations at useful speed while dissipating considerably lessthan kT of energyper logical step. In the first stage of itscomputationthe logically reversibleautomatonparallels the corresponding irreversible automaton, except that it saves all intermediate results, thereby avoiding the irreversible operation of erasure. The second stage consistsof printing out the desired output. Thethird stage then reversibly disposes of all the undesired intermediate results by retracing the steps of the first stage in backward order (a process which is only possible because the first stage has been carried out reversibly), thereby restoring the machine (except for the now-written output tape) toits original condition. The final machine configuration thus contains the desired output anda reconstructed copy o f the input, but no other undesired data. The foregoing results are demonstrated explicitly using a type of three-tape Turing machine. The biosynthesis of messenger RNA is discussed as a physical example of reversible computation.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/BCLVFNRG/Bennett - 1973 - Logical Reversibility of Computation.pdf}
}

@article{bergmeirNoteValidityCrossvalidation2018,
  title = {A Note on the Validity of Cross-Validation for Evaluating Autoregressive Time Series Prediction},
  author = {Bergmeir, Christoph and Hyndman, Rob J. and Koo, Bonsoo},
  year = {2018},
  month = apr,
  journal = {Computational Statistics \& Data Analysis},
  volume = {120},
  pages = {70--83},
  issn = {01679473},
  doi = {10.1016/j.csda.2017.11.003},
  urldate = {2025-08-12},
  langid = {english}
}

@article{bergmeirUseCrossvalidationTime2012,
  title = {On the Use of Cross-Validation for Time Series Predictor Evaluation},
  author = {Bergmeir, Christoph and Ben{\'i}tez, Jos{\'e} M.},
  year = {2012},
  month = may,
  journal = {Information Sciences},
  volume = {191},
  pages = {192--213},
  issn = {00200255},
  doi = {10.1016/j.ins.2011.12.028},
  urldate = {2025-08-12},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english}
}

@article{berthiaumeQuantumKolmogorovComplexity2001,
  title = {Quantum {{Kolmogorov Complexity}}},
  author = {Berthiaume, Andr{\'e} and Van Dam, Wim and Laplante, Sophie},
  year = {2001},
  month = sep,
  journal = {Journal of Computer and System Sciences},
  volume = {63},
  number = {2},
  pages = {201--221},
  publisher = {Elsevier BV},
  issn = {0022-0000},
  doi = {10.1006/jcss.2001.1765},
  urldate = {2025-07-15},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/QFHDXBHT/Berthiaume et al. - 2001 - Quantum Kolmogorov Complexity.pdf}
}

@article{bertschingerRealTimeComputationEdge2004,
  title = {Real-{{Time Computation}} at the {{Edge}} of {{Chaos}} in {{Recurrent Neural Networks}}},
  author = {Bertschinger, Nils and Natschl{\"a}ger, Thomas},
  year = {2004},
  month = jul,
  journal = {Neural Computation},
  volume = {16},
  number = {7},
  pages = {1413--1436},
  publisher = {MIT Press - Journals},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976604323057443},
  urldate = {2025-07-24},
  abstract = {Depending on the connectivity, recurrent networks of simple computational units can show very different types of dynamics, ranging from totally ordered to chaotic. We analyze how the type of dynamics (ordered or chaotic) exhibited by randomly connected networks of threshold gates driven by a time-varying input signal depends on the parameters describing the distribution of the connectivity matrix. In particular, we calculate the critical boundary in parameter space where the transition from ordered to chaotic dynamics takes place. Employing a recently developed framework for analyzing real-time computations, we show that only near the critical boundary can such networks perform complex computations on time series. Hence, this result strongly supports conjectures that dynamical systems that are capable of doing complex computational tasks should operate near the edge of chaos, that is, the transition from ordered to chaotic dynamics.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/ZY2WNW3T/Bertschinger and Natschläger - 2004 - Real-Time Computation at the Edge of Chaos in Recurrent Neural Networks.pdf}
}

@article{bezansonJuliaFreshApproach2017,
  title = {Julia: {{A Fresh Approach}} to {{Numerical Computing}}},
  shorttitle = {Julia},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  year = {2017},
  month = jan,
  journal = {SIAM Review},
  volume = {59},
  number = {1},
  pages = {65--98},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/141000671},
  urldate = {2025-08-12},
  langid = {english}
}

@misc{bhattacharjeeEntropyPerpetualComputers2003,
  title = {Entropy and Perpetual Computers},
  author = {Bhattacharjee, Somendra M.},
  year = {2003},
  month = oct,
  number = {arXiv:cond-mat/0310332},
  eprint = {cond-mat/0310332},
  publisher = {arXiv},
  doi = {10.48550/arXiv.cond-mat/0310332},
  urldate = {2025-07-25},
  abstract = {A definition of entropy via the Kolmogorov algorithmic complexity is discussed. As examples, we show how the meanfield theory for the Ising model, and the entropy of a perfect gas can be recovered. The connection with computations are pointed out, by paraphrasing the laws of thermodynamics for computers. Also discussed is an approach that may be adopted to develop statistical mechanics using the algorithmic point of view.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Statistical Mechanics,High Energy Physics - Lattice,High Energy Physics - Theory,Quantum Physics},
  file = {/home/elessar/Zotero/storage/XV5SUREC/Bhattacharjee - 2003 - Entropy and perpetual computers.PDF}
}

@misc{bialekPredictabilityComplexityLearning2001,
  title = {Predictability, Complexity and Learning},
  author = {Bialek, William and Nemenman, Ilya and Tishby, Naftali},
  year = {2001},
  month = jan,
  number = {arXiv:physics/0007070},
  eprint = {physics/0007070},
  publisher = {arXiv},
  doi = {10.48550/arXiv.physics/0007070},
  urldate = {2025-07-25},
  abstract = {We define \{{\textbackslash}em predictive information\} \$I\_\{{\textbackslash}rm pred\} (T)\$ as the mutual information between the past and the future of a time series. Three qualitatively different behaviors are found in the limit of large observation times \$T\$: \$I\_\{{\textbackslash}rm pred\} (T)\$ can remain finite, grow logarithmically, or grow as a fractional power law. If the time series allows us to learn a model with a finite number of parameters, then \$I\_\{{\textbackslash}rm pred\} (T)\$ grows logarithmically with a coefficient that counts the dimensionality of the model space. In contrast, power--law growth is associated, for example, with the learning of infinite parameter (or nonparametric) models such as continuous functions with smoothness constraints. There are connections between the predictive information and measures of complexity that have been defined both in learning theory and in the analysis of physical systems through statistical mechanics and dynamical systems theory. Further, in the same way that entropy provides the unique measure of available information consistent with some simple and plausible conditions, we argue that the divergent part of \$I\_\{{\textbackslash}rm pred\} (T)\$ provides the unique measure for the complexity of dynamics underlying a time series. Finally, we discuss how these ideas may be useful in different problems in physics, statistics, and biology.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Other Condensed Matter,Nonlinear Sciences - Adaptation and Self-Organizing Systems,Physics - Data Analysis Statistics and Probability,Quantitative Biology - Other Quantitative Biology},
  file = {/home/elessar/Zotero/storage/TTVMIN48/Bialek et al. - 2001 - Predictability, complexity and learning.pdf}
}

@article{bianchiInvestigatingEchoState2018,
  title = {Investigating Echo State Networks Dynamics by Means of Recurrence Analysis},
  author = {Bianchi, Filippo Maria and Livi, Lorenzo and Alippi, Cesare},
  year = {2018},
  month = feb,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {29},
  number = {2},
  eprint = {1601.07381},
  primaryclass = {nlin, physics:physics},
  pages = {427--439},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2016.2630802},
  urldate = {2023-05-30},
  abstract = {In this paper, we elaborate over the well-known interpretability issue in echo state networks. The idea is to investigate the dynamics of reservoir neurons with time-series analysis techniques taken from research on complex systems. Notably, we analyze time-series of neuron activations with Recurrence Plots (RPs) and Recurrence Quantification Analysis (RQA), which permit to visualize and characterize high-dimensional dynamical systems. We show that this approach is useful in a number of ways. First, the two-dimensional representation offered by RPs provides a way for visualizing the high-dimensional dynamics of a reservoir. Our results suggest that, if the network is stable, reservoir and input denote similar line patterns in the respective RPs. Conversely, the more unstable the ESN, the more the RP of the reservoir presents instability patterns. As a second result, we show that the Lmax measure is highly correlated with the well-established maximal local Lyapunov exponent. This suggests that complexity measures based on RP diagonal lines distribution provide a valuable tool to quantify the degree of network stability. Finally, our analysis shows that all RQA measures fluctuate on the proximity of the so-called edge of stability, where an ESN typically achieves maximum computational capability. We verify that the determination of the edge of stability provided by such RQA measures is more accurate than two well-known criteria based on the Jacobian matrix of the reservoir. Therefore, we claim that RPs and RQA-based analyses can be used as valuable tools to design an effective network given a specific problem.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Nonlinear Sciences - Chaotic Dynamics,Physics - Data Analysis Statistics and Probability},
  annotation = {GSCC: 0000119},
  file = {/home/elessar/Zotero/storage/XE73AMCM/Bianchi et al. - 2018 - Investigating echo state networks dynamics by mean.pdf}
}

@article{bianchiReservoirComputingApproaches2021,
  title = {Reservoir {{Computing Approaches}} for {{Representation}} and {{Classification}} of {{Multivariate Time Series}}},
  author = {Bianchi, Filippo Maria and Scardapane, Simone and Lokse, Sigurd and Jenssen, Robert},
  year = {2021},
  month = may,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {5},
  pages = {2169--2179},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2020.3001377},
  urldate = {2025-08-12},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}
}

@article{billingeInferringPlanarDisorder,
  title = {Inferring Planar Disorder in Close-Packed Structures Via},
  author = {Billinge, S J L and Miao, J},
  langid = {english},
  file = {/home/elessar/Zotero/storage/IX9WA3YR/Billinge and Miao - Inferring planar disorder in close-packed structures via.pdf}
}

@article{binderCollectiveBehaviourOnedimensional1992,
  title = {Collective Behaviour in One-Dimensional Locally Coupled Map Lattices},
  author = {Binder, P -M and Privman, V},
  year = {1992},
  month = jul,
  journal = {Journal of Physics A: Mathematical and General},
  volume = {25},
  number = {13},
  pages = {L775-L780},
  publisher = {IOP Publishing},
  issn = {0305-4470, 1361-6447},
  doi = {10.1088/0305-4470/25/13/004},
  urldate = {2025-07-15},
  langid = {english},
  file = {/home/elessar/Zotero/storage/ZGXYZUE6/Binder and Privman - 1992 - Collective behaviour in one-dimensional locally coupled map lattices.pdf}
}

@article{binderSecondorderDynamicsCollective1992,
  title = {Second-Order Dynamics in the Collective Temporal Evolution of Complex Systems},
  author = {Binder, P.-M. and Privman, V.},
  year = {1992},
  month = jun,
  journal = {Physical Review Letters},
  volume = {68},
  number = {26},
  pages = {3830--3833},
  publisher = {American Physical Society (APS)},
  issn = {0031-9007},
  doi = {10.1103/physrevlett.68.3830},
  urldate = {2025-07-15},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/VSWUDLXK/Binder and Privman - 1992 - Second-order dynamics in the collective temporal evolution of complex systems.pdf}
}

@inproceedings{bissaroRegularEchoState2021,
  title = {Regular Echo State Networks: Simple and Accurate Reservoir Models to Real-World Applications},
  shorttitle = {Regular Echo State Networks},
  booktitle = {Proceedings of the 36th {{Annual ACM Symposium}} on {{Applied Computing}}},
  author = {Bissaro, Lucas Z. and Jin, Yaochu and Carneiro, Murillo G.},
  year = {2021},
  month = mar,
  pages = {1063--1069},
  publisher = {ACM},
  address = {Virtual Event Republic of Korea},
  doi = {10.1145/3412841.3441983},
  urldate = {2024-01-10},
  abstract = {Reservoir computing is a computational paradigm derived from recurrent neural network models. One of its most representative technique is the Echo State Network (ESN), which is usually composed by two salient components: reservoir and readout. The former is responsible by mapping temporal (or sequential) inputs into a high-dimensional space and the latter aims at learning the patterns in such a new space. Despite ESN has attracted a lot of attention nowadays, most works in the literature are focused on the development of additional features to the model, while there is a considerable lack of investigations related to understand and evaluate some of its inherent concepts as well as their relationships. In this paper we address such a limitation by investigating ESN components related to the reservoir structure and readout layer. To be specific, we evaluate regular and small-world network models besides the extensively adopted random one, and also analyze a total of eight classification techniques instead of considering just the few techniques largely adopted in the literature (mostly linear ones). In order to consistently evaluate the alternative ESN methods, we analyzed a wide range of parameters in both reservoir and readout layers through of several experiments with five real-world data sets. The results revealed that some problems can be considerably benefited from some level of organization in the reservoir, such as those provided by regular or small-world network models; and that the non-linear support vector machine classifier achieved the best predictive performance, although it was statistically comparable with the k-nearest neighbors one, which has much smaller time complexity. Interestingly, such findings may make the adoption of ESN methods more efficient from the point of view of embedded systems and large scale problems.},
  isbn = {978-1-4503-8104-8},
  langid = {english},
  annotation = {GSCC: 0000003},
  file = {/home/elessar/Zotero/storage/764UJPTL/Bissaro et al. - 2021 - Regular echo state networks simple and accurate r.pdf;/home/elessar/Zotero/storage/IHZSWCEU/Bissaro et al_2021_Regular echo state networks.pdf}
}

@article{bivertSusskindQuantumMechanics,
  title = {Susskind {{Quantum Mechanics}} - {{Solutions}}},
  author = {Bivert, M},
  abstract = {Below are solution proposals to the exercises of The Theoretical Minimum - Quantum Mechanics, written by Leonard Susskind and Art Friedman. An effort has been so as to recall from the book all the referenced equations, and to be rather verbose regarding mathematical details, in line with the general tone of the series.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/XMA6YVMH/Bivert - Quantum Mechanics - Solutions.pdf}
}

@article{blackBAYESIANDATAANALYSIS,
  title = {{{BAYESIAN DATA ANALYSIS}}},
  author = {Black, Timothy C and Thompson, William J},
  langid = {english},
  file = {/home/elessar/Zotero/storage/FF4HU6YM/Black and Thompson - BAYESIAN DATA ANALYSIS.PDF}
}

@article{blancDelayIndependenceMutualinformation2011,
  title = {Delay Independence of Mutual-Information Rate of Two Symbolic Sequences},
  author = {Blanc, Jean-Luc and Pezard, Laurent and Lesne, Annick},
  year = {2011},
  month = sep,
  journal = {Physical Review E},
  volume = {84},
  number = {3},
  publisher = {American Physical Society (APS)},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/physreve.84.036214},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/7CSUM3DZ/Blanc et al. - 2011 - Delay independence of mutual-information rate of two symbolic sequences.pdf}
}

@article{blanchardResultsChaoticBehavior2005,
  title = {Some Results about the Chaotic Behavior of Cellular Automata},
  author = {Blanchard, F. and Cervelle, J. and Formenti, E.},
  year = {2005},
  month = dec,
  journal = {Theoretical Computer Science},
  volume = {349},
  number = {3},
  pages = {318--336},
  publisher = {Elsevier BV},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2005.06.038},
  urldate = {2025-07-25},
  abstract = {We study the behavior of cellular automata (CA for short) in the Cantor, Besicovitch and Weyl topologies. We solve an open problem about the existence of transitive CA in the Besicovitch topology. The proof of this result has some interest of its own since it is obtained by using Kolmogorov complexity. To our knowledge it is the first result about discrete dynamical systems obtained using Kolmogorov complexity. We also prove that in the Besicovitch topology every CA has either a unique periodic point (thus a fixed point) or an uncountable set of periodic points. This result underlines the fact that CA have a great degree of stability; it may be considered a further step towards the understanding of CA periodic behavior.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/CG42NY37/Blanchard et al. - 2005 - Some results about the chaotic behavior of cellular automata.pdf}
}

@article{blancQUANTIFYINGNEURALCORRELATIONS,
  title = {{{QUANTIFYING NEURAL CORRELATIONS USING LEMPEL-ZIV COMPLEXITY}}},
  author = {Blanc, Jean-Luc and Schmidt, Nicolas and Bonnier, Loic and Pezard, Laurent},
  abstract = {Spike train analysis generally focuses on two aims: (1) the estimate of the neuronal information quantity, and (2) the quantification of spikes or bursts synchronization. We introduce here a new multivariate index based on LempelZiv complexity for spike train analysis. This index, called mutual Lempel-Ziv complexity (MLZC), can both measure spikes correlations and estimate the information carried in spike trains (i.e. characterize the dynamic process). Using simulated spike trains from a Poisson process, we show that the MLZC is able to quantify spike correlations. In addition, using bursting activity generated by electrically coupled Hindmarsh-Rose neurons, the MLZC is able to quantify and characterize bursts synchronization, when classical measures fail.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/HUQ7DZZT/Blanc et al. - QUANTIFYING NEURAL CORRELATIONS USING LEMPEL-ZIV COMPLEXITY.pdf}
}

@article{boccalettiComplexNetworksStructure2006,
  title = {Complex Networks: {{Structure}} and Dynamics},
  shorttitle = {Complex Networks},
  author = {Boccaletti, S and Latora, V and Moreno, Y and Chavez, M and Hwang, D},
  year = {2006},
  month = feb,
  journal = {Physics Reports},
  volume = {424},
  number = {4-5},
  pages = {175--308},
  issn = {03701573},
  doi = {10.1016/j.physrep.2005.10.009},
  urldate = {2024-06-14},
  abstract = {Coupled biological and chemical systems, neural networks, social interacting species, the Internet and the World Wide Web, are only a few examples of systems composed by a large number of highly interconnected dynamical units. The first approach to capture the global properties of such systems is to model them as graphs whose nodes represent the dynamical units, and whose links stand for the interactions between them. On the one hand, scientists have to cope with structural issues, such as characterizing the topology of a complex wiring architecture, revealing the unifying principles that are at the basis of real networks, and developing models to mimic the growth of a network and reproduce its structural properties. On the other hand, many relevant questions arise when studying complex networks' dynamics, such as learning how a large ensemble of dynamical systems that interact through a complex wiring topology can behave collectively. We review the major concepts and results recently achieved in the study of the structure and dynamics of complex networks, and summarize the relevant applications of these ideas in many different disciplines, ranging from nonlinear science to biology, from statistical mechanics to medicine and engineering.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  annotation = {GSCC: 0013310},
  file = {/home/elessar/Zotero/storage/L3MWC4HZ/Boccaletti et al. - 2006 - Complex networks Structure and dynamics.pdf}
}

@article{boccatoExtendedEchoState2012,
  title = {An Extended Echo State Network Using {{Volterra}} Filtering and Principal Component Analysis},
  author = {Boccato, Levy and Lopes, Amauri and Attux, Romis and Von Zuben, Fernando J.},
  year = {2012},
  month = aug,
  journal = {Neural Networks},
  volume = {32},
  pages = {292--302},
  issn = {08936080},
  doi = {10.1016/j.neunet.2012.02.028},
  urldate = {2024-10-21},
  abstract = {Echo state networks (ESNs) can be interpreted as promoting an encouraging compromise between two seemingly conflicting objectives: (i) simplicity of the resulting mathematical model and (ii) capability to express a wide range of nonlinear dynamics. By imposing fixed weights to the recurrent connections, the echo state approach avoids the well-known difficulties faced by recurrent neural network training strategies, but still preserves, to a certain extent, the potential of the underlying structure due to the existence of feedback loops within the dynamical reservoir. Moreover, the overall training process is relatively simple, as it amounts essentially to adapting the readout, which usually corresponds to a linear combiner. However, the linear nature of the output layer may limit the capability of exploring the available information, since higher-order statistics of the signals are not taken into account. In this work, we present a novel architecture for an ESN in which the linear combiner is replaced by a Volterra filter structure. Additionally, the principal component analysis technique is used to reduce the number of effective signals transmitted to the output layer. This idea not only improves the processing capability of the network, but also preserves the simplicity of the training process. The proposed architecture is then analyzed in the context of a set of representative information extraction problems, more specifically supervised and unsupervised channel equalization, and blind separation of convolutive mixtures. The obtained results, when compared to those produced by already proposed ESN versions, highlight the benefits brought by the novel network proposal and characterize it as a promising tool to deal with challenging signal processing tasks.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  annotation = {GSCC: 0000067},
  file = {/home/elessar/Zotero/storage/KKLVLJ76/Boccato et al. - 2012 - An extended echo state network using Volterra filtering and principal component analysis.pdf}
}

@article{boedeckerInformationProcessingEcho2012,
  title = {Information Processing in Echo State Networks at the Edge of Chaos},
  author = {Boedecker, Joschka and Obst, Oliver and Lizier, Joseph T. and Mayer, N. Michael and Asada, Minoru},
  year = {2012},
  month = sep,
  journal = {Theory in Biosciences},
  volume = {131},
  number = {3},
  pages = {205--213},
  issn = {1431-7613, 1611-7530},
  doi = {10.1007/s12064-011-0146-8},
  urldate = {2023-05-30},
  langid = {english},
  annotation = {GSCC: 0000311},
  file = {/home/elessar/Zotero/storage/FTZMALHZ/Boedecker et al. - 2012 - Information processing in echo state networks at t.pdf}
}

@misc{bogdanovStudyLorenzRossler2014,
  title = {The Study of {{Lorenz}} and {{R{\"o}ssler}} Strange Attractors by Means of Quantum Theory},
  author = {Bogdanov, Yu I. and Bogdanova, N. A.},
  year = {2014},
  month = dec,
  number = {arXiv:1412.2242},
  eprint = {1412.2242},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.2242},
  urldate = {2024-11-27},
  abstract = {We have developed a method for complementing an arbitrary classical dynamical system to a quantum system using the Lorenz and R{\textbackslash}"ossler systems as examples. The Schr{\textbackslash}"odinger equation for the corresponding quantum statistical ensemble is described in terms of the Hamilton-Jacobi formalism. We consider both the original dynamical system in the position space and the conjugate dynamical system corresponding to the momentum space. Such simultaneous consideration of mutually complementary position and momentum frameworks provides a deeper understanding of the nature of chaotic behavior in dynamical systems. We have shown that the new formalism provides a significant simplification of the Lyapunov exponents calculations. From the point of view of quantum optics, the Lorenz and R{\textbackslash}"ossler systems correspond to three modes of a quantized electromagnetic field in a medium with cubic nonlinearity. From the computational point of view, the new formalism provides a basis for the analysis of complex dynamical systems using quantum computers.},
  archiveprefix = {arXiv},
  keywords = {Nonlinear Sciences - Chaotic Dynamics,Quantum Physics},
  file = {/home/elessar/Zotero/storage/H5GMPRXY/Bogdanov and Bogdanova - 2014 - The study of Lorenz and Rössler strange attractors by means of quantum theory.pdf;/home/elessar/Zotero/storage/NNEAUNYR/1412.html}
}

@article{bogomolnyQuantumChaoticDynamics1996,
  title = {Quantum Chaotic Dynamics and Random Polynomials},
  author = {Bogomolny, E. and Bohigas, O. and Leboeuf, P.},
  year = {1996},
  month = dec,
  journal = {Journal of Statistical Physics},
  volume = {85},
  number = {5-6},
  pages = {639--679},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {0022-4715, 1572-9613},
  doi = {10.1007/bf02199359},
  urldate = {2025-07-15},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/home/elessar/Zotero/storage/T8RKBYL6/Bogomolny et al. - 1996 - Quantum chaotic dynamics and random polynomials.pdf}
}

@article{bolltExplainingSurprisingSuccess2021,
  title = {On Explaining the Surprising Success of Reservoir Computing Forecaster of Chaos? {{The}} Universal Machine Learning Dynamical System with Contrast to {{VAR}} and {{DMD}}},
  shorttitle = {On Explaining the Surprising Success of Reservoir Computing Forecaster of Chaos?},
  author = {Bollt, Erik},
  year = {2021},
  month = jan,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {31},
  number = {1},
  pages = {013108},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0024890},
  urldate = {2025-08-12},
  abstract = {Machine learning has become a widely popular and successful paradigm, especially in data-driven science and engineering. A major application problem is data-driven forecasting of future states from a complex dynamical system. Artificial neural networks have evolved as a clear leader among many machine learning approaches, and recurrent neural networks are considered to be particularly well suited for forecasting dynamical systems. In this setting, the echo-state networks or reservoir computers (RCs) have emerged for their simplicity and computational complexity advantages. Instead of a fully trained network, an RC trains only readout weights by a simple, efficient least squares method. What is perhaps quite surprising is that nonetheless, an RC succeeds in making high quality forecasts, competitively with more intensively trained methods, even if not the leader. There remains an unanswered question as to why and how an RC works at all despite randomly selected weights. To this end, this work analyzes a further simplified RC, where the internal activation function is an identity function. Our simplification is not presented for the sake of tuning or improving an RC, but rather for the sake of analysis of what we take to be the surprise being not that it does not work better, but that such random methods work at all. We explicitly connect the RC with linear activation and linear readout to well developed time-series literature on vector autoregressive (VAR) averages that includes theorems on representability through the Wold theorem, which already performs reasonably for short-term forecasts. In the case of a linear activation and now popular quadratic readout RC, we explicitly connect to a nonlinear VAR, which performs quite well. Furthermore, we associate this paradigm to the now widely popular dynamic mode decomposition; thus, these three are in a sense different faces of the same thing. We illustrate our observations in terms of popular benchmark examples including Mackey--Glass differential delay equations and the Lorenz63 system.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/J59JRSRI/Bollt - 2021 - On explaining the surprising success of reservoir computing forecaster of chaos The universal machi.pdf}
}

@inproceedings{borensteinNoFreeLunch,
  title = {No {{Free Lunch}}, {{Kolmogorov Complexity}} and the {{Information Landscape}}},
  booktitle = {2005 {{IEEE Congress}} on {{Evolutionary Computation}}},
  author = {Borenstein, Y. and Poli, R.},
  volume = {3},
  pages = {2784--2791},
  publisher = {IEEE},
  address = {Edinburgh, Scotland, UK},
  doi = {10.1109/cec.2005.1555044},
  urldate = {2025-07-25},
  abstract = {The permutation closure of a single function is the finest level of granularity at which a no-freelunch result can hold [1]. Using the information landscape framework which was introduced in [2], we are able to identify the unique properties of each closure. In particular, we associate each closure with the amount of information its members contain. This allows us to compute bounds on the expected performance of an algorithm on members of that closure. Moreover, we suggest a new way to measure the Kolmogorov complexity of a landscape. This allows us to associate each permutation closure with a particular Kolmogorov complexity.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/RWYNHF3X/Borenstein and Poli - No Free Lunch, Kolmogorov Complexity and the Information Landscape.pdf}
}

@article{borweinChallengesMathematicalComputing2001,
  title = {Challenges in Mathematical Computing},
  author = {Borwein, J.M. and Borwein, P.B.},
  year = {2001},
  journal = {Computing in Science \& Engineering},
  volume = {3},
  number = {3},
  pages = {48--53},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {1521-9615},
  doi = {10.1109/5992.919266},
  urldate = {2025-07-25},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/QI2D5TR6/Borwein and Borwein - 2001 - Challenges in mathematical computing.PDF}
}

@article{bourkeEntropyRatesFinitestate2005,
  title = {Entropy Rates and Finite-State Dimension},
  author = {Bourke, Chris and Hitchcock, John M. and Vinodchandran, N.V.},
  year = {2005},
  month = dec,
  journal = {Theoretical Computer Science},
  volume = {349},
  number = {3},
  pages = {392--406},
  publisher = {Elsevier BV},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2005.09.040},
  urldate = {2025-07-25},
  abstract = {The effective fractal dimensions at the polynomial-space level and above can all be equivalently defined as the C-entropy rate where C is the class of languages corresponding to the level of effectivization. For example, pspace-dimension is equivalent to the PSPACE-entropy rate.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/2CXF5FGZ/Bourke et al. - 2005 - Entropy rates and finite-state dimension.pdf}
}

@article{boydCorrelationpoweredInformationEngines2017,
  title = {Correlation-Powered {{Information Engines}} and the {{Thermodynamics}} of {{Self-Correction}}},
  author = {Boyd, Alexander B. and Mandal, Dibyendu and Crutchfield, James P.},
  year = {2017},
  month = jan,
  journal = {Physical Review E},
  volume = {95},
  number = {1},
  eprint = {1606.08506},
  primaryclass = {cond-mat},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.95.012152},
  urldate = {2025-07-15},
  abstract = {Information engines can use structured environments as a resource to generate work by randomizing ordered inputs and leveraging the increased Shannon entropy to transfer energy from a thermal reservoir to a work reservoir. We give a broadly applicable expression for the work production of an information engine, generally modeled as a memoryful channel that communicates inputs to outputs as it interacts with an evolving environment. The expression establishes that an information engine must have more than one memory state in order to leverage input environment correlations. To emphasize this functioning, we designed an information engine powered solely by temporal correlations and not by statistical biases, as employed by previous engines. Key to this is the engine's ability to synchronize---the engine automatically returns to a desired dynamical phase when thrown into an unwanted, dissipative phase by corruptions in the input---that is, by unanticipated environmental fluctuations. This self-correcting mechanism is robust up to a critical level of corruption, beyond which the system fails to act as an engine. We give explicit analytical expressions for both work and critical corruption level and summarize engine performance via a thermodynamic-function phase diagram over engine control parameters. The results reveal a new thermodynamic mechanism based on nonergodicity that underlies error correction as it operates to support resilient engineered and biological systems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Condensed Matter - Statistical Mechanics,Mathematics - Dynamical Systems,Mathematics - Information Theory,Nonlinear Sciences - Chaotic Dynamics,Physics - Biological Physics},
  file = {/home/elessar/Zotero/storage/F65U8UQX/Boyd et al. - 2017 - Correlation-powered Information Engines and the Thermodynamics of Self-Correction.pdf}
}

@article{bozhkovReservoirComputingEmotion2017,
  title = {Reservoir Computing for Emotion Valence Discrimination from {{EEG}} Signals},
  author = {Bozhkov, Lachezar and {Koprinkova-Hristova}, Petia and Georgieva, Petia},
  year = {2017},
  month = mar,
  journal = {Neurocomputing},
  volume = {231},
  pages = {28--40},
  issn = {09252312},
  doi = {10.1016/j.neucom.2016.03.108},
  urldate = {2025-03-12},
  abstract = {In this paper we propose a new approach for feature dimensionality reduction based on Reservoir Computing (Echo State Networks). The method is validated with EEG data to identify the common neural signatures based on which the positive and negative valence of human emotions across multiple subjects can be reliably discriminated. The key step in the proposed approach is the Intrinsic Plasticity (IP) adaptation of the reservoir states. Learning Echo State Networks (ESN) with IP maximizes the entropy of the distribution of reservoir vectors given static data as a fixed input, which is supposed to follow Gaussian distribution. The equilibrium reservoir vector is extracted for each static input vector by iterating updates of the reservoir vector until it converges. Standard classification and clustering models provided with selected combinations of reservoir neurons are ranked based on their discriminate performance. The IP tuned ESNs is more powerful technique to map the high dimensional input feature vector into a low dimensional representation and improve the emotion valence discrimination compared to classical ESNs and Deep Neural Encoders.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/73J2UYNI/Bozhkov et al. - 2017 - Reservoir computing for emotion valence discrimination from EEG signals.pdf}
}

@article{bradleyBasicPropertiesStrong2005,
  title = {Basic {{Properties}} of {{Strong Mixing Conditions}}. {{A Survey}} and {{Some Open Questions}}},
  author = {Bradley, Richard C.},
  year = {2005},
  month = jan,
  journal = {Probability Surveys},
  volume = {2},
  number = {none},
  eprint = {math/0511078},
  issn = {1549-5787},
  doi = {10.1214/154957805100000104},
  urldate = {2025-07-15},
  abstract = {This is an update of, and a supplement to, a 1986 survey paper by the author on basic properties of strong mixing conditions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Probability},
  file = {/home/elessar/Zotero/storage/CGN3EKRG/Bradley - 2005 - Basic Properties of Strong Mixing Conditions. A Survey and Some Open Questions.pdf}
}

@book{brownleeDeepLearningTime,
  title = {Deep {{Learning}} for {{Time Series Forecasting}}},
  author = {Brownlee, Jason},
  langid = {english},
  annotation = {GSCC: 0001261},
  file = {/home/elessar/Zotero/storage/8UPGR7H3/Brownlee - Deep Learning for Time Series Forecasting.pdf}
}

@article{bruknerErratumConceptualInadequacy2003,
  title = {Erratum: {{Conceptual}} Inadequacy of the {{Shannon}} Information in Quantum Measurements [{{Phys}}. {{Rev}}. {{A}}{\textbf{63}}, 022113 (2001)]},
  shorttitle = {Erratum},
  author = {Brukner, {\v C}aslav and Zeilinger, Anton},
  year = {2003},
  month = apr,
  journal = {Physical Review A},
  volume = {67},
  number = {4},
  publisher = {American Physical Society (APS)},
  issn = {1050-2947, 1094-1622},
  doi = {10.1103/physreva.67.049901},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/HFY598JG/Brukner and Zeilinger - 2003 - Erratum Conceptual inadequacy of the Shannon information in quantum measurements [Phys. Rev. A63.pdf}
}

@book{bruntonDataDrivenScienceEngineering2022,
  title = {Data-{{Driven Science}} and {{Engineering}}: {{Machine Learning}}, {{Dynamical Systems}}, and {{Control}}},
  shorttitle = {Data-{{Driven Science}} and {{Engineering}}},
  author = {Brunton, Steven L. and Kutz, J. Nathan},
  year = {2022},
  month = may,
  edition = {2},
  publisher = {Cambridge University Press},
  doi = {10.1017/9781009089517},
  urldate = {2025-08-12},
  abstract = {Data-driven discovery is revolutionizing how we model, predict, and control complex systems. Now with Python and MATLAB{\textregistered}, this textbook trains mathematical scientists and engineers for the next generation of scientific discovery by offering a broad overview of the growing intersection of data-driven methods, machine learning, applied optimization, and classical fields of engineering mathematics and mathematical physics. With a focus on integrating dynamical systems modeling and control with modern methods in applied machine learning, this text includes methods that were chosen for their relevance, simplicity, and generality. Topics range from introductory to research-level material, making it accessible to advanced undergraduate and beginning graduate students from the engineering and physical sciences. The second edition features new chapters on reinforcement learning and physics-informed machine learning, significant new sections throughout, and chapter exercises. Online supplementary material -- including lecture videos per section, homeworks, data, and code in MATLAB{\textregistered}, Python, Julia, and R -- available on databookuw.com.},
  copyright = {https://www.cambridge.org/core/terms},
  isbn = {978-1-009-08951-7 978-1-009-09848-9}
}

@article{budhirajaReservoirComputingApproach2021,
  title = {A Reservoir Computing Approach for Forecasting and Regenerating Both Dynamical and Time-Delay Controlled Financial System Behavior},
  author = {Budhiraja, Rajat and Kumar, Manish and Das, Mrinal K. and Bafila, Anil Singh and Singh, Sanjeev},
  editor = {Arora, Pankaj Kumar},
  year = {2021},
  month = feb,
  journal = {PLOS ONE},
  volume = {16},
  number = {2},
  pages = {e0246737},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0246737},
  urldate = {2023-05-30},
  abstract = {Significant research in reservoir computing over the past two decades has revived interest in recurrent neural networks. Owing to its ingrained capability of performing high-speed and low-cost computations this has become a panacea for multi-variate complex systems having non-linearity within their relationships. Modelling economic and financial trends has always been a challenging task owing to their volatile nature and no linear dependence on associated influencers. Prior studies aimed at effectively forecasting such financial systems, but, always left a visible room for optimization in terms of cost, speed and modelling complexities. Our work employs a reservoir computing approach complying to echo-state network principles, along with varying strengths of time-delayed feedback to model a complex financial system. The derived model is demonstrated to act robustly towards influence of trends and other fluctuating parameters by effectively forecasting long-term system behavior. Moreover, it also re-generates the financial system unknowns with a high degree of accuracy when only limited future data is available, thereby, becoming a reliable feeder for any long-term decision making or policy formulations.},
  langid = {english},
  annotation = {GSCC: 0000013},
  file = {/home/elessar/Zotero/storage/UJE3Z994/Budhiraja et al. - 2021 - A reservoir computing approach for forecasting and.pdf}
}

@article{budrikis25YearsSmallworld2023,
  title = {25 Years of Small-World Network Theory},
  author = {Budrikis, Zoe},
  year = {2023},
  month = aug,
  journal = {Nature Reviews Physics},
  volume = {5},
  number = {8},
  pages = {440--440},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {2522-5820},
  doi = {10.1038/s42254-023-00628-6},
  urldate = {2025-07-15},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  langid = {english},
  file = {/home/elessar/Zotero/storage/XMAKIMY5/Budrikis - 2023 - 25 years of small-world network theory.pdf}
}

@article{buehnerTighterBoundEcho2006,
  title = {A Tighter Bound for the Echo State Property},
  author = {Buehner, M. and Young, P.},
  year = {2006},
  month = may,
  journal = {IEEE Transactions on Neural Networks},
  volume = {17},
  number = {3},
  pages = {820--824},
  issn = {1045-9227, 1941-0093},
  doi = {10.1109/TNN.2006.872357},
  urldate = {2025-08-12},
  abstract = {This letter provides a brief explanation of echo state networks and provides a rigorous bound for guaranteeing asymptotic stability of these networks. The stability bounds presented here could aid in the design of echo state networks that would be applicable to control applications where stability is required.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/7ZTLWEFF/Buehner and Young - 2006 - A tighter bound for the echo state property.pdf}
}

@article{buonomanoTemporalInformationTransformed1995,
  title = {Temporal {{Information Transformed}} into a {{Spatial Code}} by a {{Neural Network}} with {{Realistic Properties}}},
  author = {Buonomano, Dean V. and Merzenich, Michael M.},
  year = {1995},
  month = feb,
  journal = {Science},
  volume = {267},
  number = {5200},
  pages = {1028--1030},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.7863330},
  urldate = {2024-01-10},
  langid = {english},
  annotation = {GSCC: 0000598},
  file = {/home/elessar/Zotero/storage/HGP7FY9V/buonomano1995.pdf.pdf}
}

@article{burginAlgorithmicComplexityRecursive2004,
  title = {Algorithmic Complexity of Recursive and Inductive Algorithms},
  author = {Burgin, Mark},
  year = {2004},
  month = jun,
  journal = {Theoretical Computer Science},
  volume = {317},
  number = {1-3},
  pages = {31--60},
  publisher = {Elsevier BV},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2003.12.003},
  urldate = {2025-07-15},
  abstract = {The main goal of this paper is to compare recursive algorithms such as Turing machines with such super-recursive algorithms as inductive Turing machines. This comparison is made in a general setting of dual complexity measures such as Kolmogorov or algorithmic complexity. To make adequate comparison, we reconsider the standard axiomatic approach to complexity of algorithms. The new approach allows us to achieve a more adequate representation of static system complexity in the axiomatic context. It is demonstrated that for solving many problems inductive Turing machines have much lower complexity than Turing machines and other recursive algorithms. Thus, inductive Turing machines are not only more powerful, but also more e2cient than Turing machines.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/BCCNSCIH/Burgin - 2004 - Algorithmic complexity of recursive and inductive algorithms.PDF}
}

@article{butcherExtendingReservoirComputing2010,
  title = {Extending Reservoir Computing with Random Static Projections: A Hybrid between Extreme Learning and {{RC}}},
  author = {Butcher, John and Verstraeten, David and Schrauwen, Benjamin and Day, Charles and Haycock, Peter},
  year = {2010},
  journal = {Computational Intelligence},
  abstract = {Reservoir Computing is a relatively new paradigm in the field of neural networks that has shown promise in applications where traditional recurrent neural networks have performed poorly. The main advantage of using reservoirs is that only the output weights are trained, reducing computational requirements significantly. There is a trade-off, however, between the amount of memory a reservoir can possess and its capability of mapping data into a highly non-linear transformation space. A new, hybrid architecture, combining a reservoir with an extreme learning machine, is presented which overcomes this trade-off, whose performance is demonstrated on a 4th order polynomial modelling task and an isolated spoken digit recognition task.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/UGJRMLEN/Butcher et al. - 2010 - Extending reservoir computing with random static projections a hybrid between extreme learning and.pdf}
}

@article{butcherReservoirComputingExtreme2013,
  title = {Reservoir Computing and Extreme Learning Machines for Non-Linear Time-Series Data Analysis},
  author = {Butcher, J.B. and Verstraeten, D. and Schrauwen, B. and Day, C.R. and Haycock, P.W.},
  year = {2013},
  month = feb,
  journal = {Neural Networks},
  volume = {38},
  pages = {76--89},
  issn = {08936080},
  doi = {10.1016/j.neunet.2012.11.011},
  urldate = {2025-03-07},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/KISKJ4F8/Butcher et al. - 2013 - Reservoir computing and extreme learning machines for non-linear time-series data analysis.pdf}
}

@article{caludeEntropicMeasuresMarkov2002,
  title = {Entropic Measures, {{Markov}} Information Sources and Complexity},
  author = {Calude, Cristian S. and Dumitrescu, Monica},
  year = {2002},
  month = nov,
  journal = {Applied Mathematics and Computation},
  volume = {132},
  number = {2-3},
  pages = {369--384},
  publisher = {Elsevier BV},
  issn = {0096-3003},
  doi = {10.1016/s0096-3003(01)00199-0},
  urldate = {2025-07-25},
  abstract = {The concept of entropy plays a major part in communication theory. The Shannon entropy is a measure of uncertainty with respect to a priori probability distribution. In algorithmic information theory the information content of a message is measured in terms of the size in bits of the smallest program for computing that message. This paper discusses the classical entropy and entropy rate for discrete or continuous Markov sources, with finite or continuous alphabets, and their relations to program-size complexity and algorithmic probability. The accent is on ideas, constructions and results; no proofs will be given.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/GMXB3Y9J/Calude and Dumitrescu - 2002 - Entropic measures, Markov information sources and complexity.pdf}
}

@article{caludeFiniteStateComplexitySize2010,
  title = {Finite-{{State Complexity}} and the {{Size}} of {{Transducers}}},
  author = {Calude, Cristian and Salomaa, Kai and Roblot, Tania},
  year = {2010},
  month = aug,
  journal = {Electronic Proceedings in Theoretical Computer Science},
  volume = {31},
  pages = {38--47},
  publisher = {Open Publishing Association},
  issn = {2075-2180},
  doi = {10.4204/eptcs.31.6},
  urldate = {2025-07-25},
  langid = {english},
  file = {/home/elessar/Zotero/storage/KCG2TQZQ/Calude et al. - 2010 - Finite-State Complexity and the Size of Transducers.pdf}
}

@article{caludePartialRandomness2006,
  title = {On Partial Randomness},
  author = {Calude, Cristian S. and Staiger, Ludwig and Terwijn, Sebastiaan A.},
  year = {2006},
  month = mar,
  journal = {Annals of Pure and Applied Logic},
  volume = {138},
  number = {1-3},
  pages = {20--30},
  publisher = {Elsevier BV},
  issn = {0168-0072},
  doi = {10.1016/j.apal.2005.06.004},
  urldate = {2025-07-25},
  abstract = {If x = x1x2 {$\cdot$} {$\cdot$} {$\cdot$} xn {$\cdot$} {$\cdot$} {$\cdot$} is a random sequence, then the sequence y = 0x10x2 {$\cdot$} {$\cdot$} {$\cdot$} 0xn {$\cdot$} {$\cdot$} {$\cdot$} is clearly not random; however, y seems to be ``about half random''. L. Staiger [Kolmogorov complexity and Hausdorff dimension, Inform. and Comput. 103 (1993) 159--194 and A tight upper bound on Kolmogorov complexity and uniformly optimal prediction, Theory Comput. Syst. 31 (1998) 215--229] and K. Tadaki [A generalisation of Chaitin's halting probability {\textohm} and halting self-similar sets, Hokkaido Math. J. 31 (2002) 219--253] have studied the degree of randomness of sequences or reals by measuring their ``degree of compression''. This line of study leads to various definitions of partial randomness. In this paper we explore some relations between these definitions. Among other results we obtain a characterisation of {$\Sigma$}1-dimension (as defined by Schnorr and Lutz in terms of martingales) in terms of strong Martin-L{\"o}f {$\varepsilon$}-tests (a variant of Martin-L{\"o}f tests), and we show that {$\varepsilon$}-randomness for {$\varepsilon$} {$\in$} (0, 1) is different (and more difficult to study) than the classical 1-randomness. {\copyright} 2005 Elsevier B.V. All rights reserved.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/ZD3P939P/Calude et al. - 2006 - On partial randomness.pdf}
}

@article{cancelliereOCRePOptimallyConditioned2015,
  title = {{{OCReP}}: {{An Optimally Conditioned Regularization}} for Pseudoinversion Based Neural Training},
  shorttitle = {{{OCReP}}},
  author = {Cancelliere, Rossella and Gai, Mario and Gallinari, Patrick and Rubini, Luca},
  year = {2015},
  month = nov,
  journal = {Neural Networks},
  volume = {71},
  pages = {76--87},
  issn = {08936080},
  doi = {10.1016/j.neunet.2015.07.015},
  urldate = {2025-03-20},
  abstract = {In this paper we consider the training of single hidden layer neural networks by pseudoinversion, which, in spite of its popularity, is sometimes affected by numerical instability issues. Regularization is known to be effective in such cases, so that we introduce, in the framework of Tikhonov regularization, a matricial reformulation of the problem which allows us to use the condition number as a diagnostic tool for identification of instability. By imposing well-conditioning requirements on the relevant matrices, our theoretical analysis allows the identification of an optimal value for the regularization parameter from the standpoint of stability. We compare with the value derived by cross-validation for overfitting control and optimization of the generalization performance. We test our method for both regression and classification tasks. The proposed method is quite effective in terms of predictivity, often with some improvement on performance with respect to the reference cases considered. This approach, due to analytical determination of the regularization parameter, dramatically reduces the computational load required by many other techniques.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/ZJQZ589M/Cancelliere et al. - 2015 - OCReP An Optimally Conditioned Regularization for pseudoinversion based neural training.pdf}
}

@book{carlossanchezAnalisisMatematico1982,
  title = {{Analisis Matematico I}},
  author = {{Carlos Sanchez}},
  year = {1982},
  publisher = {Pueblo y Educacion},
  address = {Cuba},
  langid = {spanish},
  file = {/home/elessar/Zotero/storage/Y8V6WCV4/Análisis_Matemático-Tomo_1.pdf;/home/elessar/Zotero/storage/YCWYAGMR/Análisis_Matemático-Tomo_2 (Parte 1).pdf;/home/elessar/Zotero/storage/YRUMUXFV/Análisis_Matemático-Tomo_2 (Parte 2).pdf}
}

@inproceedings{carmichaelModDeepESNModularDeep2018,
  title = {Mod-{{DeepESN}}: {{Modular Deep Echo State Network}}},
  shorttitle = {Mod-{{DeepESN}}},
  booktitle = {2018 {{Conference}} on {{Cognitive Computational Neuroscience}}},
  author = {Carmichael, Zachariah and Syed, Humza and Burtner, Stuart and Kudithipudi, Dhireesha},
  year = {2018},
  publisher = {Cognitive Computational Neuroscience},
  address = {Philadelphia, Pennsylvania, USA},
  doi = {10.32470/CCN.2018.1239-0},
  urldate = {2024-11-26},
  abstract = {Neuro-inspired recurrent neural network algorithms, such as echo state networks, are computationally lightweight and thereby map well onto untethered devices. The baseline echo state network algorithms are shown to be efficient at solving small-scale spatio-temporal problems. However, they underperform for complex tasks that are characterized by multi-scale structures. In this research, an intrinsic plasticity-infused modular deep echo state network architecture is proposed to solve complex and multiple timescale temporal tasks. It outperforms stateof-the-art for time series prediction tasks.},
  langid = {english},
  annotation = {GSCC: 0000020},
  file = {/home/elessar/Zotero/storage/AD8DVN4I/Carmichael et al. - 2018 - Mod-DeepESN Modular Deep Echo State Network.pdf}
}

@article{carrollMutualInformationEdge,
  title = {Mutual {{Information}} and the {{Edge}} of {{Chaos}} in {{Reservoir Computers}}},
  author = {Carroll, T L},
  langid = {english},
  annotation = {GSCC: 0000005},
  file = {/home/elessar/Zotero/storage/YAJKCET9/Carroll - Mutual Information and the Edge of Chaos in Reserv.pdf}
}

@article{carrollNetworkStructureEffects2019,
  title = {Network Structure Effects in Reservoir Computers},
  author = {Carroll, T. L. and Pecora, L. M.},
  year = {2019},
  month = aug,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {29},
  number = {8},
  pages = {083130},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.5097686},
  urldate = {2025-08-12},
  abstract = {A reservoir computer is a complex nonlinear dynamical system that has been shown to be useful for solving certain problems, such as prediction of chaotic signals, speech recognition, or control of robotic systems. Typically, a reservoir computer is constructed by connecting a large number of nonlinear nodes in a network, driving the nodes with an input signal and using the node outputs to fit a training signal. In this work, we set up reservoirs where the edges (or connections) between all the network nodes are either +1 or 0 and proceed to alter the network structure by flipping some of these edges from +1 to -1. We use this simple network because it turns out to be easy to characterize; we may use the fraction of edges flipped as a measure of how much we have altered the network. In some cases, the network can be rearranged in a finite number of ways without changing its structure; these rearrangements are symmetries of the network, and the number of symmetries is also useful for characterizing the network. We find that changing the number of edges flipped in the network changes the rank of the covariance of a matrix consisting of the time series from the different nodes in the network and speculate that this rank is important for understanding the reservoir computer performance.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/BNHH7U3X/Carroll and Pecora - 2019 - Network structure effects in reservoir computers.pdf}
}

@article{carrollReservoirComputersWork2020,
  title = {Do {{Reservoir Computers Work Best}} at the {{Edge}} of {{Chaos}}?},
  author = {Carroll, Thomas L.},
  year = {2020},
  month = dec,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {30},
  number = {12},
  eprint = {2012.01409},
  primaryclass = {nlin},
  pages = {121109},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0038163},
  urldate = {2023-05-30},
  abstract = {It has been demonstrated that cellular automata had the highest computational capacity at the edge of chaos, the parameter at which their behavior transitioned from ordered to chaotic. This same concept has been applied to reservoir computers; a number of researchers have stated that the highest computational capacity for a reservoir computer is at the edge of chaos, although others have suggested that this rule is not universally true. Because many reservoir computers do not show chaotic behavior but merely become unstable, it is felt that a more accurate term for this instability transition is the "edge of stability"Here I find two examples where the computational capacity of a reservoir computer decreases as the edge of stability is approached; in one case, because generalized synchronization breaks down, and in the other case because the reservoir computer is a poor match to the problem being solved. The edge of stability as an optimal operating point for a reservoir computer is not in general true, although it may be true in some cases.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing,Nonlinear Sciences - Chaotic Dynamics},
  annotation = {GSCC: 0000060},
  file = {/home/elessar/Zotero/storage/LD9T5PWX/Carroll - 2020 - Do Reservoir Computers Work Best at the Edge of Ch.pdf}
}

@article{carterIntroductionInformationTheory,
  title = {An Introduction to Information Theory and Entropy},
  author = {Carter, Tom},
  langid = {english},
  file = {/home/elessar/Zotero/storage/673CNBN4/Carter - An introduction to information theory and entropy.PDF}
}

@misc{casanovaEnsembleReservoirComputing2023,
  title = {Ensemble {{Reservoir Computing}} for {{Dynamical Systems}}: {{Prediction}} of {{Phase-Space Stable Region}} for {{Hadron Storage Rings}}},
  shorttitle = {Ensemble {{Reservoir Computing}} for {{Dynamical Systems}}},
  author = {Casanova, Maxime and Dalena, Barbara and Bonaventura, Luca and Giovannozzi, Massimo},
  year = {2023},
  month = jan,
  number = {arXiv:2301.06786},
  eprint = {2301.06786},
  primaryclass = {physics},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.06786},
  urldate = {2024-11-26},
  abstract = {We investigate the ability of an ensemble reservoir computing approach to predict the long-term behaviour of the phase-space region in which the motion of charged particles in hadron storage rings is bounded, the so-called dynamic aperture. Currently, the calculation of the phase-space stability region of hadron storage rings is performed through direct computer simulations, which are resource- and time-intensive processes. Echo State Networks (ESN) are a class of recurrent neural networks that are computationally effective, since they avoid backpropagation and require only cross-validation. Furthermore, they have been proven to be universal approximants of dynamical systems. In this paper, we present the performance reached by ESN based on an ensemble approach for the prediction of the phase-space stability region and compare it with analytical scaling laws based on the stability-time estimate of the Nekhoroshev theorem for Hamiltonian systems. We observe that the proposed ESN approach is capable of effectively predicting the time evolution of the extent of the dynamic aperture, improving the predictions by analytical scaling laws, thus providing an efficient surrogate model.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Physics - Accelerator Physics},
  file = {/home/elessar/Zotero/storage/RD69CFA5/Casanova et al. - 2023 - Ensemble Reservoir Computing for Dynamical Systems Prediction of Phase-Space Stable Region for Hadr.pdf}
}

@misc{castellanosUnbiasedEstimationCoefficient2024,
  title = {Unbiased Estimation of the Coefficient of Determination in Linear Models: An Application to {{fMRI}} Encoding Model Comparison},
  shorttitle = {Unbiased Estimation of the Coefficient of Determination in Linear Models},
  author = {Castellanos, Agustin Lage and De Martino, Federico and Valente, Giancarlo},
  year = {2024},
  month = mar,
  doi = {10.1101/2024.03.04.583270},
  urldate = {2024-04-30},
  abstract = {Neuroscientific investigation has greatly benefited from the combination of functional Magnetic Resonance Imaging (fMRI) with linearized encoding, which allows to validate and compare computational models of neural activity based on neuroimaging data. In linearized encoding, a multidimensional feature space, usually obtained from a computational model applied to the stimuli, is related to the measured brain activity. This is often done by mapping such space to a dataset (training data, or in-sample), and validating the mapping on a separate dataset (test data, or out-of-sample), to avoid overfitting. When comparing models, the one with the highest explained variance on the test data, as indicated by the coefficient of determination (R2), is the one that better reflects the neural computations performed by the brain. An implicit assumption underlying this procedure is that the out-ofsample R2 is an unbiased estimator of the explanatory power of a computational model in the population of stimuli, and can therefore be safely used to compare models. In this work, we show that this is not the case, as the out-of-sample R2 has a negative bias, related to the amount of overfitting in the training data. This phenomenon has dramatic implications for model comparison when models of different dimensionalities are compared. To this aim, we develop an analytical framework that allows us to evaluate and correct biases in both in- and out-of-sample R2, with and without L2 regularization. Our proposed approach yields unbiased estimators of the population R2, thus enabling a valid model comparison. We validate it through illustrative simulations and with an application to a large public fMRI dataset.},
  langid = {english},
  annotation = {GSCC: 0000000},
  file = {/home/elessar/Zotero/storage/MMGHJBPC/Castellanos et al. - 2024 - Unbiased estimation of the coefficient of determin.pdf}
}

@article{catsigerasTEORIACUALITATIVAECUACIONES,
  title = {{TEORIA CUALITATIVA DE LAS ECUACIONES DIFERENCIALES}},
  author = {Catsigeras, Eleonora},
  langid = {spanish},
  file = {/home/elessar/Zotero/storage/WY4ADMW3/Catsigeras - TEORIA CUALITATIVA DE LAS ECUACIONES DIFERENCIALES.pdf}
}

@article{ceniResidualEchoState2024,
  title = {Residual {{Echo State Networks}}: {{Residual}} Recurrent Neural Networks with Stable Dynamics and Fast Learning},
  shorttitle = {Residual {{Echo State Networks}}},
  author = {Ceni, Andrea and Gallicchio, Claudio},
  year = {2024},
  month = sep,
  journal = {Neurocomputing},
  volume = {597},
  pages = {127966},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2024.127966},
  urldate = {2025-08-21},
  abstract = {Residual connections have been established as a staple for modern deep learning architectures. Most of their applications are cast towards feedforward computing. In this paper, we study the architectural bias of residual connections in the context of recurrent neural networks (RNNs), specifically in the temporal dimension. We frame our discussion from the perspective of Reservoir Computing and dynamical system theory, focusing on important aspects of neural computation like memory capacity, long-term information processing, stability, and nonlinear computation capability. Experiments corroborate the striking advantage brought by temporal residual connections for a plethora of different time series processing tasks, comprehending memory-based, forecasting, and classification problems.},
  keywords = {Echo state networks,Recurrent neural networks,Reservoir Computing,Residual networks},
  file = {/home/elessar/Zotero/storage/ARCXNNKV/Ceni and Gallicchio - 2024 - Residual Echo State Networks Residual recurrent neural networks with stable dynamics and fast learn.pdf;/home/elessar/Zotero/storage/7VMRQ4IJ/S0925231224007379.html}
}

@misc{changReinforcementLearningConvolutional2019,
  title = {Reinforcement {{Learning}} with {{Convolutional Reservoir Computing}}},
  author = {Chang, Hanten and Futagami, Katsuya},
  year = {2019},
  month = dec,
  number = {arXiv:1912.04161},
  eprint = {1912.04161},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.04161},
  urldate = {2025-03-20},
  abstract = {Recently, reinforcement learning models have achieved great success, mastering complex tasks such as Go and other games with higher scores than human players. Many of these models store considerable data on the tasks and achieve high performance by extracting visual and time-series features using convolutional neural networks (CNNs) and recurrent neural networks, respectively. However, these networks have very high computational costs because they need to be trained by repeatedly using the stored data. In this study, we propose a novel practical approach called reinforcement learning with convolutional reservoir computing (RCRC) model. The RCRC model uses a fixed random-weight CNN and a reservoir computing model to extract visual and time-series features. Using these extracted features, it decides actions with an evolution strategy method. Thereby, the RCRC model has several desirable features: (1) there is no need to train the feature extractor, (2) there is no need to store training data, (3) it can take a wide range of actions, and (4) there is only a single task-dependent weight parameter to be trained. Furthermore, we show the RCRC model can solve multiple reinforcement learning tasks with a completely identical feature extractor.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/elessar/Zotero/storage/7Y2FJHUJ/Chang and Futagami - 2019 - Reinforcement Learning with Convolutional Reservoir Computing.pdf}
}

@misc{ChatGPT,
  title = {{{ChatGPT}}},
  urldate = {2024-10-24},
  abstract = {A conversational AI system that listens, learns, and challenges},
  howpublished = {https://chatgpt.com},
  langid = {american},
  file = {/home/elessar/Zotero/storage/DPVHDIUG/6718e9aa-c748-8013-838f-7b63a09356f3.html}
}

@article{chazottesLargeDeviationsEmpirical2005,
  title = {Large Deviations for Empirical Entropies of {{Gibbsian}} Sources},
  author = {Chazottes, J.-R. and Gabrielli, D.},
  year = {2005},
  month = nov,
  journal = {Nonlinearity},
  volume = {18},
  number = {6},
  eprint = {math/0406083},
  pages = {2545--2563},
  issn = {0951-7715, 1361-6544},
  doi = {10.1088/0951-7715/18/6/007},
  urldate = {2025-07-25},
  abstract = {The entropy of an ergodic finite-alphabet process can be computed from a single typical sample path x1n using the entropy of the k-block empirical probability and letting k grow with n roughly like log n. We further assume that the distribution of the process is a g-measure. We prove large deviation principles for conditional, non-conditional and relative k(n)-block empirical entropies.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Dynamical Systems,Mathematics - Probability},
  file = {/home/elessar/Zotero/storage/IF3B765I/Chazottes and Gabrielli - 2005 - Large deviations for empirical entropies of Gibbsian sources.pdf}
}

@inproceedings{chedidKnowledgeInformationContents2007,
  title = {Knowledge {{Versus Information Contents}}},
  booktitle = {2007 {{IEEE}}/{{ACS International Conference}} on {{Computer Systems}} and {{Applications}}},
  author = {Chedid, Fouad B.},
  year = {2007},
  month = may,
  pages = {316--323},
  publisher = {IEEE},
  address = {Amman, Jordan},
  doi = {10.1109/aiccsa.2007.370900},
  urldate = {2025-07-25},
  abstract = {While the information contents of a binary string x can be measured by its prefix Kolmogorov complexity K(x), it is not clear how to measure the knowledge stored in x. In this paper, we argue that the knowledge contained by x is relative to the hypothesis assumed to explain x. So, if H is a hypothesis for x, we suggest to measure the knowledge in x by K(H). The absolute knowledge in x is K(H0), where H0 is a simplest model capable of explaining x. Using Bayes' rule and Solomonoff 's universal distribution, we obtain K(x) = K(H) + K(x {\textbar} H). We interpret K(H) as the knowledge part in x and K(x {\textbar} H) as the random aspect (accidental information) in x relative to H. Furthermore, we provide a simple explanation forKolmogorov's innovative proposal for a non-probabilistic approach to statistics and model selection. We observe that the expression used by Kolmogorov to describe positively probabilistically random objects is a rewrite of Bayes' rule combined with approximations based on Solomonoff 's universal distribution. We revisit the role of algorithmic sufficient statistic in the theory of hypothesis selection and prediction, especially as related to Kolmogorov's structure function and non-stochastic objects. Also, We derive a fundamental result relating Kolmogorov's structure function and two of its variants.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/V662SEZB/Chedid - 2007 - Knowledge Versus Information Contents.pdf}
}

@article{chelaniComplexityAnalysisCO2011,
  title = {Complexity Analysis of {{CO}} Concentrations at a Traffic Site in {{Delhi}}},
  author = {Chelani, A.B.},
  year = {2011},
  month = jan,
  journal = {Transportation Research Part D: Transport and Environment},
  volume = {16},
  number = {1},
  pages = {57--60},
  publisher = {Elsevier BV},
  issn = {1361-9209},
  doi = {10.1016/j.trd.2010.08.008},
  urldate = {2025-07-15},
  abstract = {The Lempel--Ziv complexity measure is used to examine the complexity in carbon monoxide concentration emissions over time. Rescaled range analysis and dispersion analysis is also carried out to confirm the findings. For this, time series of carbon monoxide concentrations observed between 2000 and 2009 at a traffic site in Delhi are used to examine separately the entire time series, and the series by years and months. The presence of complexity is observed in carbon monoxide concentrations over the period. The analysis showed the long-range correlations up to 15 months. The Lempel--Ziv complexity of time series over different years however indicates complexity in all the years except 2000, 2001 and 2006, reflecting the probable impact of control measures in Delhi.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/PYX77TVR/Chelani - 2011 - Complexity analysis of CO concentrations at a traffic site in Delhi.pdf}
}

@article{chenAutoreservoirComputingMultistep2020,
  title = {Autoreservoir Computing for Multistep Ahead Prediction Based on the Spatiotemporal Information Transformation},
  author = {Chen, Pei and Liu, Rui and Aihara, Kazuyuki and Chen, Luonan},
  year = {2020},
  month = sep,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {4568},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-18381-0},
  urldate = {2024-02-13},
  abstract = {Abstract             We develop an auto-reservoir computing framework, Auto-Reservoir Neural Network (ARNN), to efficiently and accurately make multi-step-ahead predictions based on a short-term high-dimensional time series. Different from traditional reservoir computing whose reservoir is an external dynamical system irrelevant to the target system, ARNN directly transforms the observed high-dimensional dynamics as its reservoir, which maps the high-dimensional/spatial data to the future temporal values of a target variable based on our spatiotemporal information (STI) transformation. Thus, the multi-step prediction of the target variable is achieved in an accurate and computationally efficient manner. ARNN is successfully applied to both representative models and real-world datasets, all of which show satisfactory performance in the multi-step-ahead prediction, even when the data are perturbed by noise and when the system is time-varying. Actually, such ARNN transformation equivalently expands the sample size and thus has great potential in practical applications in artificial intelligence and machine learning.},
  langid = {english},
  annotation = {GSCC: 0000107},
  file = {/home/elessar/Zotero/storage/46R8ERJS/Chen et al_2020_Autoreservoir computing for multistep ahead prediction based on the.pdf;/home/elessar/Zotero/storage/MI3H9RT6/Chen et al. - 2020 - Autoreservoir computing for multistep ahead predic.pdf}
}

@article{chenEchoStateNetwork2023,
  title = {Echo {{State Network With Probabilistic Regularization}} for {{Time Series Prediction}}},
  author = {Chen, Xiufang and Liu, Mei and Li, Shuai},
  year = {2023},
  month = aug,
  journal = {IEEE/CAA Journal of Automatica Sinica},
  volume = {10},
  number = {8},
  pages = {1743--1753},
  issn = {2329-9266, 2329-9274},
  doi = {10.1109/JAS.2023.123489},
  urldate = {2025-03-12},
  abstract = {Recent decades have witnessed a trend that the echo state network (ESN) is widely utilized in field of time series prediction due to its powerful computational abilities. However, most of the existing research on ESN is conducted under the assumption that data is free of noise or polluted by the Gaussian noise, which lacks robustness or even fails to solve real-world tasks. This work handles this issue by proposing a probabilistic regularized ESN (PRESN) with robustness guaranteed. Specifically, we design a novel objective function for minimizing both the mean and variance of modeling error, and then a scheme is derived for getting output weights of the PRESN. Furthermore, generalization performance, robustness, and unbiased estimation abilities of the PRESN are revealed by theoretical analyses. Finally, experiments on a benchmark dataset and two real-world datasets are conducted to verify the performance of the proposed PRESN. The source code is publicly available at https://github.com/LongJinlab/probabilistic-regularized-echo-state-network.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/LH6Y5V34/Chen et al. - 2023 - Echo State Network With Probabilistic Regularization for Time Series Prediction.pdf}
}

@article{chenLempelZivFactorization2008,
  title = {Lempel--{{Ziv Factorization Using Less Time}} \& {{Space}}},
  author = {Chen, Gang and Puglisi, Simon J. and Smyth, W. F.},
  year = {2008},
  month = jun,
  journal = {Mathematics in Computer Science},
  volume = {1},
  number = {4},
  pages = {605--623},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {1661-8270, 1661-8289},
  doi = {10.1007/s11786-007-0024-4},
  urldate = {2025-07-15},
  abstract = {For 30 years the Lempel--Ziv factorization LZx of a string x = x[1..n] has been a fundamental data structure of string processing, especially valuable for string compression and for computing all the repetitions (runs) in x. Traditionally the standard method for computing LZx was based on {$\Theta$}(n)-time (or, depending on the measure used, O(n log n)-time) processing of the suffix tree STx of x. Recently Abouelhoda et al. proposed an efficient Lempel--Ziv factorization algorithm based on an ``enhanced'' suffix array --that is, a suffix array SAx together with supporting data structures, principally an ``interval tree''. In this paper we introduce a collection of fast spaceefficient algorithms for LZ factorization, also based on suffix arrays, that in theory as well as in many practical circumstances are superior to those previously proposed; one family out of this collection achieves true {$\Theta$}(n)-time alphabet-independent processing in the worst case by avoiding tree structures altogether.},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/home/elessar/Zotero/storage/KJQREKHH/Chen et al. - 2008 - Lempel–Ziv Factorization Using Less Time & Space.pdf}
}

@article{chenProperChoiceHyperparameters2023,
  title = {Proper Choice of Hyperparameters in Reservoir Computing of Chaotic Maps},
  author = {Chen, Wei and Gao, Jian and Yan, Zixiang and Xiao, Jinghua},
  year = {2023},
  month = oct,
  journal = {Journal of Physics A: Mathematical and Theoretical},
  volume = {56},
  number = {41},
  pages = {415702},
  issn = {1751-8113, 1751-8121},
  doi = {10.1088/1751-8121/acfb54},
  urldate = {2025-08-12},
  abstract = {Abstract             Reservoir computing (RC) are powerful to learn and predict dynamical behaviors. However, it has been found that both the reservoir size and the hyperparameters can greatly affect the learning ability of RC on dynamical systems, the mechanism of which still remains unclear. This paper discusses the influence of hyperparameters of RC with different sizes of reservoir on learning typical chaotic maps. An analytic method is purposed to obtain the hyperparameters that can exhibit better learning ability of RC by analyzing high order derivatives of the error loss function. In the case of RC with one or two nodes, the well-performing hyperparameters are analytically obtained for learning the logistic map, which are consistent with numerical results. The analytic method also shows its ability in RC with multiple nodes to learn singer and sine chaotic maps. This work provides deeper insight in learning and predicting behaviors of RC as well as presents guidance for the selection of hyperparameters of RC to learn chaotic systems.}
}

@article{chenTimeSeriesReconstructing2022,
  title = {Time Series Reconstructing Using Calibrated Reservoir Computing},
  author = {Chen, Yeyuge and Qian, Yu and Cui, Xiaohua},
  year = {2022},
  month = sep,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {16318},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-20331-3},
  urldate = {2024-04-30},
  abstract = {Abstract             Reservoir computing, a new method of machine learning, has recently been used to predict the state evolution of various chaotic dynamic systems. It has significant advantages in terms of training cost and adjusted parameters; however, the prediction length is limited. For classic reservoir computing, the prediction length can only reach five to six Lyapunov times. Here, we modified the method of reservoir computing by adding feedback, continuous or discrete, to ``calibrate'' the input of the reservoir and then reconstruct the entire dynamic systems. The reconstruction length appreciably increased and the training length obviously decreased. The reconstructing of dynamical systems is studied in detail under this method. The reconstruction can be significantly improved both in length and accuracy. Additionally, we summarized the effect of different kinds of input feedback. The more it interacts with others in dynamical equations, the better the reconstructions. Nonlinear terms can reveal more information than linear terms once the interaction terms are equal. This method has proven effective via several classical chaotic systems. It can be superior to traditional reservoir computing in reconstruction, provides new hints in computing promotion, and may be used in some real applications.},
  langid = {english},
  annotation = {GSCC: 0000008},
  file = {/home/elessar/Zotero/storage/N9W7R836/Chen et al. - 2022 - Time series reconstructing using calibrated reserv.pdf}
}

@book{cholletDeepLearningPython2021,
  title = {Deep Learning with {{Python}}},
  author = {Chollet, Fran{\c c}ois},
  year = {2021},
  edition = {Second edition},
  publisher = {Manning Publications},
  address = {Shelter Island},
  abstract = {Recent innovations in deep learning unlock exciting new software capabilities like automated language translation, image recognition, and more. Deep learning is quickly becoming essential knowledge for every software developer, and modern tools like Keras and TensorFlow put it within your reach-- even if you have no background in mathematics or data science. This book shows you how to get started. "Deep learning with Python, second edition" introduces the field of deep learning using Python and the powerful Keras library. In this revised and expanded new edition, Keras creator Fran{\c c}ois Chollet offers insights for both novice and experienced machine learning practitioners. As you move through this book, you'll build your understanding through intuitive explanations, crisp illustrations, and clear examples. You'll quickly pick up the skills you need to start developing deep-learning applications.--},
  isbn = {978-1-61729-686-4},
  langid = {english},
  lccn = {QA76.73.P98 C465 2021},
  keywords = {Machine learning,Neural networks (Computer science),Python (Computer program language)},
  annotation = {GSCC: 0007037 \\
OCLC: on1289290141},
  file = {/home/elessar/Zotero/storage/B7P4EMAE/Chollet - 2021 - Deep learning with Python.pdf}
}

@misc{cholletMeasureIntelligence2019,
  title = {On the {{Measure}} of {{Intelligence}}},
  author = {Chollet, Fran{\c c}ois},
  year = {2019},
  month = nov,
  number = {arXiv:1911.01547},
  eprint = {1911.01547},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-30},
  abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks, such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to ``buy'' arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience, as critical pieces to be accounted for in characterizing intelligent systems. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a new benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  annotation = {GSCC: 0000585},
  file = {/home/elessar/Zotero/storage/NTJY4SN5/Chollet - 2019 - On the Measure of Intelligence.pdf}
}

@article{chouikhiPSObasedAnalysisEcho2017,
  title = {{{PSO-based}} Analysis of {{Echo State Network}} Parameters for Time Series Forecasting},
  author = {Chouikhi, Naima and Ammar, Boudour and Rokbani, Nizar and Alimi, Adel M.},
  year = {2017},
  month = jun,
  journal = {Applied Soft Computing},
  volume = {55},
  pages = {211--225},
  issn = {15684946},
  doi = {10.1016/j.asoc.2017.01.049},
  urldate = {2025-03-12},
  abstract = {Echo State Networks, ESNs, are standardly composed of additive units undergoing sigmoid function activation. They consist of a randomly recurrent neuronal infra-structure called reservoir. Coming up with a good reservoir depends mainly on picking up the right parameters for the network initialization. Human expertise as well as repeatedly tests may sometimes provide acceptable parameters. Nevertheless, they are non-guaranteed. On the other hand, optimization techniques based on evolutionary learning have proven their strong effectiveness in unscrambling optimal solutions in complex spaces. Particle swarm optimization (PSO) is one of the most popular continuous evolutionary algorithms. Throughout this paper, a PSO algorithm is associated to ESN to pre-train some fixed weights values within the network. Once the network's initial parameters are set, some untrained weights are selected for optimization. The new weights, already optimized, are re-squirted to the network which launches its normal training process. The performances of the network are a subject of the error and the time processing evaluation metrics. The testing results after PSO pre-training are compared to those of ESN without optimization and other existent approaches. The conceived approach is tested for time series prediction purpose on a set of benchmarks and real-life datasets. Experimental results show obvious enhancement of ESN learning results.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/3AWZ6ERB/Chouikhi et al. - 2017 - PSO-based analysis of Echo State Network parameters for time series forecasting.pdf}
}

@article{ciupercaComputationEstimationGeneralized2011,
  title = {Computation and {{Estimation}} of {{Generalized Entropy Rates}} for {{Denumerable Markov Chains}}},
  author = {Ciuperca, Gabriela and Girardin, Valerie and Lhote, Lo{\"i}ck},
  year = {2011},
  month = jul,
  journal = {IEEE Transactions on Information Theory},
  volume = {57},
  number = {7},
  pages = {4026--4034},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/tit.2011.2133710},
  urldate = {2025-07-24},
  abstract = {We study entropy rates of random sequences for general entropy functionals including the classical Shannon and R{\'e}nyi entropies and the more recent Tsallis and Sharma-Mittal ones. In the first part, we obtain an explicit formula for the entropy rate for a large class of entropy functionals, as soon as the process satisfies a regularity property known in dynamical systems theory as the quasi-power property. Independent and identically distributed sequence of random variables naturally satisfy this property. Markov chains are proven to satisfy it too, under simple explicit conditions on their transition probabilities. All the entropy rates under study are thus shown to be either infinite or zero except at a threshold where they are equal to Shannon or R{\'e}nyi entropy rates up to a multiplicative constant. In the second part, we focus on the estimation of the marginal generalized entropy and entropy rate for parametric Markov chains. Estimators with good asymptotic properties are built through a plug-in procedure using a maximum likelihood estimation of the parameter.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/ZPIQ7UX5/Ciuperca et al. - 2011 - Computation and Estimation of Generalized Entropy Rates for Denumerable Markov Chains.pdf}
}

@article{coccoTrajectoriesPhaseDiagrams2001,
  title = {Trajectories in {{Phase Diagrams}}, {{Growth Processes}}, and {{Computational Complexity}}: {{How Search Algorithms Solve}} the 3-{{Satisfiability Problem}}},
  shorttitle = {Trajectories in {{Phase Diagrams}}, {{Growth Processes}}, and {{Computational Complexity}}},
  author = {Cocco, Simona and Monasson, R{\'e}mi},
  year = {2001},
  month = feb,
  journal = {Physical Review Letters},
  volume = {86},
  number = {8},
  pages = {1654--1657},
  publisher = {American Physical Society (APS)},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/physrevlett.86.1654},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/YMJXEWUK/Cocco and Monasson - 2001 - Trajectories in Phase Diagrams, Growth Processes, and Computational Complexity How Search Algorithm.pdf}
}

@book{concepcionvaldescastroAnalisisFuncionesVarias20,
  title = {{Analisis de funciones de varias variables}},
  author = {{Concepcion Valdes Castro}},
  year = {20},
  publisher = {Varela},
  address = {La Habana},
  isbn = {978-959-07-0250-1 978-959-07-0437-6},
  langid = {spanish},
  file = {/home/elessar/Zotero/storage/HUY5EAWL/Pérez Gallardo - Derecho notarial.pdf}
}

@article{connorRecurrentNetworksNARMA,
  title = {Recurrent {{Networks}} and {{NARMA Modeling}}},
  author = {Connor, Jerome T and Atlas, Les E and Martin, Douglas R},
  abstract = {There exist large classes of time series, such as those with nonlinear moving average components, that are not well modeled by feedforward networks or linear models, but can be modeled by recurrent networks. We show that recurrent neural networks are a type of nonlinear autoregressive-moving average (N ARMA) model. Practical ability will be shown in the results of a competition sponsored by the Puget Sound Power and Light Company, where the recurrent networks gave the best performance on electric load forecasting.},
  langid = {english},
  annotation = {GSCC: 0000117},
  file = {/home/elessar/Zotero/storage/KG96EKV3/Connor et al. - Recurrent Networks and NARMA Modeling.pdf}
}

@article{conroySupercriticalBehaviorOrdered,
  title = {Supercritical Behavior of an Ordered Trajectory},
  author = {Conroy, Kathleen},
  langid = {english},
  file = {/home/elessar/Zotero/storage/CHPILEJA/Conroy - Supercritical behavior of an ordered trajectory.pdf}
}

@article{constantinescuLEMPELZIVCOMPLEXITY,
  title = {{{THE LEMPEL}}--{{ZIV COMPLEXITY OF FIXED POINTS OF MORPHISMS}}},
  author = {Constantinescu, Sorin and Ilie, Lucian},
  abstract = {The Lempel--Ziv complexity is a fundamental measure of complexity for words, closely connected with the famous LZ77 compression algorithm. We investigate this complexity measure for one of the most important families of infinite words in combinatorics, namely the fixed points of morphisms. We give a complete characterization of the complexity classes which are {$\Theta$}(1), {$\Theta$}(log n), and {$\Theta$}(n1/k), k {$\in$} N, k {$\geq$} 2, depending on the periodicity of the word and the growth function of the morphism. The relation with the well-known classification of Ehrenfeucht, Lee, Rozenberg, and Pansiot for factor complexity classes is also investigated. The two measures complete each other, giving an improved picture for the complexity of these infinite words.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/8N9QP76S/Constantinescu and Ilie - THE LEMPEL–ZIV COMPLEXITY OF FIXED POINTS OF MORPHISMS.pdf}
}

@article{corsoGraphNeuralNetworks2024,
  title = {Graph Neural Networks},
  author = {Corso, Gabriele and Stark, Hannes and Jegelka, Stefanie and Jaakkola, Tommi and Barzilay, Regina},
  year = {2024},
  month = mar,
  journal = {Nature Reviews Methods Primers},
  volume = {4},
  number = {1},
  pages = {17},
  issn = {2662-8449},
  doi = {10.1038/s43586-024-00294-7},
  urldate = {2025-03-20},
  langid = {english},
  file = {/home/elessar/Zotero/storage/4AXK5QI2/Corso et al. - 2024 - Graph neural networks.pdf}
}

@article{costaMultiscaleEntropyAnalysis2002,
  title = {Multiscale {{Entropy Analysis}} of {{Complex Physiologic Time Series}}},
  author = {Costa, Madalena and Goldberger, Ary L. and Peng, C.-K.},
  year = {2002},
  month = jul,
  journal = {Physical Review Letters},
  volume = {89},
  number = {6},
  publisher = {American Physical Society (APS)},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/physrevlett.89.068102},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/6RUJYZ8Y/Costa et al. - 2002 - Multiscale Entropy Analysis of Complex Physiologic Time Series.pdf}
}

@article{coverELEMENTSINFORMATIONTHEORY,
  title = {{{ELEMENTS OF INFORMATION THEORY}}},
  author = {Cover, Thomas M and Thomas, Joy A},
  langid = {english},
  file = {/home/elessar/Zotero/storage/CK7ZIQIG/Cover and Thomas - ELEMENTS OF INFORMATION THEORY.pdf}
}

@article{coverNearestNeighborPattern1967,
  title = {Nearest Neighbor Pattern Classification},
  author = {Cover, T. and Hart, P.},
  year = {1967},
  month = jan,
  journal = {IEEE Transactions on Information Theory},
  volume = {13},
  number = {1},
  pages = {21--27},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/tit.1967.1053964},
  urldate = {2025-07-25},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/VMMMD9DM/Cover and Hart - 1967 - Nearest neighbor pattern classification.pdf}
}

@article{crauelRandomAttractors,
  title = {Random Attractors},
  author = {Crauel, Hans and Debussche, Arnaud and Flandoli, Franco},
  langid = {english},
  file = {/home/elessar/Zotero/storage/INLUHBPE/Crauel et al. - Random attractors.pdf}
}

@inproceedings{crochemoreSimpleAlgorithmComputing2008,
  title = {A {{Simple Algorithm}} for {{Computing}} the {{Lempel Ziv Factorization}}},
  booktitle = {Data {{Compression Conference}} (Dcc 2008)},
  author = {Crochemore, Maxime and Ilie, Lucian and Smyth, W. F.},
  year = {2008},
  month = mar,
  pages = {482--488},
  publisher = {IEEE},
  address = {Snowbird, UT, USA},
  issn = {1068-0314},
  doi = {10.1109/dcc.2008.36},
  urldate = {2025-07-15},
  abstract = {We give a space-efficient simple algorithm for computing the Lempel--Ziv factorization of a string. For a string of length n over an integer alphabet, it runs in O(n) time independently of alphabet size and uses o(n) additional space.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/AWFQSQSA/Crochemore et al. - 2008 - A Simple Algorithm for Computing the Lempel Ziv Factorization.pdf}
}

@article{crutchfieldEvolutionEmergentComputation1995,
  title = {The Evolution of Emergent Computation.},
  author = {Crutchfield, J P and Mitchell, M},
  year = {1995},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {92},
  number = {23},
  pages = {10742--10746},
  publisher = {Proceedings of the National Academy of Sciences},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.92.23.10742},
  urldate = {2025-07-15},
  abstract = {A simple evolutionary process can discover sophisticated methods for emergent information processing in decentralized spatially extended systems. The mechanisms underlying the resulting emergent computation are explicated by a technique for analyzing particle-based logic embedded in pattern-forming systems. Understanding how globally coordinated computation can emerge in evolution is relevant both for the scientific understanding of natural information processing and for engineering new forms of parallel computing systems.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/U8DEFWFF/Crutchfield and Mitchell - 1995 - The evolution of emergent computation..pdf}
}

@misc{crutchfieldExactComplexitySpectral2013,
  title = {Exact {{Complexity}}: {{The Spectral Decomposition}} of {{Intrinsic Computation}}},
  shorttitle = {Exact {{Complexity}}},
  author = {Crutchfield, James P. and Ellison, Christopher J. and Riechers, Paul M.},
  year = {2013},
  month = sep,
  number = {arXiv:1309.3792},
  eprint = {1309.3792},
  primaryclass = {cond-mat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1309.3792},
  urldate = {2025-07-15},
  abstract = {We give exact formulae for a wide family of complexity measures that capture the organization of hidden nonlinear processes. The spectral decomposition of operator-valued functions leads to closedform expressions involving the full eigenvalue spectrum of the mixed-state presentation of a process's -machine causal-state dynamic. Measures include correlation functions, power spectra, past-future mutual information, transient and synchronization informations, and many others. As a result, a direct and complete analysis of intrinsic computation is now available for the temporal organization of finitary hidden Markov models and nonlinear dynamical systems with generating partitions and for the spatial organization in one-dimensional systems, including spin systems, cellular automata, and complex materials via chaotic crystallography.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Condensed Matter - Statistical Mechanics,Mathematics - Information Theory,Nonlinear Sciences - Cellular Automata and Lattice Gases,Nonlinear Sciences - Chaotic Dynamics},
  file = {/home/elessar/Zotero/storage/5F9PJ4FN/Crutchfield et al. - 2013 - Exact Complexity The Spectral Decomposition of Intrinsic Computation.pdf}
}

@misc{crutchfieldFuturePresent2010,
  title = {The {{Past}} and the {{Future}} in the {{Present}}},
  author = {Crutchfield, James P. and Ellison, Christopher J.},
  year = {2010},
  month = dec,
  number = {arXiv:1012.0356},
  eprint = {1012.0356},
  primaryclass = {nlin},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1012.0356},
  urldate = {2025-07-15},
  abstract = {We show how the shared information between the past and future---the excess entropy---derives from the components of directional information stored in the present---the predictive and retrodictive causal states. A detailed proof allows us to highlight a number of the subtle problems in estimation and analysis that impede accurate calculation of the excess entropy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Mathematics - Dynamical Systems,Mathematics - Information Theory,Mathematics - Statistics Theory,Nonlinear Sciences - Chaotic Dynamics,Statistics - Statistics Theory},
  file = {/home/elessar/Zotero/storage/67BSKRNZ/Crutchfield and Ellison - 2010 - The Past and the Future in the Present.pdf}
}

@article{crutchfieldIntrinsicQuantumComputation2008,
  title = {Intrinsic Quantum Computation},
  author = {Crutchfield, James P. and Wiesner, Karoline},
  year = {2008},
  month = jan,
  journal = {Physics Letters A},
  volume = {372},
  number = {4},
  pages = {375--380},
  publisher = {Elsevier BV},
  issn = {0375-9601},
  doi = {10.1016/j.physleta.2007.07.052},
  urldate = {2025-07-25},
  abstract = {We introduce ways to measure information storage in quantum systems, using a recently introduced computation-theoretic model that accounts for measurement effects. The first, the quantum excess entropy, quantifies the shared information between a quantum process's past and its future. The second, the quantum transient information, determines the difficulty with which an observer comes to know the internal state of a quantum process through measurements. We contrast these with von Neumann entropy and quantum entropy rate and provide a closed-form expression for the latter for the class of deterministic quantum processes.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/B8UY64I5/Crutchfield and Wiesner - 2008 - Intrinsic quantum computation.pdf}
}

@article{crutchfieldUnreconstructibleAnyRadius1992,
  title = {Unreconstructible at Any Radius},
  author = {Crutchfield, James P.},
  year = {1992},
  month = nov,
  journal = {Physics Letters A},
  volume = {171},
  number = {1-2},
  pages = {52--60},
  publisher = {Elsevier BV},
  issn = {0375-9601},
  doi = {10.1016/0375-9601(92)90132-6},
  urldate = {2025-07-15},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/KWD8X3QS/Crutchfield - 1992 - Unreconstructible at any radius.pdf}
}

@article{cuiArchitectureDynamicReservoir2012,
  title = {The Architecture of Dynamic Reservoir in the Echo State Network},
  author = {Cui, Hongyan and Liu, Xiang and Li, Lixiang},
  year = {2012},
  month = sep,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {22},
  number = {3},
  pages = {033127},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.4746765},
  urldate = {2023-06-05},
  langid = {english},
  annotation = {GSCC: 0000068},
  file = {/home/elessar/Zotero/storage/NR86K22M/Cui et al. - 2012 - The architecture of dynamic reservoir in the echo .pdf}
}

@article{cvitanovicFareyOrganizationFractional1985,
  title = {Farey {{Organization}} of the {{Fractional Hall Effect}}},
  author = {Cvitanovi{\'c}, Predrag},
  year = {1985},
  month = jan,
  journal = {Physica Scripta},
  volume = {T9},
  pages = {202--202},
  issn = {0031-8949, 1402-4896},
  doi = {10.1088/0031-8949/1985/T9/033},
  urldate = {2025-07-29},
  abstract = {We conjecture that the fractional quantized Hall effect Landau-level occupation factors are organized by the Haldane tree. This is a variant of the Farey tree, suitably refined to account for the absence of forbidden states.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/QEMCXNA6/Cvitanović - 1985 - Farey Organization of the Fractional Hall Effect.pdf}
}

@article{cvitanovicModelockingUniversalityCritical1990,
  title = {On the Mode-Locking Universality for Critical Circle Maps},
  author = {Cvitanovic, P and Gunaratne, G H and Vinson, M J},
  year = {1990},
  month = aug,
  journal = {Nonlinearity},
  volume = {3},
  number = {3},
  pages = {873--885},
  issn = {0951-7715, 1361-6544},
  doi = {10.1088/0951-7715/3/3/015},
  urldate = {2025-07-29},
  abstract = {The conjectured universality of the Hausdorff dimension of the fractal set formed by the set of the irrational winding parameter values for critical circle maps is shown to follow from the universal scalings for quadratic irrational winding numbers.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/AYLBB4CI/Cvitanovic et al. - 1990 - On the mode-locking universality for critical circle maps.pdf}
}

@article{cvitanovicRenormalizationUnstableManifolds1985,
  title = {Renormalization, Unstable Manifolds, and the Fractal Structure of Mode Locking},
  author = {Cvitanovi{\'c}, Predrag and Jensen, Mogens H. and Kadanoff, Leo P. and Procaccia, Itamar},
  year = {1985},
  month = jul,
  journal = {Physical Review Letters},
  volume = {55},
  number = {4},
  pages = {343--346},
  issn = {0031-9007},
  doi = {10.1103/PhysRevLett.55.343},
  urldate = {2025-07-29},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/EX4ET3TA/Cvitanović et al. - 1985 - Renormalization, unstable manifolds, and the fractal structure of mode locking.pdf}
}

@article{cvitanovicScalingLawsMode1985,
  title = {Scaling {{Laws}} for {{Mode Lockings}} in {{Circle Maps}}},
  author = {Cvitanovic, Predrag and Shraiman, Boris and S{\"o}derberg, Bo},
  year = {1985},
  month = oct,
  journal = {Physica Scripta},
  volume = {32},
  number = {4},
  pages = {263--270},
  issn = {0031-8949, 1402-4896},
  doi = {10.1088/0031-8949/32/4/003},
  urldate = {2025-07-29},
  abstract = {The self-similar structure of mode lockings for circle maps is studied by means of the associated Farey trees. We investigate numerically several classes of scaling relations implicit in the Farey organization of mode lockings and discuss the extent to which they lead to universal scaling laws.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/GUPRYWN5/Cvitanovic et al. - 1985 - Scaling Laws for Mode Lockings in Circle Maps.pdf}
}

@article{d.gusevComplexityMeasuresGenetic1999,
  title = {On the Complexity Measures of Genetic Sequences},
  author = {D.Gusev, Vladimir and A.Nemytikova, Lubov and A.Chuzhanova, Nadia},
  year = {1999},
  month = dec,
  journal = {Bioinformatics},
  volume = {15},
  number = {12},
  pages = {994--999},
  publisher = {Oxford University Press (OUP)},
  issn = {1367-4811, 1367-4803},
  doi = {10.1093/bioinformatics/15.12.994},
  urldate = {2025-07-15},
  abstract = {Motivation: It is well known that the regulatory regions of genomes are highly repetitive. They are rich in direct, symmetric and complemented repeats, and there is no doubt about the functional significance of these repeats. Among known measures of complexity, the Ziv--Lempel complexity measure reflects most adequately repeats occurring in the text. But this measure does not take into account isomorphic repeats. By isomorphic repeats we mean fragments that are identical (or symmetric) modulo some permutation of the alphabet letters.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/LCE8Y67Q/D.Gusev et al. - 1999 - On the complexity measures of genetic sequences.pdf}
}

@article{dabramoNonconventionalIdeasAlgorithmic2005,
  title = {Some Non-Conventional Ideas about Algorithmic Complexity},
  author = {D'Abramo, Germano},
  year = {2005},
  month = jul,
  journal = {Chaos, Solitons \& Fractals},
  volume = {25},
  number = {1},
  pages = {29--32},
  publisher = {Elsevier BV},
  issn = {0960-0779},
  doi = {10.1016/j.chaos.2004.11.040},
  urldate = {2025-07-15},
  abstract = {In this paper the author presents some non-conventional thoughts on the complexity of the Universe and the algorithmic reproducibility of the human brain, essentially sparked off by the notion of algorithmic complexity. We must warn that though they evoke suggestive scenarios, they are still quite speculative.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/9TD332VB/D’Abramo - 2005 - Some non-conventional ideas about algorithmic complexity.pdf}
}

@article{daiComputableVersionRandom2004,
  title = {A Computable Version of the Random Signs Problem and {{Kolmogorov}} Complexity},
  author = {Dai, Jack Jie},
  year = {2004},
  month = mar,
  journal = {Statistics \& Probability Letters},
  volume = {67},
  number = {1},
  pages = {27--31},
  publisher = {Elsevier BV},
  issn = {0167-7152},
  doi = {10.1016/j.spl.2003.12.002},
  urldate = {2025-07-25},
  abstract = {An e+ective solution to a computable version ofthe random signs problem is formulated using a coe,cient from Kolmogorov complexity theory. c{\copyright} 2003 Published by Elsevier B.V.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/M3U4ANSQ/Dai - 2004 - A computable version of the random signs problem and Kolmogorov complexity.pdf}
}

@article{daiFinitestateDimension2004,
  title = {Finite-State Dimension},
  author = {Dai, Jack J. and Lathrop, James I. and Lutz, Jack H. and Mayordomo, Elvira},
  year = {2004},
  month = jan,
  journal = {Theoretical Computer Science},
  volume = {310},
  number = {1-3},
  pages = {1--33},
  publisher = {Elsevier BV},
  issn = {0304-3975},
  doi = {10.1016/s0304-3975(03)00244-5},
  urldate = {2025-07-15},
  abstract = {Classical Hausdor0 dimension (sometimes called fractal dimension) was recently e0ectivized using gales (betting strategies that generalize martingales), thereby endowing various complexity classes with dimension structure and also de3ning the constructive dimensions of individual binary (in3nite) sequences. In this paper we use gales computedby multi-account 3nite-state gamblers to develop the 3nite-state dimensions of sets of binary sequences and individual binary sequences. The theorem of Eggleston (Quart. J. Math. OxfordSer. 20 (1949) 31--36) relating Hausdor0 dimension to entropy is shown to hold for 3nite-state dimension, both in the space of all sequences andin the space of all rational sequences (binary expansions of rational numbers). Every rational sequence has 3nite-state dimension 0, but every rational number in [0,1] is the 3nite-state dimension of a sequence in the low-level complexity class AC0. Our main theorem shows that the 3nite-state dimension of a sequence is precisely the in3mum of all compression ratios achievable on the sequence by information-lossless 3nite-state compressors.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/XFQT8K68/Dai et al. - 2004 - Finite-state dimension.pdf}
}

@article{danischMakiejlFlexibleHighperformance2021,
  title = {Makie.Jl: {{Flexible}} High-Performance Data Visualization for {{Julia}}},
  shorttitle = {Makie.Jl},
  author = {Danisch, Simon and Krumbiegel, Julius},
  year = {2021},
  month = sep,
  journal = {Journal of Open Source Software},
  volume = {6},
  number = {65},
  pages = {3349},
  issn = {2475-9066},
  doi = {10.21105/joss.03349},
  urldate = {2025-08-12},
  copyright = {http://creativecommons.org/licenses/by/4.0/}
}

@article{dassSymmetriesConservationLaws2001,
  title = {Symmetries and {{Conservation Laws}} in {{Histories-Based Theories}}},
  author = {Dass, Tulsi and Joglekar, Yogesh N.},
  year = {2001},
  month = feb,
  journal = {Annals of Physics},
  volume = {287},
  number = {2},
  pages = {191--228},
  publisher = {Elsevier BV},
  issn = {0003-4916},
  doi = {10.1006/aphy.2000.6106},
  urldate = {2025-07-25},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/WQG77I7M/Dass and Joglekar - 2001 - Symmetries and Conservation Laws in Histories-Based Theories.pdf}
}

@article{datserisEstimatingFractalDimensions2023,
  title = {Estimating Fractal Dimensions: {{A}} Comparative Review and Open Source Implementations},
  shorttitle = {Estimating Fractal Dimensions},
  author = {Datseris, George and Kottlarz, Inga and Braun, Anton P. and Parlitz, Ulrich},
  year = {2023},
  month = oct,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {33},
  number = {10},
  pages = {102101},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0160394},
  urldate = {2025-08-12},
  abstract = {The fractal dimension is a central quantity in nonlinear dynamics and can be estimated via several different numerical techniques. In this review paper, we present a self-contained and comprehensive introduction to the fractal dimension. We collect and present various numerical estimators and focus on the three most promising ones: generalized entropy, correlation sum, and extreme value theory. We then perform an extensive quantitative evaluation of these estimators, comparing their performance and precision using different datasets and comparing the impact of features like length, noise, embedding dimension, and falsify-ability, among many others. Our analysis shows that for synthetic noiseless data, the correlation sum is the best estimator with extreme value theory following closely. For real experimental data, we found the correlation sum to be more strongly affected by noise vs the entropy and extreme value theory. The recent extreme value theory estimator seems powerful as it has some of the advantages of both alternative methods. However, using four different ways for checking for significance, we found that the method yielded ``significant'' low-dimensional results for inappropriate data like stock market timeseries. This fact, combined with some ambiguities we found in the literature of the method applications, has implications for both previous and future real-world applications using the extreme value theory approach, as, for example, the argument for small effective dimensionality in the data cannot come from the method itself. All algorithms discussed are implemented as performant and easy to use open source code via the DynamicalSystems.jl library.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/SEU9WCEB/Datseris et al. - 2023 - Estimating fractal dimensions A comparative review and open source implementations.pdf}
}

@book{datserisNonlinearDynamicsConcise2022,
  title = {Nonlinear {{Dynamics}}: {{A Concise Introduction Interlaced}} with {{Code}}},
  shorttitle = {Nonlinear {{Dynamics}}},
  author = {Datseris, George and Parlitz, Ulrich},
  year = {2022},
  series = {Undergraduate {{Lecture Notes}} in {{Physics}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-91032-7},
  urldate = {2024-01-10},
  isbn = {978-3-030-91031-0 978-3-030-91032-7},
  langid = {english},
  annotation = {GSCC: 0000034},
  file = {/home/elessar/Zotero/storage/AQXJJAMT/Datseris and Parlitz - 2022 - Nonlinear Dynamics A Concise Introduction Interla.pdf}
}

@article{davidsenEarthquakeRecurrenceRecord2006,
  title = {Earthquake Recurrence as a Record Breaking Process},
  author = {Davidsen, Joern and Grassberger, Peter and Paczuski, Maya},
  year = {2006},
  month = jun,
  journal = {Geophysical Research Letters},
  volume = {33},
  number = {11},
  eprint = {physics/0507082},
  issn = {0094-8276, 1944-8007},
  doi = {10.1029/2006GL026122},
  urldate = {2025-07-25},
  abstract = {Extending the central concept of recurrence times for a point process to recurrent events in space-time allows us to characterize seismicity as a record breaking process using only spatiotemporal relations among events. Linking record breaking events with edges between nodes in a graph generates a complex dynamical network isolated from any length, time or magnitude scales set by the observer. For Southern California, the network of recurrences reveals new statistical features of seismicity with robust scaling laws. The rupture length and its scaling with magnitude emerges as a generic measure for distance between recurrent events. Further, the relative separations for subsequent records in space (or time) form a hierarchy with unexpected scaling properties.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Statistical Mechanics,Mathematics - Probability,Physics - Data Analysis Statistics and Probability,Physics - Geophysics},
  file = {/home/elessar/Zotero/storage/YYQLSJ33/Davidsen et al. - 2006 - Earthquake recurrence as a record breaking process.pdf}
}

@article{davieDecidableLimSup2013,
  title = {Decidable Lim Sup and {{Borel}}--{{Cantelli-like}} Lemmas for Random Sequences},
  author = {Davie, George},
  year = {2013},
  month = jan,
  journal = {Statistics \& Probability Letters},
  volume = {83},
  number = {1},
  pages = {278--285},
  publisher = {Elsevier BV},
  issn = {0167-7152},
  doi = {10.1016/j.spl.2012.09.010},
  urldate = {2025-07-25},
  abstract = {We prove computable versions of lim sup events and Borel--Cantelli-like results for algorithmically random sequences using a coefficient from Kolmogorov complexity. In particular we show that under suitable conditions on events, lim sup is layerwise decidable.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/EK6F2YEN/Davie - 2013 - Decidable lim sup and Borel–Cantelli-like lemmas for random sequences.pdf}
}

@article{dehmerHistoryGraphEntropy2011,
  title = {A History of Graph Entropy Measures},
  author = {Dehmer, Matthias and Mowshowitz, Abbe},
  year = {2011},
  month = jan,
  journal = {Information Sciences},
  volume = {181},
  number = {1},
  pages = {57--78},
  publisher = {Elsevier BV},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2010.08.041},
  urldate = {2025-07-25},
  abstract = {This survey seeks to describe methods for measuring the entropy of graphs and to demonstrate the wide applicability of entropy measures. Setting the scene with a review of classical measures for determining the structural information content of graphs, we discuss graph entropy measures which play an important role in a variety of problem areas, including biology, chemistry, and sociology. In addition, we examine relationships between selected entropy measures, illustrating differences quantitatively with concrete examples. {\'O} 2010 Elsevier Inc. All rights reserved.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/5PX8AXLF/Dehmer and Mowshowitz - 2011 - A history of graph entropy measures.pdf}
}

@article{delaiglesiaPrincipalDynamicalComponents2013,
  title = {Principal {{Dynamical Components}}},
  author = {De La Iglesia, Manuel D. and Tabak, Esteban G.},
  year = {2013},
  month = jan,
  journal = {Communications on Pure and Applied Mathematics},
  volume = {66},
  number = {1},
  pages = {48--82},
  publisher = {Wiley},
  issn = {0010-3640, 1097-0312},
  doi = {10.1002/cpa.21411},
  urldate = {2025-07-25},
  abstract = {A procedure is proposed for a dimension reduction in time series. Similarly to principal components, the procedure seeks a low-dimensional manifold that minimizes information loss. Unlike principal components, however, the procedure involves dynamical considerations through the proposal of a predictive dynamical model in the reduced manifold. Hence the minimization of the uncertainty is not only over the choice of a reduced manifold, as in principal components, but also over the parameters of the dynamical model, as in autoregressive analysis and principal interaction patterns. Further generalizations are provided to nonautonomous and non-Markovian scenarios, which are then applied to historical sea-surface temperature data. {\copyright} 2012 Wiley Periodicals, Inc.},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  file = {/home/elessar/Zotero/storage/SY3LLXVF/De La Iglesia and Tabak - 2013 - Principal Dynamical Components.pdf}
}

@article{delsarteAssociationSchemesCoding,
  title = {Association {{Schemes}} and {{Coding Theory}}},
  author = {Delsarte, Philippe and Levenshtein, Vladimir I},
  abstract = {This paper contains a survey of association scheme theory (with its algebraic and analytical aspects) and of its applications to coding theory (in a wide sense). It is mainly concerned with a class of subjects that involve the central notion of the distance distribution of a code. Special emphasis is put on the linear programming method, inspired by the MacWilliams transform. This produces upper bounds for the size of a code with a given minimum distance, and lower bounds for the size of a design with a given strength. The most specific results are obtained in the case where the underlying association scheme satisfies certain well-defined ``polynomial properties;'' this leads one into the realm of orthogonal polynomial theory. In particular, some ``universal bounds'' are derived for codes and designs in polynomial type association schemes. Throughout the paper, the main concepts, methods, and results are illustrated by two examples that are of major significance in classical coding theory, namely, the Hamming scheme and the Johnson scheme. Other topics that receive special attention are spherical codes and designs, and additive codes in translation schemes, including 4-additive binary codes.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/NC6BZ4NM/Delsarte and Levenshtein - Association Schemes and Coding Theory.pdf}
}

@article{dengUniversalScalingSports2012,
  title = {Universal Scaling in Sports Ranking},
  author = {Deng, Weibing and Li, Wei and Cai, Xu and Bulou, Alain and Wang, Qiuping A},
  year = {2012},
  month = sep,
  journal = {New Journal of Physics},
  volume = {14},
  number = {9},
  pages = {093038},
  publisher = {IOP Publishing},
  issn = {1367-2630},
  doi = {10.1088/1367-2630/14/9/093038},
  urldate = {2025-07-24},
  langid = {english},
  file = {/home/elessar/Zotero/storage/Y7BKHBXR/Deng et al. - 2012 - Universal scaling in sports ranking.pdf}
}

@article{denysRELATIONSHIPSKOLMOGOROVCOMPLEXITY,
  title = {{{RELATIONSHIPS BETWEEN KOLMOGOROV COMPLEXITY}}, {{SHANNON ENTROPY}}, {{AND LEMPEL-ZIV CODES}}},
  author = {Denys, Elizabeth A},
  langid = {english},
  file = {/home/elessar/Zotero/storage/QXUPTPDF/Denys - RELATIONSHIPS BETWEEN KOLMOGOROV COMPLEXITY, SHANNON ENTROPY, AND LEMPEL-ZIV CODES.pdf}
}

@article{derryConvolutionalNeuralNetworks2023,
  title = {Convolutional Neural Networks},
  author = {Derry, Alexander and Krzywinski, Martin and Altman, Naomi},
  year = {2023},
  month = sep,
  journal = {Nature Methods},
  volume = {20},
  number = {9},
  pages = {1269--1270},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-023-01973-1},
  urldate = {2024-01-10},
  langid = {english},
  annotation = {GSCC: 0002991},
  file = {/home/elessar/Zotero/storage/4NR5GKND/Derry et al. - 2023 - Convolutional neural networks.pdf}
}

@article{deweyAlgorithmicComplexityProtein1996,
  title = {Algorithmic Complexity of a Protein},
  author = {Dewey, T. Gregory},
  year = {1996},
  month = jul,
  journal = {Physical Review E},
  volume = {54},
  number = {1},
  pages = {R39-R41},
  publisher = {American Physical Society (APS)},
  issn = {1063-651X, 1095-3787},
  doi = {10.1103/physreve.54.r39},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/S9SMQ8YM/Dewey - 1996 - Algorithmic complexity of a protein.pdf}
}

@misc{doanPhysicsInformedEchoState2019,
  title = {Physics-{{Informed Echo State Networks}} for {{Chaotic Systems Forecasting}}},
  author = {Doan, Nguyen Anh Khoa and Polifke, Wolfgang and Magri, Luca},
  year = {2019},
  month = apr,
  number = {arXiv:1906.11122},
  eprint = {1906.11122},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.11122},
  urldate = {2024-11-26},
  abstract = {We propose a physics-informed Echo State Network (ESN) to predict the evolution of chaotic systems. Compared to conventional ESNs, the physics-informed ESNs are trained to solve supervised learning tasks while ensuring that their predictions do not violate physical laws. This is achieved by introducing an additional loss function during the training of the ESNs, which penalizes non-physical predictions without the need of any additional training data. This approach is demonstrated on a chaotic Lorenz system, where the physics-informed ESNs improve the predictability horizon by about two Lyapunov times as compared to conventional ESNs. The proposed framework shows the potential of using machine learning combined with prior physical knowledge to improve the time-accurate prediction of chaotic dynamical systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Emerging Technologies,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Physics - Physics and Society,Statistics - Machine Learning},
  file = {/home/elessar/Zotero/storage/5RI2ZW6W/Doan et al. - 2019 - Physics-Informed Echo State Networks for Chaotic Systems Forecasting.pdf;/home/elessar/Zotero/storage/C3Q22PUT/1906.html}
}

@article{doanPhysicsinformedEchoState2020,
  title = {Physics-Informed Echo State Networks},
  author = {Doan, N.A.K. and Polifke, W. and Magri, L.},
  year = {2020},
  month = nov,
  journal = {Journal of Computational Science},
  volume = {47},
  pages = {101237},
  issn = {18777503},
  doi = {10.1016/j.jocs.2020.101237},
  urldate = {2025-03-20},
  abstract = {We propose a physics-informed echo state network (ESN) to predict the evolution of chaotic systems. Compared to conventional ESNs, the physics-informed ESNs are trained to solve supervised learning tasks while ensuring that their predictions do not violate physical laws. This is achieved by introducing an additional loss function during the training, which is based on the system's governing equations. The additional loss function penalizes non-physical predictions without the need of any additional training data. This approach is demonstrated on a chaotic Lorenz system and a truncation of the Charney--DeVore system. Compared to the conventional ESNs, the physics-informed ESNs improve the predictability horizon by about two Lyapunov times. This approach is also shown to be robust with regard to noise. The proposed framework shows the potential of using machine learning combined with prior physical knowledge to improve the time-accurate prediction of chaotic dynamical systems.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/GK3BR3DN/Doan et al. - 2020 - Physics-informed echo state networks.pdf}
}

@inbook{doganaksoyLempelZivComplexitySequences2006,
  title = {On {{Lempel-Ziv Complexity}} of {{Sequences}}},
  booktitle = {Lecture {{Notes}} in {{Computer Science}}},
  year = {2006},
  pages = {180--189},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  issn = {0302-9743, 1611-3349},
  doi = {10.1007/11863854_15},
  urldate = {2025-07-15},
  abstract = {We derive recurrences for counting the number a(n, r) of n-sequences with Lempel-Ziv complexity r, which has important applications, for instance testing randomness. We also give algorithms to compute these recurrences. We employed these algorithms to compute a(n, r) and expected value, E Pn, of number of patterns of an n-sequence, for relatively large n. We also give the outputs of these programs.},
  collaborator = {Do{\u g}anaksoy, Ali and G{\"o}lo{\u g}lu, Faruk},
  isbn = {978-3-540-44523-4 978-3-540-44524-1},
  langid = {english},
  file = {/home/elessar/Zotero/storage/3CXYTCEJ/2006 - On Lempel-Ziv Complexity of Sequences.pdf;/home/elessar/Zotero/storage/8LJ3WQS3/2006 - On Lempel-Ziv Complexity of Sequences.pdf}
}

@misc{dolinskyReadoutsEchostateNetworks2012,
  title = {Readouts for {{Echo-state Networks Built}} Using {{Locally Regularized Orthogonal Forward Regression}}},
  author = {Dolinsk{\'y}, J{\'a}n and Hirose, Kei and Konishi, Sadanori},
  year = {2012},
  month = jul,
  number = {arXiv:1110.4304},
  eprint = {1110.4304},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1110.4304},
  urldate = {2025-03-07},
  abstract = {Echo state network (ESN) is viewed as a temporal non-orthogonal expansion with pseudo-random parameters. Such expansions naturally give rise to regressors of various relevance to a teacher output. We illustrate that often only a certain amount of the generated echo-regressors effectively explain the variance of the teacher output and also that sole local regularization is not able to provide in-depth information concerning the importance of the generated regressors. The importance is therefore determined by a joint calculation of the individual variance contributions and Bayesian relevance using locally regularized orthogonal forward regression (LROFR) algorithm. This information can be advantageously used in a variety of ways for an in-depth analysis of an ESN structure and its state-space parameters in relation to the unknown dynamics of the underlying problem. We present locally regularized linear readout built using LROFR. The readout may have a different dimensionality than an ESN model itself, and besides improving robustness and accuracy of an ESN it relates the echo-regressors to different features of the training data and may determine what type of an additional readout is suitable for a task at hand. Moreover, as flexibility of the linear readout has limitations and might sometimes be insufficient for certain tasks, we also present a radial basis function (RBF) readout built using LROFR. It is a flexible and parsimonious readout with excellent generalization abilities and is a viable alternative to readouts based on a feed-forward neural network (FFNN) or an RBF net built using relevance vector machine (RVM).},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/elessar/Zotero/storage/QZT5DIXP/Dolinský et al. - 2012 - Readouts for Echo-state Networks Built using Locally Regularized Orthogonal Forward Regression.pdf;/home/elessar/Zotero/storage/AAC4YKJX/1110.html}
}

@article{donatoPhaseLockingControl2006,
  title = {Phase Locking Control in the {{Circle Map}}},
  author = {Donato, Pedro Fernando Almeida Di and Macau, Elbert E. N. and Grebogi, Celso},
  year = {2006},
  month = dec,
  journal = {Nonlinear Dynamics},
  volume = {47},
  number = {1-3},
  pages = {75--82},
  issn = {0924-090X, 1573-269X},
  doi = {10.1007/s11071-006-9055-7},
  urldate = {2025-07-29},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/home/elessar/Zotero/storage/UAXNJUXK/Donato et al. - 2006 - Phase locking control in the Circle Map.pdf}
}

@article{dongOpticalReservoirComputing2020,
  title = {Optical {{Reservoir Computing}} Using Multiple Light Scattering for Chaotic Systems Prediction},
  author = {Dong, Jonathan and Rafayelyan, Mushegh and Krzakala, Florent and Gigan, Sylvain},
  year = {2020},
  month = jan,
  journal = {IEEE Journal of Selected Topics in Quantum Electronics},
  volume = {26},
  number = {1},
  eprint = {1907.00657},
  primaryclass = {cs},
  pages = {1--12},
  issn = {1077-260X, 1558-4542},
  doi = {10.1109/JSTQE.2019.2936281},
  urldate = {2024-01-10},
  abstract = {Reservoir Computing is a relatively recent computational framework based on a large Recurrent Neural Network with fixed weights. Many physical implementations of Reservoir Computing have been proposed to improve speed and energy efficiency. In this study, we report new advances in Optical Reservoir Computing using multiple light scattering to accelerate the recursive computation of the reservoir states. Two different spatial light modulation technologies, namely, phase or binary amplitude modulations, are compared. Phase modulation is a promising direction already employed in other photonic implementations of Reservoir Computing. Additionally, we report a Digital-Micromirror-based Reservoir Computing at up to 640 Hz, more than double the previously reported frequency using a remotely controlled optical device developed by LightOn, and present new binarization strategies to improve the performance of binarized Reservoir Computing.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Emerging Technologies},
  annotation = {GSCC: 0000123},
  file = {/home/elessar/Zotero/storage/NPCW468Z/Dong et al_2020_Optical Reservoir Computing using multiple light scattering for chaotic systems.pdf;/home/elessar/Zotero/storage/Q4VD8TA7/Dong et al_2020_Optical Reservoir Computing using multiple light scattering for chaotic systems.pdf;/home/elessar/Zotero/storage/RFFIKTZ2/Dong et al. - 2020 - Optical Reservoir Computing using multiple light s.pdf}
}

@article{dorflerSynchronizationComplexNetworks2014,
  title = {Synchronization in Complex Networks of Phase Oscillators: {{A}} Survey},
  shorttitle = {Synchronization in Complex Networks of Phase Oscillators},
  author = {D{\"o}rfler, Florian and Bullo, Francesco},
  year = {2014},
  month = jun,
  journal = {Automatica},
  volume = {50},
  number = {6},
  pages = {1539--1564},
  issn = {00051098},
  doi = {10.1016/j.automatica.2014.04.012},
  urldate = {2025-07-29},
  abstract = {The emergence of synchronization in a network of coupled oscillators is a fascinating subject of multidisciplinary research. This survey reviews the vast literature on the theory and the applications of complex oscillator networks. We focus on phase oscillator models that are widespread in real-world synchronization phenomena, that generalize the celebrated Kuramoto model, and that feature a rich phenomenology. We review the history and the countless applications of this model throughout science and engineering. We justify the importance of the widespread coupled oscillator model as a locally canonical model and describe some selected applications relevant to control scientists, including vehicle coordination, electric power networks, and clock synchronization. We introduce the reader to several synchronization notions and performance estimates. We propose analysis approaches to phase and frequency synchronization, phase balancing, pattern formation, and partial synchronization. We present the sharpest known results about synchronization in networks of homogeneous and heterogeneous oscillators, with complete or sparse interconnection topologies, and in finite-dimensional and infinitedimensional settings. We conclude by summarizing the limitations of existing analysis methods and by highlighting some directions for future research.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/UILAIA6P/Dörfler and Bullo - 2014 - Synchronization in complex networks of phase oscillators A survey.pdf}
}

@article{dosovitskiyIMAGEWORTH16X162021,
  title = {{{AN IMAGE IS WORTH 16X16 WORDS}}: {{TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  langid = {english},
  keywords = {No DOI found},
  annotation = {GSCC: 0044239},
  file = {/home/elessar/Zotero/storage/N6VIBU6B/Dosovitskiy et al. - 2021 - AN IMAGE IS WORTH 16X16 WORDS TRANSFORMERS FOR IM.pdf}
}

@article{downeyRandomnessReducibility2004,
  title = {Randomness and Reducibility},
  author = {Downey, Rod G. and Hirschfeldt, Denis R. and LaForte, Geoff},
  year = {2004},
  month = feb,
  journal = {Journal of Computer and System Sciences},
  volume = {68},
  number = {1},
  pages = {96--114},
  publisher = {Elsevier BV},
  issn = {0022-0000},
  doi = {10.1016/j.jcss.2003.07.004},
  urldate = {2025-07-15},
  abstract = {We study reducibilities that act as measures of relative randomness on reals, concentrating particularly on their behavior on the computably enumerable reals. One such reducibility, called domination or Solovay reducibility, has already proved to be a powerful tool in the study of randomness of effectively presented reals. Motivated by certain shortcomings of Solovay reducibility, we introduce two new measures of relative randomness and investigate their properties and the relationships between them and Solovay reducibility.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/I3R8XNHB/Downey et al. - 2004 - Randomness and reducibility.PDF}
}

@book{downeyThinkBayes2016,
  title = {Think {{Bayes}}},
  author = {Downey, Allen B.},
  year = {2016},
  series = {Bayesian Statistics in {{Python}}},
  edition = {fourth, [revised] release},
  publisher = {O'Reilly},
  address = {Beijing K{\"o}ln},
  abstract = {Bayes's theorem -- Computational statistics -- Estimation -- More estimation -- Odds and addends -- Decision analysis -- Prediction -- Observer bias -- Two dimensions -- Approximate Bayesian computation -- Hypothesis testing -- Evidence -- Simulation -- A hierarchical model -- Dealing with dimensions},
  isbn = {978-1-4493-7078-7},
  langid = {english},
  file = {/home/elessar/Zotero/storage/EGVYY9YZ/Downey - 2016 - Think Bayes.pdf}
}

@book{downeyThinkComplexityExploring2012,
  title = {Think {{Complexity}}: {{Exploring Complexity Science}} with {{Python}}},
  shorttitle = {Think {{Complexity}}},
  author = {Downey, Allen B.},
  year = {2012},
  series = {Open Textbook Library},
  edition = {2e},
  publisher = {Green Tea Press},
  address = {Place of publication not identified},
  abstract = {Complexity Science is an interdisciplinary field-at the intersection of mathematics, computer science, and natural science-that focuses on discrete models of physical and social systems. In particular, it focuses on complex systems, which are systems with many interacting components. Complex systems include networks and graphs, cellular automatons, agent-based models and swarms, fractals and self-organizing systems, chaotic systems and cybernetic systems. This book is primarily about complexity science, but studying complexity science gives you a chance to explore topics and ideas you might not encounter otherwise, practice programming in Python, and learn about data structures and algorithms. This book picks up where Think Python leaves off. I assume that you have read that book or have equivalent knowledge of Python. As always, I try to emphasize fundamental ideas that apply to programming in many languages, but along the way you will learn useful features that are specific to Python. The models and results in this book raise a number of questions relevant to the philosophy of science, including the nature of scientific laws, theory choice, realism and instrumentalism, holism and reductionism, and Bayesian epistemology},
  collaborator = {{Open Textbook Library}},
  isbn = {978-1-4493-1463-7},
  langid = {english},
  file = {/home/elessar/Zotero/storage/QIBLHE5Y/Downey - 2012 - Think Complexity Exploring Complexity Science with Python.pdf}
}

@book{downeyThinkDSPDigital2016,
  title = {Think {{DSP}}: Digital Signal Processing in {{Python}}},
  shorttitle = {Think {{DSP}}},
  author = {Downey, Allen},
  year = {2016},
  edition = {First edition},
  publisher = {O'Reilly Media, Inc},
  address = {Sebastopol, CA},
  abstract = {If you understand basic mathematics and know how to program with Python, you're ready to dive into signal processing. While most resources start with theory to teach this complex subject, this practical book introduces techniques by showing you how they're applied in the real world. In the first chapter alone, you'll be able to decompose a sound into its harmonics, modify the harmonics, and generate new sounds. Author Allen Downey explains techniques such as spectral decomposition, filtering, convolution, and the Fast Fourier Transform. This book also provides exercises and code examples to help you understand the material. You'll explore: Periodic signals and their spectrums, Harmonic structure of simple waveforms, Chirps and other sounds whose spectrum changes over time, Noise signals and natural sources of noise, The autocorrelation function for estimating pitch, The discrete cosine transform (DCT) for compression, The Fast Fourier Transform for spectral analysis, Relating operations in time to filters in the frequency domain, Linear time-invariant (LTI) system theory, Amplitude modulation (AM) used in radio. -- Provided by publisher},
  isbn = {978-1-4919-3845-4},
  lccn = {TK5102.9 .D68 2016},
  keywords = {Digital techniques Data processing,Python (Computer program language),Signal processing},
  file = {/home/elessar/Zotero/storage/X9ENTJJ2/Downey - 2016 - Think DSP digital signal processing in Python.pdf}
}

@book{downeyThinkPython2012,
  title = {Think {{Python}}},
  author = {Downey, Allen},
  year = {2012},
  publisher = {O'Reilly},
  address = {Sebastopol, CA},
  abstract = {If you want to learn how to program, working with Python is an excellent way to start. This hands-on guide takes you through the language one step at a time, beginning with basic programming concepts before moving on to functions, recursion, data structures, and object-oriented design. Through exercises in each chapter, you'll try out programming concepts as you learn them. Think Python is ideal for students at the high school or college level, as well as self-learners, home-schooled students, and professionals who need to learn programming basics. Start with the basics, including language syntax and semantics Get a clear definition of each programming concept Learn values, variables, statements, functions, and data structures in a logical progression Discover how to work with files and databases Understand objects, methods, and object-oriented programming Use debugging techniques to fix syntax, runtime, and semantic errors Explore interface design, data structures, and GUI-based programs through case studies},
  isbn = {978-1-4493-3203-7},
  langid = {english},
  annotation = {OCLC: 827797127},
  file = {/home/elessar/Zotero/storage/FWYJHW48/Downey - 2012 - Think Python.pdf}
}

@book{downeyThinkStats2011,
  title = {Think Stats},
  author = {Downey, Allen},
  year = {2011},
  edition = {1st ed},
  publisher = {O'Reilly Media},
  address = {Sebastopol, Calif.},
  abstract = {"Think Stats: Probability and Statistics for Programmers is a textbook for a new kind of introductory prob-stat class. It emphasizes the use of statistics to explore large datasets. It takes a computation approach: students write programs in Python as a way of developing and testing their understanding."--Home page, overview},
  isbn = {978-1-4493-1369-2},
  langid = {english},
  annotation = {OCLC: 744705704},
  file = {/home/elessar/Zotero/storage/S362PBKL/Downey - 2011 - Think stats.pdf}
}

@article{dunleavyUsingMutualInformation2012,
  title = {Using Mutual Information to Measure Order in Model Glass Formers},
  author = {Dunleavy, Andrew J. and Wiesner, Karoline and Royall, C. Patrick},
  year = {2012},
  month = oct,
  journal = {Physical Review E},
  volume = {86},
  number = {4},
  publisher = {American Physical Society (APS)},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/physreve.86.041505},
  urldate = {2025-07-24},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/EVIEWEA4/Dunleavy et al. - 2012 - Using mutual information to measure order in model glass formers.pdf}
}

@article{durandDescriptiveComplexityComputable,
  title = {Descriptive Complexity of Computable Sequences},
  author = {Durand, Bruno and Shen, Alexander and Vereshchagin, Nikolai},
  abstract = {Our goal is to study the complexity of in/nite binary recursive sequences. We introduce several measures of the quantity of information they contain. Some measures are based on size of programs that generate the sequence, the others are based on the Kolmogorov complexity of its /nite pre/xes. The relations between these complexity measures are established. The most surprising among them are obtained using a speci/c two-players game 2 . c{\copyright} 2002 Elsevier Science B.V. All rights reserved.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/WFVP5RDK/Durand et al. - Descriptive complexity of computable sequences.pdf}
}

@inbook{duRecurrentNeuralNetworks2014,
  title = {Recurrent {{Neural Networks}}},
  booktitle = {Neural {{Networks}} and {{Statistical Learning}}},
  author = {Du, Ke-Lin and Swamy, M. N. S.},
  year = {2014},
  pages = {337--353},
  publisher = {Springer London},
  address = {London},
  doi = {10.1007/978-1-4471-5571-3_11},
  urldate = {2023-05-30},
  collaborator = {Du, Ke-Lin and Swamy, M. N. S.},
  isbn = {978-1-4471-5570-6 978-1-4471-5571-3},
  langid = {english},
  annotation = {GSCC: 0000240},
  file = {/home/elessar/Zotero/storage/TJQSNJXP/Du and Swamy - 2014 - Recurrent Neural Networks.pdf}
}

@article{durstewitzReconstructingComputationalSystem2023,
  title = {Reconstructing Computational System Dynamics from Neural Data with Recurrent Neural Networks},
  author = {Durstewitz, Daniel and Koppe, Georgia and Thurm, Max Ingo},
  year = {2023},
  month = nov,
  journal = {Nature Reviews Neuroscience},
  volume = {24},
  number = {11},
  pages = {693--710},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/s41583-023-00740-7},
  urldate = {2025-08-12},
  langid = {english}
}

@article{eastchinauniversityofscienceandtechnologyFixedStringElementary2010,
  title = {The {{Fixed String}} of {{Elementary Cellular Automata}}},
  author = {{East China University of Science and Technology} and Zhisong, Jiang and Dakang, Qin and {School of Science Nantong University}},
  year = {2010},
  month = oct,
  journal = {Complex Systems},
  volume = {19},
  number = {3},
  pages = {243--262},
  publisher = {Wolfram Research, Inc.},
  issn = {0891-2513},
  doi = {10.25088/complexsystems.19.3.243},
  urldate = {2025-07-15},
  langid = {english},
  file = {/home/elessar/Zotero/storage/6L6IUGAV/East China University of Science and Technology et al. - 2010 - The Fixed String of Elementary Cellular Automata.pdf}
}

@article{eckeScalingArnoldTongues1989,
  title = {Scaling of the {{Arnold}} Tongues},
  author = {Ecke, R E and Farmer, J D and Umberger, D K},
  year = {1989},
  month = may,
  journal = {Nonlinearity},
  volume = {2},
  number = {2},
  pages = {175--196},
  issn = {0951-7715, 1361-6544},
  doi = {10.1088/0951-7715/2/2/001},
  urldate = {2025-07-29},
  abstract = {When two oscillatorsare coupledtogether there are parameter regions called `Arnold tongues' where they mode lock and their motion is periodic with a common frequency.We perform several numerical experiments on a circle map, studying the width of the Arnold tongues as a function of the period q , winding numberp / q , and nonlinearityparameter k , in the subcritical region below the transition to chaos. There are several interestingscaling laws. In the limit as k -+ 0 at fixed q, we find that the width of the tongues, AQ, scales as k q , as originally suggested by Arnold. In the limit as q +m at fixed k , however, AS2 scales as q - 3 , just as it does in the critical case. In addition, we find several interestingscaling laws under variations in p and k . The q - 3 scaling, token together with the observed p scaling, provides evidence that the ergodic region between the Amold tongues is a fat fractal, with an exponent that is 3 throughout the subcritical range. This indirect evidence is supported by direct calculations of the fat-fractal exponent which yield values between 0.6 and 0.7 for 0.4},
  langid = {english},
  file = {/home/elessar/Zotero/storage/PGLQZ5GN/Ecke et al. - 1989 - Scaling of the Arnold tongues.pdf}
}

@article{eckmannLiapunovExponentsTime1986,
  title = {Liapunov Exponents from Time Series},
  author = {Eckmann, J. -P. and Kamphorst, S. Oliffson and Ruelle, D. and Ciliberto, S.},
  year = {1986},
  month = dec,
  journal = {Physical Review A},
  volume = {34},
  number = {6},
  pages = {4971--4979},
  issn = {0556-2791},
  doi = {10.1103/PhysRevA.34.4971},
  urldate = {2023-05-30},
  langid = {english},
  annotation = {GSCC: 0001490},
  file = {/home/elessar/Zotero/storage/54V2IBKV/Eckmann et al. - 1986 - Liapunov exponents from time series.pdf}
}

@misc{edsonLyapunovExponentsKuramotoSivashinsky2019,
  title = {Lyapunov Exponents of the {{Kuramoto-Sivashinsky PDE}}},
  author = {Edson, Russell A. and Bunder, J. E. and Mattner, Trent W. and Roberts, A. J.},
  year = {2019},
  month = feb,
  number = {arXiv:1902.09651},
  eprint = {1902.09651},
  primaryclass = {nlin},
  publisher = {arXiv},
  urldate = {2024-04-30},
  abstract = {The Kuramoto--Sivashinsky equation is a prototypical chaotic nonlinear partial differential equation (pde) in which the size of the spatial domain plays the role of a bifurcation parameter. We investigate the changing dynamics of the Kuramoto--Sivashinsky pde by calculating the Lyapunov spectra over a large range of domain sizes. Our comprehensive computation and analysis of the Lyapunov exponents and the associated Kaplan--Yorke dimension provides new insights into the chaotic dynamics of the Kuramoto--Sivashinsky pde, and the transition to its 1D turbulence.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Dynamical Systems,Nonlinear Sciences - Chaotic Dynamics},
  annotation = {GSCC: 0000039},
  file = {/home/elessar/Zotero/storage/4KZKYR59/Edson et al. - 2019 - Lyapunov exponents of the Kuramoto-Sivashinsky PDE.pdf;/home/elessar/Zotero/storage/7YKYYU6I/1902.html}
}

@article{ehlersImprovingPerformanceEcho2025,
  title = {Improving the {{Performance}} of {{Echo State Networks Through State Feedback}}},
  author = {Ehlers, Peter J. and Nurdin, Hendra I. and Soh, Daniel},
  year = {2025},
  month = apr,
  journal = {Neural Networks},
  volume = {184},
  eprint = {2312.15141},
  primaryclass = {cs},
  pages = {107101},
  issn = {08936080},
  doi = {10.1016/j.neunet.2024.107101},
  urldate = {2025-03-20},
  abstract = {Reservoir computing, using nonlinear dynamical systems, offers a cost-effective alternative to neural networks for complex tasks involving processing of sequential data, time series modeling, and system identification. Echo state networks (ESNs), a type of reservoir computer, mirror neural networks but simplify training. They apply fixed, random linear transformations to the internal state, followed by nonlinear changes. This process, guided by input signals and linear regression, adapts the system to match target characteristics, reducing computational demands. A potential drawback of ESNs is that the fixed reservoir may not offer the complexity needed for specific problems. While directly altering (training) the internal ESN would reintroduce the computational burden, an indirect modification can be achieved by redirecting some output as input. This feedback can influence the internal reservoir state, yielding ESNs with enhanced complexity suitable for broader challenges. In this paper, we demonstrate that by feeding some component of the reservoir state back into the network through the input, we can drastically improve upon the performance of a given ESN. We rigorously prove that, for any given ESN, feedback will almost always improve the accuracy of the output. For a set of three tasks, each representing different problem classes, we find that with feedback the average error measures are reduced by 30\% - 60\%. Remarkably, feedback provides at least an equivalent performance boost to doubling the initial number of computational nodes, a computationally expensive and technologically challenging alternative. These results demonstrate the broad applicability and substantial usefulness of this feedback scheme.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control,Nonlinear Sciences - Adaptation and Self-Organizing Systems,Statistics - Machine Learning},
  file = {/home/elessar/Zotero/storage/AW4IIVNX/Ehlers et al. - 2025 - Improving the Performance of Echo State Networks Through State Feedback.pdf}
}

@book{elbassioniAlgorithmsComputation26th2015,
  title = {Algorithms and {{Computation}}: 26th {{International Symposium}}, {{ISAAC}} 2015, {{Nagoya}}, {{Japan}}, {{December}} 9-11, 2015, {{Proceedings}}},
  shorttitle = {Algorithms and {{Computation}}},
  editor = {Elbassioni, Khaled and Makino, Kazuhisa},
  year = {2015},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  issn = {0302-9743, 1611-3349},
  doi = {10.1007/978-3-662-48971-0},
  urldate = {2025-07-15},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-662-48970-3 978-3-662-48971-0},
  langid = {english},
  file = {/home/elessar/Zotero/storage/PPYJQA54/Elbassioni and Makino - 2015 - Algorithms and Computation 26th International Symposium, ISAAC 2015, Nagoya, Japan, December 9-11,.pdf}
}

@article{ellisonInformationSymmetriesIrreversible2011,
  title = {Information {{Symmetries}} in {{Irreversible Processes}}},
  author = {Ellison, Christopher J. and Mahoney, John R. and James, Ryan G. and Crutchfield, James P. and Reichardt, Joerg},
  year = {2011},
  month = sep,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {21},
  number = {3},
  eprint = {1107.2168},
  primaryclass = {cond-mat},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.3637490},
  urldate = {2025-07-15},
  abstract = {We study dynamical reversibility in stationary stochastic processes from an information theoretic perspective. Extending earlier work on the reversibility of Markov chains, we focus on finitary processes with arbitrarily long conditional correlations. In particular, we examine stationary processes represented or generated by edge-emitting, finite-state hidden Markov models. Surprisingly, we find pervasive temporal asymmetries in the statistics of such stationary processes with the consequence that the computational resources necessary to generate a process in the forward and reverse temporal directions are generally not the same. In fact, an exhaustive survey indicates that most stationary processes are irreversible. We study the ensuing relations between model topology in different representations, the process's statistical properties, and its reversibility in detail. A process's temporal asymmetry is efficiently captured using two canonical unifilar representations of the generating model, the forward-time and reverse-time epsilon-machines. We analyze example irreversible processes whose epsilon-machine presentations change size under time reversal, including one which has a finite number of recurrent causal states in one direction, but an infinite number in the opposite. From the forward-time and reverse-time epsilon-machines, we are able to construct a symmetrized, but nonunifilar, generator of a process---the bidirectional machine. Using the bidirectional machine, we show how to directly calculate a process's fundamental information properties, many of which are otherwise only poorly approximated via process samples. The tools we introduce and the insights we offer provide a better understanding of the many facets of reversibility and irreversibility in stochastic processes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Condensed Matter - Statistical Mechanics,Mathematics - Information Theory,Mathematics - Statistics Theory,Nonlinear Sciences - Chaotic Dynamics,Statistics - Statistics Theory},
  file = {/home/elessar/Zotero/storage/F537GW6H/Ellison et al. - 2011 - Information Symmetries in Irreversible Processes.pdf}
}

@misc{engelkenLyapunovSpectraChaotic2020,
  title = {Lyapunov Spectra of Chaotic Recurrent Neural Networks},
  author = {Engelken, Rainer and Wolf, Fred and Abbott, L. F.},
  year = {2020},
  month = jun,
  number = {arXiv:2006.02427},
  eprint = {2006.02427},
  primaryclass = {nlin},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2006.02427},
  urldate = {2025-03-20},
  abstract = {Brains process information through the collective dynamics of large neural networks. Collective chaos was suggested to underlie the complex ongoing dynamics observed in cerebral cortical circuits and determine the impact and processing of incoming information streams. In dissipative systems, chaotic dynamics takes place on a subset of phase space of reduced dimensionality and is organized by a complex tangle of stable, neutral and unstable manifolds. Key topological invariants of this phase space structure such as attractor dimension, and Kolmogorov-Sinai entropy so far remained elusive. Here we calculate the complete Lyapunov spectrum of recurrent neural networks. We show that chaos in these networks is extensive with a size-invariant Lyapunov spectrum and characterized by attractor dimensions much smaller than the number of phase space dimensions. We find that near the onset of chaos, for very intense chaos, and discrete-time dynamics, random matrix theory provides analytical approximations to the full Lyapunov spectrum. We show that a generalized time-reversal symmetry of the network dynamics induces a point-symmetry of the Lyapunov spectrum reminiscent of the symplectic structure of chaotic Hamiltonian systems. Fluctuating input reduces both the entropy rate and the attractor dimension. For trained recurrent networks, we find that Lyapunov spectrum analysis provides a quantification of error propagation and stability achieved. Our methods apply to systems of arbitrary connectivity, and we describe a comprehensive set of controls for the accuracy and convergence of Lyapunov exponents. Our results open a novel avenue for characterizing the complex dynamics of recurrent neural networks and the geometry of the corresponding chaotic attractors. They also highlight the potential of Lyapunov spectrum analysis as a diagnostic for machine learning applications of recurrent networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Nonlinear Sciences - Chaotic Dynamics,Quantitative Biology - Neurons and Cognition},
  file = {/home/elessar/Zotero/storage/SNPFHLEL/Engelken et al. - 2020 - Lyapunov spectra of chaotic recurrent neural networks.pdf}
}

@article{EntropyFunctionsFinitestate,
  title = {{The entropy of functions of finite-state Markov chains}},
  langid = {russian},
  file = {/home/elessar/Zotero/storage/YBWMFI33/The entropy of functions of finite-state Markov chains.pdf}
}

@inproceedings{erdodiFileCompressionLZO2012,
  title = {File Compression with {{LZO}} Algorithm Using {{NVIDIA CUDA}} Architecture},
  booktitle = {2012 4th {{IEEE International Symposium}} on {{Logistics}} and {{Industrial Informatics}}},
  author = {Erdodi, L. and Erd{\H o}di, L.},
  year = {2012},
  month = sep,
  pages = {251--254},
  publisher = {IEEE},
  address = {Smolenice, Slovakia},
  doi = {10.1109/lindi.2012.6319497},
  urldate = {2025-07-15},
  abstract = {File compression in the case of large files can be time consuming and it is not even necessarily effective. Vast majority of the compression software use algorithms with implementations for CPU architecture. From the beginning of the 2000's the performance of graphic processing units (GPU) have been continuously increasing and at the present time in some cases the GPU exceeds the CPU in performance. However this high performance of the GPU is rarely exploited except in the case of some special tasks such as password cracking or linear algebra calculations. One of the most well-known compression algorithms is the LZO (Lempel-Ziv-Oberhumer). This study discusses the possible ways for the implementation of LZO for GPU Fermi architecture. Three different algorithms are provided and compared and finally it is also shown that the use of GPU can significantly decrease the time of the file compression.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/I283SFBB/Erdodi and Erdődi - 2012 - File compression with LZO algorithm using NVIDIA CUDA architecture.pdf}
}

@article{erdosSpectralStatisticsErdH2012,
  title = {Spectral {{Statistics}} of {{Erd}}\{{\textbackslash}\vphantom\}{{H}} O\vphantom\{\}s-{{R}}{\textbackslash}'enyi {{Graphs II}}: {{Eigenvalue Spacing}} and the {{Extreme Eigenvalues}}},
  shorttitle = {Spectral {{Statistics}} of {{Erd}}\{{\textbackslash}\vphantom\}{{H}} O\vphantom\{\}s-{{R}}{\textbackslash}'enyi {{Graphs II}}},
  author = {Erdos, Laszlo and Knowles, Antti and Yau, Horng-Tzer and Yin, Jun},
  year = {2012},
  month = sep,
  journal = {Communications in Mathematical Physics},
  volume = {314},
  number = {3},
  eprint = {1103.3869},
  primaryclass = {math-ph},
  pages = {587--640},
  issn = {0010-3616, 1432-0916},
  doi = {10.1007/s00220-012-1527-7},
  urldate = {2023-05-30},
  abstract = {We consider the ensemble of adjacency matrices of Erd\{{\textbackslash}H o\}s-R{\textbackslash}'enyi random graphs, i.e.{\textbackslash} graphs on \$N\$ vertices where every edge is chosen independently and with probability \$p {\textbackslash}equiv p(N)\$. We rescale the matrix so that its bulk eigenvalues are of order one. Under the assumption \$p N {\textbackslash}gg N{\textasciicircum}\{2/3\}\$, we prove the universality of eigenvalue distributions both in the bulk and at the edge of the spectrum. More precisely, we prove (1) that the eigenvalue spacing of the Erd\{{\textbackslash}H o\}s-R{\textbackslash}'enyi graph in the bulk of the spectrum has the same distribution as that of the Gaussian orthogonal ensemble; and (2) that the second largest eigenvalue of the Erd\{{\textbackslash}H o\}s-R{\textbackslash}'enyi graph has the same distribution as the largest eigenvalue of the Gaussian orthogonal ensemble. As an application of our method, we prove the bulk universality of generalized Wigner matrices under the assumption that the matrix entries have at least \$4 + {\textbackslash}epsilon\$ moments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {15B52 60B20 05C80,Mathematical Physics,Mathematics - Probability},
  annotation = {GSCC: 0000029},
  file = {/home/elessar/Zotero/storage/4MYAUR87/Erdos et al. - 2012 - Spectral Statistics of Erd H o s-R'enyi Graphs I.pdf}
}

@inbook{ernstInformationContentTwoDimensional1982,
  title = {The {{Information Content}} of {{Two-Dimensional Fourier Spectroscopy}}},
  booktitle = {{{ACS Symposium Series}}},
  year = {1982},
  month = jul,
  pages = {47--61},
  publisher = {AMERICAN CHEMICAL SOCIETY},
  address = {WASHINGTON, D. C.},
  issn = {0097-6156, 1947-5918},
  doi = {10.1021/bk-1982-0191.ch004},
  urldate = {2025-07-25},
  collaborator = {Ernst, R. R.},
  isbn = {978-0-8412-0723-3 978-0-8412-0892-6},
  langid = {english},
  file = {/home/elessar/Zotero/storage/6ZMUG2GS/1982 - The Information Content of Two-Dimensional Fourier Spectroscopy.pdf}
}

@article{estevez-moyaComplexityTransitionChaos2023,
  title = {Complexity and Transition to Chaos in Coupled {{Adler-type}} Oscillators},
  author = {{Estevez-Moya}, D. and {Estevez-Rams}, E. and Kantz, H.},
  year = {2023},
  month = apr,
  journal = {Physical Review E},
  volume = {107},
  number = {4},
  publisher = {American Physical Society (APS)},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/physreve.107.044212},
  urldate = {2025-07-15},
  copyright = {https://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/X7Z2XIBL/Estevez-Moya et al. - 2023 - Complexity and transition to chaos in coupled Adler-type oscillators.pdf}
}

@article{estevez-ramsComplexityentropyAnalysisDifferent2019,
  title = {Complexity-Entropy Analysis at Different Levels of Organisation in Written Language},
  author = {{Estevez-Rams}, Ernesto and {Mesa-Rodriguez}, Ania and {Estevez-Moya}, Daniel},
  editor = {Amancio, Diego Raphael},
  year = {2019},
  month = may,
  journal = {PLOS ONE},
  volume = {14},
  number = {5},
  pages = {e0214863},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0214863},
  urldate = {2023-05-30},
  abstract = {Written language is complex. A written text can be considered an attempt to convey a meaningful message which ends up being constrained by language rules, context dependence and highly redundant in its use of resources. Despite all these constraints, unpredictability is an essential element of natural language. Here we present the use of entropic measures to assert the balance between predictability and surprise in written text. In short, it is possible to measure innovation and context preservation in a document. It is shown that this can also be done at the different levels of organization of a text. The type of analysis presented is reasonably general, and can also be used to analyze the same balance in other complex messages such as DNA, where a hierarchy of organizational levels are known to exist.},
  copyright = {CC0 1.0 Universal Public Domain Dedication},
  langid = {english},
  annotation = {GSCC: 0000016},
  file = {/home/elessar/Zotero/storage/MPWCNMKQ/Estevez-Rams et al. - 2019 - Complexity-entropy analysis at different levels of.pdf}
}

@article{estevez-ramsComputationalCapabilitiesEdge2019,
  title = {Computational Capabilities at the Edge of Chaos for One Dimensional Systems Undergoing Continuous Transitions},
  author = {{Estevez-Rams}, E. and {Estevez-Moya}, D. and {Garcia-Medina}, K. and {Lora-Serrano}, R.},
  year = {2019},
  month = apr,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {29},
  number = {4},
  pages = {043105},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.5062364},
  urldate = {2023-05-30},
  copyright = {CC0 1.0 Universal Public Domain Dedication},
  langid = {english},
  annotation = {GSCC: 0000014},
  file = {/home/elessar/Zotero/storage/PGYCHU4H/Estevez-Rams et al. - 2019 - Computational capabilities at the edge of chaos fo.pdf}
}

@article{estevez-ramsPhenomenologyCoupledNonlinear2018,
  title = {Phenomenology of Coupled Nonlinear Oscillators},
  author = {{Estevez-Rams}, E. and {Estevez-Moya}, D. and {Arag{\'o}n-Fern{\'a}ndez}, B.},
  year = {2018},
  month = feb,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {28},
  number = {2},
  pages = {023110},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.5007747},
  urldate = {2023-05-30},
  copyright = {CC0 1.0 Universal Public Domain Dedication},
  langid = {english},
  annotation = {GSCC: 0000019},
  file = {/home/elessar/Zotero/storage/I5268WRL/Estevez-Rams et al. - 2018 - Phenomenology of coupled nonlinear oscillators.pdf}
}

@article{estevezAnalisisComplejidadJuego2017,
  title = {An{\'a}lisis de La Complejidad Del {{Juego}} de {{Minor{\'{\i}}a}}},
  author = {Estevez, Daniel},
  year = {2017},
  file = {/home/elessar/Zotero/storage/H7C64KKP/thesis.pdf}
}

@article{estevezAnalisisEntropicoComplejidad2020,
  title = {{An{\'a}lisis entr{\'o}pico de la complejidad. Mapas circulares y osciladores acoplados no lineales}},
  author = {Est{\'e}vez, Daniel},
  year = {2020},
  langid = {spanish},
  file = {/home/elessar/Zotero/storage/I3PWNWWA/Estévez - Análisis entrópico de la complejidad. Mapas circulares y osciladores acoplados no lineales.pdf}
}

@article{eyebefoudaDetectingRegularDynamics2015,
  title = {Detecting Regular Dynamics from Time Series Using Permutations Slopes},
  author = {Eyebe Fouda, J.S. Armand and Koepf, Wolfram},
  year = {2015},
  month = oct,
  journal = {Communications in Nonlinear Science and Numerical Simulation},
  volume = {27},
  number = {1-3},
  pages = {216--227},
  issn = {10075704},
  doi = {10.1016/j.cnsns.2015.03.008},
  urldate = {2025-02-13},
  abstract = {In this paper we present the entropy related to the largest slope of the permutation as an efficient approach for distinguishing between regular and non-regular dynamics, as well as the similarities between this method and the three-state test (3ST) algorithm. We theoretically establish that for suitably chosen delay times, permutations generated in the case of regular dynamics present the same largest slope if their order is greater than the period of the underlying orbit. This investigation helps making a clear decision (even in a noisy environment) in the detection of regular dynamics with large periods for which PE gives an arbitrary nonzero complexity measure.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/XLHRRDT8/Eyebe Fouda and Koepf - 2015 - Detecting regular dynamics from time series using permutations slopes.pdf}
}

@article{fahnerEntropyEstimatesDynamical,
  title = {Entropy {{Estimates}} for {{Dynamical Systems}}},
  author = {Fahner, Gerald},
  langid = {english},
  file = {/home/elessar/Zotero/storage/6YJZPACP/Fahner - Entropy Estimates for Dynamical Systems.pdf}
}

@misc{fanLongtermPredictionChaotic2020,
  title = {Long-Term Prediction of Chaotic Systems with Recurrent Neural Networks},
  author = {Fan, Huawei and Jiang, Junjie and Zhang, Chun and Wang, Xingang and Lai, Ying-Cheng},
  year = {2020},
  month = mar,
  number = {arXiv:2004.01258},
  eprint = {2004.01258},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.01258},
  urldate = {2025-07-28},
  abstract = {Reservoir computing systems, a class of recurrent neural networks, have recently been exploited for model-free, data-based prediction of the state evolution of a variety of chaotic dynamical systems. The prediction horizon demonstrated has been about half dozen Lyapunov time. Is it possible to significantly extend the prediction time beyond what has been achieved so far? We articulate a scheme incorporating time-dependent but sparse data inputs into reservoir computing and demonstrate that such rare "updates" of the actual state practically enable an arbitrarily long prediction horizon for a variety of chaotic systems. A physical understanding based on the theory of temporal synchronization is developed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Nonlinear Sciences - Adaptation and Self-Organizing Systems,Statistics - Machine Learning},
  file = {/home/elessar/Zotero/storage/KY3X468K/Fan et al. - 2020 - Long-term prediction of chaotic systems with recurrent neural networks.pdf;/home/elessar/Zotero/storage/D8G9NFNN/2004.html}
}

@article{fanLongtermPredictionChaotic2020a,
  title = {Long-Term Prediction of Chaotic Systems with Machine Learning},
  author = {Fan, Huawei and Jiang, Junjie and Zhang, Chun and Wang, Xingang and Lai, Ying-Cheng},
  year = {2020},
  month = mar,
  journal = {Physical Review Research},
  volume = {2},
  number = {1},
  pages = {012080},
  issn = {2643-1564},
  doi = {10.1103/PhysRevResearch.2.012080},
  urldate = {2025-08-12},
  langid = {english},
  file = {/home/elessar/Zotero/storage/NPZZG62P/Fan et al. - 2020 - Long-term prediction of chaotic systems with machine learning.pdf}
}

@article{farkasComputationalAnalysisMemory2016,
  title = {Computational Analysis of Memory Capacity in Echo State Networks},
  author = {Farka{\v s}, Igor and Bos{\'a}k, Radom{\'i}r and Gerge{\v l}, Peter},
  year = {2016},
  month = nov,
  journal = {Neural Networks},
  volume = {83},
  pages = {109--120},
  issn = {08936080},
  doi = {10.1016/j.neunet.2016.07.012},
  urldate = {2025-03-20},
  abstract = {Reservoir computing became very popular due to its potential for efficient design of recurrent neural networks, exploiting the computational properties of the reservoir structure. Various approaches, ranging from appropriate reservoir initialization to its optimization by training have been proposed. In this paper, we extend our previous work and focus on short-term memory capacity, introduced by Jaeger in case of echo state networks. Memory capacity has been previously shown to peak at criticality, when the network switches from a stable regime to an unstable dynamic regime. Using computational experiments with nonlinear ESNs, we systematically analyze the memory capacity from the perspective of several parameters and their relationship, namely the input and reservoir weights scaling, reservoir size and its sparsity. We also derive and test two gradient descent based orthogonalization procedures for recurrent weights matrix, which considerably increase the memory capacity, approaching the upper bound, which is equal to the reservoir size, as proved for linear reservoirs. Orthogonalization procedures are discussed in the context of existing methods and their benefit is assessed.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/HT4GUWZW/Farkaš et al. - 2016 - Computational analysis of memory capacity in echo state networks.pdf}
}

@article{farmerRenormalizationQuasiperiodicTransition1985,
  title = {Renormalization of the Quasiperiodic Transition to Chaos for Arbitrary Winding Numbers},
  author = {Farmer, J. Doyne and Satija, Indubala I.},
  year = {1985},
  month = may,
  journal = {Physical Review A},
  volume = {31},
  number = {5},
  pages = {3520--3522},
  issn = {0556-2791},
  doi = {10.1103/PhysRevA.31.3520},
  urldate = {2025-07-29},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/DANVQMHJ/Farmer and Satija - 1985 - Renormalization of the quasiperiodic transition to chaos for arbitrary winding numbers.pdf}
}

@article{federUniversalPredictionIndividual,
  title = {Universal {{Prediction}} of {{Individual Sequences}}},
  author = {Feder, Meir and Merhav, Neri and Gutman, Michael},
  langid = {english},
  file = {/home/elessar/Zotero/storage/VPJX5BK5/Feder et al. - Universal Prediction of Individual Sequences.pdf;/home/elessar/Zotero/storage/VQDZPI69/Feder et al. - Universal Prediction of Individual Sequences.pdf}
}

@article{feigcnbaumUniversalMetricProperties,
  title = {The Universal Metric Properties of Nonlinear Transformations},
  author = {Feigcnbaum, Mitchell J},
  langid = {english},
  file = {/home/elessar/Zotero/storage/H4RAY43P/Feigcnbaum - The universal metric properties of nonlinear transformations.pdf}
}

@article{feigenbaumQuantitativeUniversalityClass1978,
  title = {Quantitative Universality for a Class of Nonlinear Transformations},
  author = {Feigenbaum, Mitchell J.},
  year = {1978},
  month = jul,
  journal = {Journal of Statistical Physics},
  volume = {19},
  number = {1},
  pages = {25--52},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {0022-4715, 1572-9613},
  doi = {10.1007/bf01020332},
  urldate = {2025-07-25},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/home/elessar/Zotero/storage/9K2WVE4T/Feigenbaum - 1978 - Quantitative universality for a class of nonlinear transformations.pdf}
}

@article{feldmanOrganizationIntrinsicComputation2008,
  title = {The {{Organization}} of {{Intrinsic Computation}}: {{Complexity-Entropy Diagrams}} and the {{Diversity}} of {{Natural Information Processing}}},
  shorttitle = {The {{Organization}} of {{Intrinsic Computation}}},
  author = {Feldman, David P. and McTague, Carl S. and Crutchfield, James P.},
  year = {2008},
  month = dec,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {18},
  number = {4},
  eprint = {0806.4789},
  primaryclass = {nlin},
  pages = {043106},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.2991106},
  urldate = {2024-01-10},
  abstract = {Intrinsic computation refers to how dynamical systems store, structure, and transform historical and spatial information. By graphing a measure of structural complexity against a measure of randomness, complexity-entropy diagrams display the range and different kinds of intrinsic computation across an entire class of system. Here, we use complexity-entropy diagrams to analyze intrinsic computation in a broad array of deterministic nonlinear and linear stochastic processes, including maps of the interval, cellular automata and Ising spin systems in one and two dimensions, Markov chains, and probabilistic minimal finite-state machines. Since complexity-entropy diagrams are a function only of observed configurations, they can be used to compare systems without reference to system coordinates or parameters. It has been known for some time that in special cases complexity-entropy diagrams reveal that high degrees of information processing are associated with phase transitions in the underlying process space, the so-called ``edge of chaos''. Generally, though, complexity-entropy diagrams differ substantially in character, demonstrating a genuine diversity of distinct kinds of intrinsic computation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Nonlinear Sciences - Cellular Automata and Lattice Gases,Nonlinear Sciences - Chaotic Dynamics},
  annotation = {GSCC: 0000191},
  file = {/home/elessar/Zotero/storage/C3LUXQLF/Feldman et al. - 2008 - The Organization of Intrinsic Computation Complexity-Entropy Diagrams and the Diversity of Natural.pdf;/home/elessar/Zotero/storage/VCKF5C5P/Feldman et al_2008_The Organization of Intrinsic Computation.pdf}
}

@misc{fernandezTeLUActivationFunction2025,
  title = {{{TeLU Activation Function}} for {{Fast}} and {{Stable Deep Learning}}},
  author = {Fernandez, Alfredo and Mali, Ankur},
  year = {2025},
  month = jan,
  number = {arXiv:2412.20269},
  eprint = {2412.20269},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.20269},
  urldate = {2025-02-18},
  abstract = {We propose the Hyperbolic Tangent Exponential Linear Unit (TeLU), a neural network hidden activation function defined as TeLU(x)=xtanh(exp(x)). TeLU's design is grounded in the core principles of key activation functions, achieving strong convergence by closely approximating the identity function in its active region while effectively mitigating the vanishing gradient problem in its saturating region. Its simple formulation enhances computational efficiency, leading to improvements in scalability and convergence speed. Unlike many modern activation functions, TeLU seamlessly combines the simplicity and effectiveness of ReLU with the smoothness and analytic properties essential for learning stability in deep neural networks. TeLU's ability to mimic the behavior and optimal hyperparameter settings of ReLU, while introducing the benefits of smoothness and curvature, makes it an ideal drop-in replacement. Its analytic nature positions TeLU as a powerful universal approximator, enhancing both robustness and generalization across a multitude of experiments. We rigorously validate these claims through theoretical analysis and experimental validation, demonstrating TeLU's performance across challenging benchmarks; including ResNet18 on ImageNet, Dynamic-Pooling Transformers on Text8, and Recurrent Neural Networks (RNNs) on the Penn TreeBank dataset. These results highlight TeLU's potential to set a new standard in activation functions, driving more efficient and stable learning in deep neural networks, thereby accelerating scientific discoveries across various fields.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/elessar/Zotero/storage/FJBGHWP8/Fernandez and Mali - 2025 - TeLU Activation Function for Fast and Stable Deep Learning.pdf;/home/elessar/Zotero/storage/P72VQB4V/2412.html}
}

@inproceedings{ferrarioComplexityAnalysis24,
  title = {Complexity Analysis of 24 Hours Heart Rate Variability Time Series},
  booktitle = {The 26th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}}},
  author = {Ferrario, M. and Signorini, M.G. and Cerutti, S.},
  volume = {4},
  pages = {3956--3959},
  publisher = {IEEE},
  address = {San Francisco, CA, USA},
  doi = {10.1109/iembs.2004.1404105},
  urldate = {2025-07-15},
  langid = {english},
  file = {/home/elessar/Zotero/storage/65SJZDQJ/Ferrario et al. - Complexity analysis of 24 hours heart rate variability time series.pdf}
}

@article{ferreiraApproachReservoirComputing2013,
  title = {An Approach to Reservoir Computing Design and Training},
  author = {Ferreira, Aida A. and Ludermir, Teresa B. and De Aquino, Ronaldo R.B.},
  year = {2013},
  month = aug,
  journal = {Expert Systems with Applications},
  volume = {40},
  number = {10},
  pages = {4172--4182},
  issn = {09574174},
  doi = {10.1016/j.eswa.2013.01.029},
  urldate = {2025-03-12},
  abstract = {Reservoir computing is a framework for computation like a recurrent neural network that allows for the black box modeling of dynamical systems. In contrast to other recurrent neural network approaches, reservoir computing does not train the input and internal weights of the network, only the readout is trained. However it is necessary to adjust parameters to create a ``good'' reservoir for a given application. In this study we introduce a method, called RCDESIGN (reservoir computing and design training). RCDESIGN combines an evolutionary algorithm with reservoir computing and simultaneously looks for the best values of parameters, topology and weight matrices without rescaling the reservoir matrix by the spectral radius. The idea of adjust the spectral radius within the unit circle in the complex plane comes from the linear system theory. However, this argument does not necessarily apply to nonlinear systems, which is the case of reservoir computing. The results obtained with the proposed method are compared with results obtained by a genetic algorithm search for global parameters generation of reservoir computing. Four time series were used to validate RCDESIGN.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/EP926D5N/Ferreira et al. - 2013 - An approach to reservoir computing design and training.pdf}
}

@article{ferreiraTimeSeriesAnalysis2003,
  title = {Time Series Analysis for Minority Game Simulations of Financial Markets},
  author = {Ferreira, Fernando F. and Francisco, Gerson and Machado, Birajara S. and Muruganandam, Paulsamy},
  year = {2003},
  month = apr,
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {321},
  number = {3-4},
  eprint = {physics/0203038},
  pages = {619--632},
  issn = {0378-4371},
  doi = {10.1016/S0378-4371(02)01733-8},
  urldate = {2025-07-24},
  abstract = {The minority game (MG) model introduced recently provides promising insights into the understanding of the evolution of prices, indices and rates in the financial markets. In this paper we perform a time series analysis of the model employing tools from statistics, dynamical systems theory and stochastic processes. Using benchmark systems and a financial index for comparison, several conclusions are obtained about the generating mechanism for this kind of evolution. The motion is deterministic, driven by occasional random external perturbation. When the interval between two successive perturbations is sufficiently large, one can find low dimensional chaos in this regime. However, the full motion of the MG model is found to be similar to that of the first differences of the SP500 index: stochastic, nonlinear and (unit root) stationary.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Nonlinear Sciences - Chaotic Dynamics,Physics - Data Analysis Statistics and Probability,Quantitative Finance - Trading and Market Microstructure},
  file = {/home/elessar/Zotero/storage/SRPCLV5D/Ferreira et al. - 2003 - Time series analysis for minority game simulations of financial markets.pdf}
}

@incollection{feurerHyperparameterOptimization2019,
  title = {Hyperparameter {{Optimization}}},
  booktitle = {Automated {{Machine Learning}}},
  author = {Feurer, Matthias and Hutter, Frank},
  editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  year = {2019},
  pages = {3--33},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-05318-5_1},
  urldate = {2024-01-10},
  abstract = {Recent interest in complex and computationally expensive machine learning models with many hyperparameters, such as automated machine learning (AutoML) frameworks and deep neural networks, has resulted in a resurgence of research on hyperparameter optimization (HPO). In this chapter, we give an overview of the most prominent approaches for HPO. We first discuss blackbox function optimization methods based on model-free methods and Bayesian optimization. Since the high computational demand of many modern machine learning applications renders pure blackbox optimization extremely costly, we next focus on modern multi-fidelity methods that use (much) cheaper variants of the blackbox function to approximately assess the quality of hyperparameter settings. Lastly, we point to open problems and future research directions.},
  isbn = {978-3-030-05317-8 978-3-030-05318-5},
  langid = {english},
  annotation = {GSCC: 0001642},
  file = {/home/elessar/Zotero/storage/ARDZFP8F/Feurer and Hutter - 2019 - Hyperparameter Optimization.pdf}
}

@article{figueiraRandomnessUniversalMachines2006,
  title = {Randomness and Universal Machines},
  author = {Figueira, Santiago and Stephan, Frank and Wu, Guohua},
  year = {2006},
  month = dec,
  journal = {Journal of Complexity},
  volume = {22},
  number = {6},
  pages = {738--751},
  publisher = {Elsevier BV},
  issn = {0885-064X},
  doi = {10.1016/j.jco.2006.05.001},
  urldate = {2025-07-15},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/LFUWAVHQ/Figueira et al. - 2006 - Randomness and universal machines.pdf}
}

@misc{fischerLempelZivComputation2015,
  title = {Lempel {{Ziv Computation In Small Space}} ({{LZ-CISS}})},
  author = {Fischer, Johannes and I, Tomohiro and K{\"o}ppl, Dominik},
  year = {2015},
  month = apr,
  number = {arXiv:1504.02605},
  eprint = {1504.02605},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1504.02605},
  urldate = {2025-07-15},
  abstract = {For both the Lempel Ziv 77- and 78-factorization we propose algorithms generating the respective factorization using (1 + {\k o})n lg n + O(n) bits (for any positive constant {\k o} {$\leq$} 1) working space (including the space for the output) for any text of size n over an integer alphabet in O(n/{\k o}2) time.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/home/elessar/Zotero/storage/A7C6ITNI/Fischer et al. - 2015 - Lempel Ziv Computation In Small Space (LZ-CISS).pdf}
}

@article{fokkinkHankelMatricesPerioddoubling2017,
  title = {Hankel Matrices for the Period-Doubling Sequence},
  author = {Fokkink, Robbert J. and Kraaikamp, Cor and Shallit, Jeffrey},
  year = {2017},
  month = feb,
  journal = {Indagationes Mathematicae},
  volume = {28},
  number = {1},
  pages = {108--119},
  publisher = {Elsevier BV},
  issn = {0019-3577},
  doi = {10.1016/j.indag.2016.11.008},
  urldate = {2025-07-25},
  abstract = {We give an explicit evaluation, in terms of products of Jacobsthal numbers, of the Hankel determinants of order a power of two for the period-doubling sequence. We also explicitly give the eigenvalues and eigenvectors of the corresponding Hankel matrices. Similar considerations give the Hankel determinants for other orders.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/MCIBXJUD/Fokkink et al. - 2017 - Hankel matrices for the period-doubling sequence.pdf}
}

@article{fongUniversalityRealMinimal2025,
  title = {Universality of {{Real Minimal Complexity Reservoir}}},
  author = {Fong, Robert Simon and Li, Boyu and Tino, Peter},
  year = {2025},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {39},
  number = {16},
  pages = {16622--16629},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v39i16.33826},
  urldate = {2025-08-12},
  abstract = {Reservoir Computing (RC) models, a subclass of recurrent neural networks, are distinguished by their fixed, non-trainable input layer and dynamically coupled reservoir, with only the static readout layer being trained. This design circumvents the issues associated with backpropagating error signals through time, thereby enhancing both stability and training efficiency. RC models have been successfully applied across a broad range of application domains. Crucially, they have been demonstrated to be universal approximators of time-invariant dynamic filters with fading memory, under various settings of approximation norms and input driving sources. Simple Cycle Reservoirs (SCR) represent a specialized class of RC models with a highly constrained reservoir architecture, characterized by uniform ring connectivity and binary input-to-reservoir weights with an aperiodic sign pattern. For linear reservoirs, given the reservoir size, the reservoir construction has only one degree of freedom -- the reservoir cycle weight. Such architectures are particularly amenable to hardware implementations without significant performance degradation in many practical tasks. In this study we endow these observations with solid theoretical foundations by proving that SCRs operating in real domain are universal approximators of time-invariant dynamic filters with fading memory. Our results supplement recent research showing that SCRs in the complex domain can approximate, to arbitrary precision, any unrestricted linear reservoir with a non-linear readout. We furthermore introduce a novel method to drastically reduce the number of SCR units, making such highly constrained architectures natural candidates for low-complexity hardware implementations. Our findings are supported by empirical studies on real-world time series datasets.}
}

@article{franekCrochemoresAlgorithmRepetitions,
  title = {Crochemore's Algorithm for Repetitions Revisited - Computing Runs},
  author = {Franek, F and Jiang, M},
  langid = {english},
  file = {/home/elessar/Zotero/storage/U6E9VKC5/Franek and Jiang - Crochemore’s algorithm for repetitions revisited - computing runs.pdf}
}

@article{friggSelforganisedCriticalityWhat2003,
  title = {Self-Organised Criticality---What It Is and What It Isn't},
  author = {Frigg, Roman},
  year = {2003},
  month = sep,
  journal = {Studies in History and Philosophy of Science Part A},
  volume = {34},
  number = {3},
  pages = {613--632},
  publisher = {Elsevier BV},
  issn = {0039-3681},
  doi = {10.1016/s0039-3681(03)00046-3},
  urldate = {2025-07-15},
  abstract = {The last decade and a half has seen an ardent development of self-organised criticality (SOC), a new approach to complex systems, which has become important in many domains of natural as well as social science, such as geology, biology, astronomy, and economics, to mention just a few. This has led many to adopt a generalist stance towards SOC, which is now repeatedly claimed to be a universal theory of complex behaviour. The aim of this paper is twofold. First, I provide a brief and non-technical introduction to SOC. Second, I critically discuss the various bold claims that have been made in connection with it. Throughout, I will adopt a rather sober attitude and argue that some people have been too readily carried away by fancy contentions. My overall conclusion will be that none of these bold claims can be maintained. Nevertheless, stripped of exaggerated expectations and daring assertions, many SOC models are interesting vehicles for promising scientific research.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/AAQCV7E7/Frigg - 2003 - Self-organised criticality—what it is and what it isn’t.pdf}
}

@article{fryFINITESTATEMACHINES,
  title = {{{FINITE-STATE MACHINES}}},
  author = {Fry, John},
  langid = {english},
  file = {/home/elessar/Zotero/storage/AT5DCJXW/Fry - FINITE-STATE MACHINES.pdf}
}

@article{fuDoublecycleEchoState2023,
  title = {A Double-Cycle Echo State Network Topology for Time Series Prediction},
  author = {Fu, Jun and Li, Guangli and Tang, Jianfeng and Xia, Lei and Wang, Lidan and Duan, Shukai},
  year = {2023},
  month = sep,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {33},
  number = {9},
  pages = {093113},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0159966},
  urldate = {2025-08-12},
  abstract = {Echo state network (ESN) has gained wide acceptance in the field of time series prediction, relying on sufficiently complex reservoir connections to remember the historical features of the data and using these features to obtain the outputs by a simple linear readout. However, the randomness of its input and reservoir connections pose negative impacts on the prediction performance and performance stability of the models, the complexity of reservoir connections brings high time consumption during network computing, and the presence of randomness and complexity makes the hardware implementation of the ESN difficult. In response, we propose a double-cycle ESN (DCESN) based on the Li-ESN model, which has fixed weights to improve prediction performance and performance stability and simpler reservoir connections compared to the classical ESN to reduce the time consumption. The existence of both greatly reduces the difficulty of hardware implementation of the ESN and provides many conveniences for the future application of the ESN. Experimental results on many widely used time series datasets show that the DCESN has comparable or even better prediction performance than the ESN and good robustness against noise and parameter fluctuations.},
  langid = {english}
}

@article{funahashiApproximationDynamicalSystems1993,
  title = {Approximation of Dynamical Systems by Continuous Time Recurrent Neural Networks},
  author = {Funahashi, Ken-ichi and Nakamura, Yuichi},
  year = {1993},
  month = jan,
  journal = {Neural Networks},
  volume = {6},
  number = {6},
  pages = {801--806},
  issn = {08936080},
  doi = {10.1016/S0893-6080(05)80125-X},
  urldate = {2023-05-30},
  abstract = {In this paper, we prove that any finite time trajectory of a given n-dimensional dynamical system can be approximately realized by the internal state of the output units of a continuous time recurrent neural network with n output units, some hidden units, and an appropriate initial condition. The essential idea ofthe proof is to embed the n-dimensional dynamical system into a higher dimensional one which defines a recurrent neural network. As a corollary, we also show that any continuous curve can be approximated by the output o f a recurrent neural network.},
  langid = {english},
  annotation = {GSCC: 0001345},
  file = {/home/elessar/Zotero/storage/F2XVFBPE/funahashi1993.pdf.pdf;/home/elessar/Zotero/storage/MNU3FKQ9/Funahashi and Nakamura - 1993 - Approximation of dynamical systems by continuous t.pdf}
}

@article{galindoInformationComputationClassical2002,
  title = {Information and Computation: {{Classical}} and Quantum Aspects},
  shorttitle = {Information and Computation},
  author = {Galindo, A. and {Mart{\'i}n-Delgado}, M. A.},
  year = {2002},
  month = may,
  journal = {Reviews of Modern Physics},
  volume = {74},
  number = {2},
  pages = {347--423},
  publisher = {American Physical Society (APS)},
  issn = {0034-6861, 1539-0756},
  doi = {10.1103/revmodphys.74.347},
  urldate = {2025-07-15},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/43QIVPK6/Galindo and Martín-Delgado - 2002 - Information and computation Classical and quantum aspects.pdf}
}

@article{gallicchioArchitecturalMarkovianFactors2011,
  title = {Architectural and {{Markovian}} Factors of Echo State Networks},
  author = {Gallicchio, Claudio and Micheli, Alessio},
  year = {2011},
  month = jun,
  journal = {Neural Networks},
  volume = {24},
  number = {5},
  pages = {440--456},
  issn = {08936080},
  doi = {10.1016/j.neunet.2011.02.002},
  urldate = {2023-05-30},
  abstract = {Echo State Networks (ESNs) constitute an emerging approach for efficiently modeling Recurrent Neural Networks (RNNs). In this paper we investigate some of the main aspects that can be accounted for the success and limitations of this class of models. In particular, we propose complementary classes of factors related to contractivity and architecture of reservoirs and we study their relative relevance.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  annotation = {GSCC: 0000212},
  file = {/home/elessar/Zotero/storage/RZQ5N74E/Gallicchio and Micheli - 2011 - Architectural and Markovian factors of echo state .pdf}
}

@misc{gallicchioChasingEchoState2019,
  title = {Chasing the {{Echo State Property}}},
  author = {Gallicchio, Claudio},
  year = {2019},
  month = sep,
  number = {arXiv:1811.10892},
  eprint = {1811.10892},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-05-30},
  abstract = {Reservoir Computing (RC) provides an efficient way for designing dynamical recurrent neural models. While training is restricted to a simple output component, the recurrent connections are left untrained after initialization, subject to stability constraints specified by the Echo State Property (ESP). Literature conditions for the ESP typically fail to properly account for the effects of driving input signals, often limiting the potentialities of the RC approach. In this paper, we study the fundamental aspect of asymptotic stability of RC models in presence of driving input, introducing an empirical ESP index that enables to easily analyze the stability regimes of reservoirs. Results on two benchmark datasets reveal interesting insights on the dynamical properties of input-driven reservoirs, suggesting that the actual domain of ESP validity is much wider than what covered by literature conditions commonly used in RC practice.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  annotation = {GSCC: 0000033},
  file = {/home/elessar/Zotero/storage/JGLMP5PN/Gallicchio - 2019 - Chasing the Echo State Property.pdf}
}

@misc{gallicchioDeepEchoState2020,
  title = {Deep {{Echo State Network}} ({{DeepESN}}): {{A Brief Survey}}},
  shorttitle = {Deep {{Echo State Network}} ({{DeepESN}})},
  author = {Gallicchio, Claudio and Micheli, Alessio},
  year = {2020},
  month = sep,
  number = {arXiv:1712.04323},
  eprint = {1712.04323},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1712.04323},
  urldate = {2025-03-07},
  abstract = {The study of deep recurrent neural networks (RNNs) and, in particular, of deep Reservoir Computing (RC) is gaining an increasing research attention in the neural networks community. The recently introduced Deep Echo State Network (DeepESN) model opened the way to an extremely efficient approach for designing deep neural networks for temporal data. At the same time, the study of DeepESNs allowed to shed light on the intrinsic properties of state dynamics developed by hierarchical compositions of recurrent layers, i.e. on the bias of depth in RNNs architectural design. In this paper, we summarize the advancements in the development, analysis and applications of DeepESNs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/elessar/Zotero/storage/NMZF4QTM/Gallicchio and Micheli - 2020 - Deep Echo State Network (DeepESN) A Brief Survey.pdf;/home/elessar/Zotero/storage/LUJ2KJ7H/1712.html}
}

@article{gallicchioDeepReservoirComputing2016,
  title = {Deep {{Reservoir Computing}}: {{A Critical Analysis}}},
  author = {Gallicchio, Claudio and Micheli, Alessio},
  year = {2016},
  journal = {Computational Intelligence},
  abstract = {In this paper we propose an empirical analysis of deep recurrent neural networks (RNNs) with stacked layers. The analysis aims at the study and proposal of approaches to develop and enhance multiple timescale and hierarchical dynamics in deep recurrent architectures, within the efficient Reservoir Computing (RC) approach for RNN modeling. Results point out the actual relevance of layering and RC parameters aspects on the diversification of temporal representations in deep recurrent models.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/6YMFNFPF/Gallicchio and Micheli - 2016 - Deep Reservoir Computing A Critical Analysis.pdf}
}

@article{gallicchioDesignDeepEcho2018,
  title = {Design of Deep Echo State Networks},
  author = {Gallicchio, Claudio and Micheli, Alessio and Pedrelli, Luca},
  year = {2018},
  month = dec,
  journal = {Neural Networks},
  volume = {108},
  pages = {33--47},
  publisher = {Elsevier BV},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2018.08.002},
  urldate = {2025-07-24},
  abstract = {In this paper, we provide a novel approach to the architectural design of deep Recurrent Neural Networks using signal frequency analysis. In particular, focusing on the Reservoir Computing framework and inspired by the principles related to the inherent effect of layering, we address a fundamental open issue in deep learning, namely the question of how to establish the number of layers in recurrent architectures in the form of deep echo state networks (DeepESNs). The proposed method is first analyzed and refined on a controlled scenario and then it is experimentally assessed on challenging real-world tasks. The achieved results also show the ability of properly designed DeepESNs to outperform RC approaches on a speech recognition task, and to compete with the state-of-the-art in time-series prediction on polyphonic music tasks.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/F23Q5854/Gallicchio et al. - 2018 - Design of deep echo state networks.pdf}
}

@article{gallicchioEulerStateNetworks2024,
  title = {Euler {{State Networks}}: {{Non-dissipative Reservoir Computing}}},
  shorttitle = {Euler {{State Networks}}},
  author = {Gallicchio, Claudio},
  year = {2024},
  month = apr,
  journal = {Neurocomputing},
  volume = {579},
  pages = {127411},
  issn = {09252312},
  doi = {10.1016/j.neucom.2024.127411},
  urldate = {2024-11-04},
  abstract = {Inspired by the numerical solution of ordinary differential equations, in this paper, we propose a novel Reservoir Computing (RC) model, called the Euler State Network (EuSN). The presented approach makes use of forward Euler discretization and antisymmetric recurrent matrices to design reservoir dynamics that are both stable and non-dissipative by construction.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/L8TYGCJR/Gallicchio - 2024 - Euler State Networks Non-dissipative Reservoir Computing.pdf}
}

@inproceedings{gallicchioFastSpectralRadius2020,
  title = {Fast {{Spectral Radius Initialization}} for {{Recurrent Neural Networks}}},
  booktitle = {Recent {{Advances}} in {{Big Data}} and {{Deep Learning}}},
  author = {Gallicchio, Claudio and Micheli, Alessio and Pedrelli, Luca},
  editor = {Oneto, Luca and Navarin, Nicol{\`o} and Sperduti, Alessandro and Anguita, Davide},
  year = {2020},
  pages = {380--390},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-16841-4_39},
  abstract = {In this paper we address the problem of grounded weights initialization for Recurrent Neural Networks. Specifically, we propose a method, rooted in the field of Random Matrix theory, to perform a fast initialization of recurrent weight matrices that meet specific constraints on their spectral radius. Focusing on the Reservoir Computing (RC) framework, the proposed approach allows us to overcome the typical computational bottleneck related to the eigendecomposition of large matrices, enabling to efficiently design large reservoir networks and hence to address time-series tasks characterized by medium/big datasets. Experimental results show that the proposed method enables an accurate control of the spectral radius of randomly initialized recurrent matrices, providing an initialization approach that is extremely more efficient compared to common RC practice.},
  isbn = {978-3-030-16841-4},
  langid = {english},
  keywords = {Echo state networks,Recurrent Neural Networks,Spectral radius of random matrices,Time-series prediction},
  file = {/home/elessar/Zotero/storage/7YE7H6N7/Gallicchio et al. - 2020 - Fast Spectral Radius Initialization for Recurrent Neural Networks.pdf}
}

@article{gallicchioFrontiersReservoirComputing,
  title = {Frontiers in {{Reservoir Computing}}},
  author = {Gallicchio, Claudio and Luko{\v s}evi{\v c}ius, Mantas and Scardapane, Simone},
  abstract = {Reservoir computing (RC) studies the properties of large recurrent networks of artificial neurons, with either fixed or random connectivity. Over the last years, reservoirs have become a key tool for pattern recognition and neuroscience problems, being able to develop a rich representation of the temporal information even if left untrained. The common paradigm has been instantiated into several models, among which the Echo State Network and the Liquid State Machine represent the most widely known ones. Nowadays, RC represents the de facto state-of-the-art approach for efficient learning in the temporal domain. Besides, theoretical studies in RC area can contribute to the broader field of Recurrent Neural Networks research by enabling a deeper understanding of the fundamental capabilities of dynamical recurrent models, even in the absence of training of the recurrent connections. RC paradigm also allows using different dynamical systems, including hardware, for computation.},
  langid = {english},
  annotation = {GSCC: 0000003},
  file = {/home/elessar/Zotero/storage/WW94RKTC/Gallicchio et al. - Frontiers in Reservoir Computing.pdf}
}

@inproceedings{gallicchioResidualReservoirComputing2023,
  title = {Residual {{Reservoir Computing Neural Networks}} for {{Time-series Classification}}},
  booktitle = {{{ESANN}} 2023 Proceesdings},
  author = {Gallicchio, Claudio and Ceni, Andrea},
  year = {2023},
  pages = {507--512},
  publisher = {Ciaco - i6doc.com},
  address = {Bruges (Belgium) and online},
  doi = {10.14428/esann/2023.ES2023-112},
  urldate = {2025-02-16},
  abstract = {We introduce a novel class of Reservoir Computing (RC) models, a family of efficiently trainable Recurrent Neural Networks based on untrained connections. Aiming to improve the forward propagation of input information through time, we augment standard Echo State Networks (ESNs) with linear reservoir-skip connections modulated by an untrained orthogonal weight matrix. We analyze the mathematical properties of the resulting reservoir systems and show that the dynamical regime of the proposed class of models is controllably close to the edge of stability. Experiments on several time-series classification tasks highlight the striking performance advantage of the proposed approach over standard ESNs.},
  isbn = {978-2-87587-088-9},
  langid = {english},
  file = {/home/elessar/Zotero/storage/BLHSFM3D/Gallicchio and Ceni - 2023 - Residual Reservoir Computing Neural Networks for Time-series Classification.pdf}
}

@misc{galtierLocalEchoState2015,
  title = {A Local {{Echo State Property}} through the Largest {{Lyapunov}} Exponent},
  author = {Galtier, Mathieu and Wainrib, Gilles},
  year = {2015},
  month = may,
  number = {arXiv:1402.1619},
  eprint = {1402.1619},
  primaryclass = {nlin},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1402.1619},
  urldate = {2025-07-19},
  abstract = {Echo State Networks are efficient time-series predictors, which highly depend on the value of the spectral radius of the reservoir connectivity matrix. Based on recent results on the mean field theory of driven random recurrent neural networks, enabling the computation of the largest Lyapunov exponent of an ESN, we develop a cheap algorithm to establish a local and operational version of the Echo State Property.},
  archiveprefix = {arXiv},
  keywords = {Nonlinear Sciences - Chaotic Dynamics},
  file = {/home/elessar/Zotero/storage/8DVHPMAL/Galtier and Wainrib - 2015 - A local Echo State Property through the largest Lyapunov exponent.pdf;/home/elessar/Zotero/storage/4GN2EL4D/1402.html}
}

@article{ganeshIrreversibilityDissipationFinitestate2013,
  title = {Irreversibility and Dissipation in Finite-State Automata},
  author = {Ganesh, Natesh and Anderson, Neal G.},
  year = {2013},
  month = dec,
  journal = {Physics Letters A},
  volume = {377},
  number = {45-48},
  pages = {3266--3271},
  publisher = {Elsevier BV},
  issn = {0375-9601},
  doi = {10.1016/j.physleta.2013.10.010},
  urldate = {2025-07-25},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/QXEJ43YQ/Ganesh and Anderson - 2013 - Irreversibility and dissipation in finite-state automata.pdf}
}

@article{gauthierLearningUnseenCoexisting2022,
  title = {Learning Unseen Coexisting Attractors},
  author = {Gauthier, Daniel J. and Fischer, Ingo and R{\"o}hm, Andr{\'e}},
  year = {2022},
  month = nov,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {32},
  number = {11},
  pages = {113107},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0116784},
  urldate = {2025-08-12},
  abstract = {Reservoir computing is a machine learning approach that can generate a surrogate model of a dynamical system. It can learn the underlying dynamical system using fewer trainable parameters and, hence, smaller training data sets than competing approaches. Recently, a simpler formulation, known as next-generation reservoir computing, removed many algorithm metaparameters and identified a well-performing traditional reservoir computer, thus simplifying training even further. Here, we study a particularly challenging problem of learning a dynamical system that has both disparate time scales and multiple co-existing dynamical states (attractors). We compare the next-generation and traditional reservoir computer using metrics quantifying the geometry of the ground-truth and forecasted attractors. For the studied four-dimensional system, the next-generation reservoir computing approach uses {$\sim$}1.7{\texttimes} less training data, requires 103{\texttimes} shorter ``warmup'' time, has fewer metaparameters, and has an {$\sim$}100{\texttimes} higher accuracy in predicting the co-existing attractor characteristics in comparison to a traditional reservoir computer. Furthermore, we demonstrate that it predicts the basin of attraction with high accuracy. This work lends further support to the superior learning ability of this new machine learning algorithm for dynamical systems.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/2DC4B79F/Gauthier et al. - 2022 - Learning unseen coexisting attractors.pdf}
}

@article{gauthierNextGenerationReservoir2021,
  title = {Next Generation Reservoir Computing},
  author = {Gauthier, Daniel J. and Bollt, Erik and Griffith, Aaron and Barbosa, Wendson A. S.},
  year = {2021},
  month = sep,
  journal = {Nature Communications},
  volume = {12},
  number = {1},
  pages = {5564},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-25801-2},
  urldate = {2023-05-30},
  abstract = {Abstract             Reservoir computing is a best-in-class machine learning algorithm for processing information generated by dynamical systems using observed time-series data. Importantly, it requires very small training data sets, uses linear optimization, and thus requires minimal computing resources. However, the algorithm uses randomly sampled matrices to define the underlying recurrent neural network and has a multitude of metaparameters that must be optimized. Recent results demonstrate the equivalence of reservoir computing to nonlinear vector autoregression, which requires no random matrices, fewer metaparameters, and provides interpretable results. Here, we demonstrate that nonlinear vector autoregression excels at reservoir computing benchmark tasks and requires even shorter training data sets and training time, heralding the next generation of reservoir computing.},
  langid = {english},
  annotation = {GSCC: 0000366},
  file = {/home/elessar/Zotero/storage/ZI5DNKZ9/Gauthier et al. - 2021 - Next generation reservoir computing.pdf}
}

@article{gemelosEntropyRatePattern,
  title = {On the {{Entropy Rate}} of {{Pattern Processes}}},
  author = {Gemelos, George and Weissman, Tsachy},
  langid = {english},
  file = {/home/elessar/Zotero/storage/TWQLR4S3/Gemelos and Weissman - On the Entropy Rate of Pattern Processes.pdf}
}

@article{gilbertLempelZivAlgorithmMessage1992,
  title = {The {{Lempel-Ziv}} Algorithm and Message Complexity},
  author = {Gilbert, E.N. and Kadota, T.T.},
  year = {1992},
  journal = {IEEE Transactions on Information Theory},
  volume = {38},
  number = {6},
  pages = {1839--1842},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9448},
  doi = {10.1109/18.165463},
  urldate = {2025-07-15},
  abstract = {Data compression has been suggested as a non-parametric way of discriminating between message sources (e.g., a complex noise message should compress less than a more redundant signal message). Compressions obtained from a Lempel-Ziv algorithm for relatively short messages, such as those encountered in practice, are examined. The intuitive notion of message complexity has less connection with compression than one might expect from known asymptotic results about infinite messages. Nevertheless, discrimination by compression remains an interesting possibility.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/LPY9CMWZ/Gilbert and Kadota - 1992 - The Lempel-Ziv algorithm and message complexity.pdf}
}

@book{gilmoreTopologyChaosAlice2002,
  title = {The Topology of Chaos: {{Alice}} in Stretch and Squeezeland},
  shorttitle = {The Topology of Chaos},
  author = {Gilmore, Robert and Lefranc, Marc},
  year = {2002},
  publisher = {Wiley-Interscience},
  address = {New York},
  isbn = {978-0-471-40816-1},
  langid = {english},
  lccn = {QA614.813 .G55 2002},
  keywords = {Attractors (Mathematics),Chaotic behavior in systems},
  file = {/home/elessar/Zotero/storage/V65Y3K78/Gilmore and Lefranc - 2002 - The topology of chaos Alice in stretch and squeezeland.pdf}
}

@misc{gilpinChaosInterpretableBenchmark2023,
  title = {Chaos as an Interpretable Benchmark for Forecasting and Data-Driven Modelling},
  author = {Gilpin, William},
  year = {2023},
  month = jan,
  number = {arXiv:2110.05266},
  eprint = {2110.05266},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.05266},
  urldate = {2024-10-24},
  abstract = {The striking fractal geometry of strange attractors underscores the generative nature of chaos: like probability distributions, chaotic systems can be repeatedly measured to produce arbitrarily-detailed information about the underlying attractor. Chaotic systems thus pose a unique challenge to modern statistical learning techniques, while retaining quantifiable mathematical properties that make them controllable and interpretable as benchmarks. Here, we present a growing database currently comprising 131 known chaotic dynamical systems spanning fields such as astrophysics, climatology, and biochemistry. Each system is paired with precomputed multivariate and univariate time series. Our dataset has comparable scale to existing static time series databases; however, our systems can be re-integrated to produce additional datasets of arbitrary length and granularity. Our dataset is annotated with known mathematical properties of each system, and we perform feature analysis to broadly categorize the diverse dynamics present across the collection. Chaotic systems inherently challenge forecasting models, and across extensive benchmarks we correlate forecasting performance with the degree of chaos present. We also exploit the unique generative properties of our dataset in several proof-of-concept experiments: surrogate transfer learning to improve time series classification, importance sampling to accelerate model training, and benchmarking symbolic regression algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Nonlinear Sciences - Chaotic Dynamics},
  file = {/home/elessar/Zotero/storage/ADUX5B56/Gilpin - 2023 - Chaos as an interpretable benchmark for forecasting and data-driven modelling.pdf;/home/elessar/Zotero/storage/PJCLBKN2/2110.html}
}

@article{gilpinGenerativeLearningNonlinear2024,
  title = {Generative Learning for Nonlinear Dynamics},
  author = {Gilpin, William},
  year = {2024},
  month = feb,
  journal = {Nature Reviews Physics},
  volume = {6},
  number = {3},
  pages = {194--206},
  issn = {2522-5820},
  doi = {10.1038/s42254-024-00688-2},
  urldate = {2025-08-12},
  langid = {english}
}

@misc{gilpinModelScaleDomain2023,
  title = {Model Scale versus Domain Knowledge in Statistical Forecasting of Chaotic Systems},
  author = {Gilpin, William},
  year = {2023},
  month = nov,
  number = {arXiv:2303.08011},
  eprint = {2303.08011},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08011},
  urldate = {2024-10-24},
  abstract = {Chaos and unpredictability are traditionally synonymous, yet large-scale machine learning methods recently have demonstrated a surprising ability to forecast chaotic systems well beyond typical predictability horizons. However, recent works disagree on whether specialized methods grounded in dynamical systems theory, such as reservoir computers or neural ordinary differential equations, outperform general-purpose large-scale learning methods such as transformers or recurrent neural networks. These prior studies perform comparisons on few individually-chosen chaotic systems, thereby precluding robust quantification of how statistical modeling choices and dynamical invariants of different chaotic systems jointly determine empirical predictability. Here, we perform the largest to-date comparative study of forecasting methods on the classical problem of forecasting chaos: we benchmark 24 state-of-the-art forecasting methods on a crowdsourced database of 135 low-dimensional systems with 17 forecast metrics. We find that large-scale, domain-agnostic forecasting methods consistently produce predictions that remain accurate up to two dozen Lyapunov times, thereby accessing a new long-horizon forecasting regime well beyond classical methods. We find that, in this regime, accuracy decorrelates with classical invariant measures of predictability like the Lyapunov exponent. However, in data-limited settings outside the long-horizon regime, we find that physics-based hybrid methods retain a comparative advantage due to their strong inductive biases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Physics - Computational Physics},
  file = {/home/elessar/Zotero/storage/3M2UIA95/Gilpin - 2023 - Model scale versus domain knowledge in statistical forecasting of chaotic systems.pdf;/home/elessar/Zotero/storage/V9DG38E3/2303.html}
}

@book{goldsteinGoldsteinClassicalMechanics2016,
  title = {Goldstein {{Classical}} Mechanics},
  year = {2016},
  edition = {Third edition, fourteenth impression},
  publisher = {Pearson India Education Services},
  address = {Noida},
  collaborator = {Goldstein, Herbert and Poole, Charles P. and Safko, John L.},
  isbn = {978-81-317-5891-5},
  langid = {english},
  annotation = {OCLC: 1423811257},
  file = {/home/elessar/Zotero/storage/S4CJVBXQ/Goldstein et al. - 2016 - Classical mechanics.pdf}
}

@book{goldsteinGoldsteinMecanicaClasica2018,
  title = {Goldstein {{Mec{\'a}nica}} Cl{\'a}sica},
  author = {Goldstein, Herbert},
  year = {2018},
  edition = {2ª ed., reimp. digital},
  publisher = {Revert{\'e}},
  address = {Barcelona [etc]},
  isbn = {978-84-291-4306-5},
  langid = {english},
  annotation = {OCLC: 1083204274},
  file = {/home/elessar/Zotero/storage/J7PV2VS6/Goldstein - 2018 - Mecánica clásica.pdf}
}

@article{goltsevCriticalPhenomenaNetworks2003,
  title = {Critical Phenomena in Networks},
  author = {Goltsev, A. V. and Dorogovtsev, S. N. and Mendes, J. F. F.},
  year = {2003},
  month = feb,
  journal = {Physical Review E},
  volume = {67},
  number = {2},
  publisher = {American Physical Society (APS)},
  issn = {1063-651X, 1095-3787},
  doi = {10.1103/physreve.67.026123},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/UVU6KUJH/Goltsev et al. - 2003 - Critical phenomena in networks.pdf}
}

@inproceedings{gomezDecreasedLempelZivComplexity2005,
  title = {Decreased {{Lempel-Ziv}} Complexity in {{Alzheimer}}'s Disease Patients' Magnetoencephalograms},
  booktitle = {2005 {{IEEE Engineering}} in {{Medicine}} and {{Biology}} 27th {{Annual Conference}}},
  author = {Gomez, C. and Hornero, R. and Abasolo, D. and Lopez, M. and Fernandez, A.},
  year = {2005},
  pages = {4514--4517},
  publisher = {IEEE},
  address = {Shanghai, China},
  doi = {10.1109/iembs.2005.1615472},
  urldate = {2025-07-15},
  abstract = {The aim of the present research is to study the magnetoencephalogram (MEG) background activity in patients with Alzheimer's disease (AD) using the Lempel-Ziv (LZ) complexity. We recorded the MEG with a 148-channel wholehead magnetometer (MAGNES 2500 WH, 4D Neuroimaging) in 10 patients with probable AD and 10 age-matched control subjects, during five minutes. Artefact-free epochs were selected for the non-linear analysis. In all MEG channels, the AD patients had lower complexity than control subjects. In 77 of them the differences were statistically significant (p {$<$} 0.01). These preliminary results suggest that cognitive dysfunction in AD is associated with a decreased complexity in certain regions of the brain.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/DPMF57FB/Gomez et al. - 2005 - Decreased Lempel-Ziv complexity in Alzheimer's disease patients' magnetoencephalograms.pdf}
}

@article{gononInfinitedimensionalReservoirComputing2024,
  title = {Infinite-Dimensional Reservoir Computing},
  author = {Gonon, Lukas and Grigoryeva, Lyudmila and Ortega, Juan-Pablo},
  year = {2024},
  month = nov,
  journal = {Neural Networks},
  volume = {179},
  pages = {106486},
  issn = {08936080},
  doi = {10.1016/j.neunet.2024.106486},
  urldate = {2025-03-12},
  abstract = {Reservoir computing approximation and generalization bounds are proved for a new concept class of input/output systems that extends the so-called generalized Barron functionals to a dynamic context. This new class is characterized by the readouts with a certain integral representation built on infinite-dimensional state--space systems. It is shown that this class is very rich and possesses useful features and universal approximation properties. The reservoir architectures used for the approximation and estimation of elements in the new class are randomly generated echo state networks with either linear or ReLU activation functions. Their readouts are built using randomly generated neural networks in which only the output layer is trained (extreme learning machines or random feature neural networks). The results in the paper yield a recurrent neural network-based learning algorithm with provable convergence guarantees that do not suffer from the curse of dimensionality when learning input/output systems in the class of generalized Barron functionals and measuring the error in a mean-squared sense.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/ZZCCRZZB/Gonon et al. - 2024 - Infinite-dimensional reservoir computing.pdf}
}

@article{gonzalez-zapataOptimizingEchoState2022,
  title = {Optimizing {{Echo State Networks}} for {{Enhancing Large Prediction Horizons}} of {{Chaotic Time Series}}},
  author = {{Gonz{\'a}lez-Zapata}, Astrid Maritza and {Tlelo-Cuautle}, Esteban and {Ovilla-Martinez}, Brisbane and {Cruz-Vega}, Israel and De La Fraga, Luis Gerardo},
  year = {2022},
  month = oct,
  journal = {Mathematics},
  volume = {10},
  number = {20},
  pages = {3886},
  issn = {2227-7390},
  doi = {10.3390/math10203886},
  urldate = {2024-01-10},
  abstract = {Reservoir computing has shown promising results in predicting chaotic time series. However, the main challenges of time-series predictions are associated with reducing computational costs and increasing the prediction horizon. In this sense, we propose the optimization of Echo State Networks (ESN), where the main goal is to increase the prediction horizon using a lower count number of neurons compared with state-of-the-art models. In addition, we show that the application of the decimation technique allows us to emulate an increase in the prediction of up to 10,000 steps ahead. The optimization is performed by applying particle swarm optimization and considering two chaotic systems as case studies, namely the chaotic Hindmarsh--Rose neuron with slow dynamic behavior and the well-known Lorenz system. The results show that although similar works used from 200 to 5000 neurons in the reservoir of the ESN to predict from 120 to 700 steps ahead, our optimized ESN including decimation used 100 neurons in the reservoir, with a capability of predicting up to 10,000 steps ahead. The main conclusion is that we ensured larger prediction horizons compared to recent works, achieving an improvement of more than one order of magnitude, and the computational costs were greatly reduced.},
  langid = {english},
  annotation = {GSCC: 0000008},
  file = {/home/elessar/Zotero/storage/UW7YMCS3/González-Zapata et al. - 2022 - Optimizing Echo State Networks for Enhancing Large.pdf}
}

@misc{goodmanExtendedCommentLanguage2002,
  title = {Extended {{Comment}} on {{Language Trees}} and {{Zipping}}},
  author = {Goodman, Joshua},
  year = {2002},
  month = feb,
  number = {arXiv:cond-mat/0202383},
  eprint = {cond-mat/0202383},
  publisher = {arXiv},
  doi = {10.48550/arXiv.cond-mat/0202383},
  urldate = {2025-07-15},
  abstract = {This is the extended version of a Comment submitted to Physical Review Letters. I first point out the inappropriateness of publishing a Letter unrelated to physics. Next, I give experimental results showing that the technique used in the Letter is 3 times worse and 17 times slower than a simple baseline. And finally, I review the literature, showing that the ideas of the Letter are not novel. I conclude by suggesting that Physical Review Letters should not publish Letters unrelated to physics.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Condensed Matter - Statistical Mechanics},
  file = {/home/elessar/Zotero/storage/B99NM6HF/Goodman - 2002 - Extended Comment on Language Trees and Zipping.pdf}
}

@article{goosLectureNotesComputer,
  title = {Lecture {{Notes}} in {{Computer Science}}},
  author = {Goos, Gerhard and Hartmanis, Juris and {van Leeuwen}, Jan and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and Rangan, C Pandu and Steffen, Bernhard},
  langid = {english},
  annotation = {GSCC: 0000034},
  file = {/home/elessar/Zotero/storage/RLV82FT6/Goos et al. - Lecture Notes in Computer Science.pdf}
}

@article{goswamiDelayEmbeddedEchoState2023,
  title = {Delay {{Embedded Echo-State Network}}: {{A Predictor}} for {{Partially Observed Systems}}},
  shorttitle = {Delay {{Embedded Echo-State Network}}},
  author = {Goswami, Debdipta},
  year = {2023},
  month = jan,
  journal = {IFAC-PapersOnLine},
  series = {22nd {{IFAC World Congress}}},
  volume = {56},
  number = {2},
  pages = {6826--6832},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2023.10.470},
  urldate = {2025-03-23},
  abstract = {This paper considers the problem of data-driven prediction of partially observed systems using a recurrent neural network. While neural network based dynamic predictors perform well with full-state training data, prediction with partial observation during training phase poses a significant challenge. Here a predictor for partial observations is developed using an echo-state network (ESN) and time delay embedding of the partially observed state. The proposed method is theoretically justified with Taken's embedding theorem and strong observability of a nonlinear system. The efficacy of the proposed method is demonstrated on three systems: two synthetic datasets from chaotic dynamical systems and a set of real-time traffic data.},
  keywords = {Chaotic attractor,Machine learning,Neural networks,Nonlinear system identification,Observability,Reservoir computer},
  file = {/home/elessar/Zotero/storage/ULGCF9XT/Goswami - 2023 - Delay Embedded Echo-State Network A Predictor for Partially Observed Systems.pdf;/home/elessar/Zotero/storage/BAPD8746/S2405896323008376.html}
}

@inproceedings{gotoSimplerFasterLempel2013,
  title = {Simpler and {{Faster Lempel Ziv Factorization}}},
  booktitle = {2013 {{Data Compression Conference}}},
  author = {Goto, K. and Bannai, H.},
  year = {2013},
  month = mar,
  pages = {133--142},
  publisher = {IEEE},
  address = {Snowbird, UT},
  doi = {10.1109/dcc.2013.21},
  urldate = {2025-07-15},
  abstract = {We present a new, simple, and efficient approach for computing the Lempel-Ziv (LZ77) factorization of a string in linear time, based on suffix arrays. Computational experiments on various data sets show that our approach constantly outperforms the fastest previous algorithm LZ OG (Ohlebusch and Gog 2011), and can be up to 2 to 3 times faster in the processing after obtaining the suffix array, while requiring the same or a little more space.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/B4LHS9MU/Goto and Bannai - 2013 - Simpler and Faster Lempel Ziv Factorization.pdf}
}

@article{grassbergerCharacterizationStrangeAttractors1983,
  title = {Characterization of {{Strange Attractors}}},
  author = {Grassberger, Peter and Procaccia, Itamar},
  year = {1983},
  month = jan,
  journal = {Physical Review Letters},
  volume = {50},
  number = {5},
  pages = {346--349},
  publisher = {American Physical Society (APS)},
  issn = {0031-9007},
  doi = {10.1103/physrevlett.50.346},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/P9HY8H7S/Grassberger and Procaccia - 1983 - Characterization of Strange Attractors.pdf}
}

@misc{grassbergerEntropyEstimatesInsufficient2008,
  title = {Entropy {{Estimates}} from {{Insufficient Samplings}}},
  author = {Grassberger, P.},
  year = {2008},
  month = jan,
  number = {arXiv:physics/0307138},
  eprint = {physics/0307138},
  publisher = {arXiv},
  doi = {10.48550/arXiv.physics/0307138},
  urldate = {2025-07-25},
  abstract = {We present a detailed derivation of some estimators of Shannon entropy for discrete distributions. They hold for finite samples of N points distributed into M "boxes", with N and M -{$>$} oo, but N/M {$<$} oo. In the high sampling regime ({$<<$} 1 points in each box) they have exponentially small biases. In the low sampling regime the errors increase but are still much smaller than for most other estimators. One advantage is that our main estimators are given analytically, with explicitly known analytical formulas for the biases.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Physics - Computational Physics,Physics - Data Analysis Statistics and Probability},
  file = {/home/elessar/Zotero/storage/P6XFIZ5L/Grassberger - 2008 - Entropy Estimates from Insufficient Samplings.pdf}
}

@article{grassbergerEstimationKolmogorovEntropy1983,
  title = {Estimation of the {{Kolmogorov}} Entropy from a Chaotic Signal},
  author = {Grassberger, Peter and Procaccia, Itamar},
  year = {1983},
  month = oct,
  journal = {Physical Review A},
  volume = {28},
  number = {4},
  pages = {2591--2593},
  publisher = {American Physical Society (APS)},
  issn = {0556-2791},
  doi = {10.1103/physreva.28.2591},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/XTAB8BLR/Grassberger and Procaccia - 1983 - Estimation of the Kolmogorov entropy from a chaotic signal.pdf}
}

@article{grassbergerQuantitativeTheorySelfgenerated1986,
  title = {Toward a Quantitative Theory of Self-Generated Complexity},
  author = {Grassberger, Peter},
  year = {1986},
  month = sep,
  journal = {International Journal of Theoretical Physics},
  volume = {25},
  number = {9},
  pages = {907--938},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {0020-7748, 1572-9575},
  doi = {10.1007/bf00668821},
  urldate = {2025-07-25},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/home/elessar/Zotero/storage/W9RW8Y5F/Grassberger - 1986 - Toward a quantitative theory of self-generated complexity.pdf}
}

@misc{grassbergerRandomnessInformationComplexity2012,
  title = {Randomness, {{Information}}, and {{Complexity}}},
  author = {Grassberger, Peter},
  year = {2012},
  month = aug,
  number = {arXiv:1208.3459},
  eprint = {1208.3459},
  primaryclass = {physics},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1208.3459},
  urldate = {2025-07-15},
  abstract = {We review possible measures of complexity which might in particular be applicable to situations where the complexity seems to arise spontaneously. We point out that not all of them correspond to the intuitive (or ``naive'') notion, and that one should not expect a unique observable of complexity. One of the main problems is to distinguish complex from disordered systems. This and the fact that complexity is closely related to information requires that we also give a review of information measures. We finally concentrate on quantities which measure in some way or other the difficulty of classifying and forecasting sequences of discrete symbols, and study them in simple examples.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Physics - Data Analysis Statistics and Probability},
  file = {/home/elessar/Zotero/storage/L7AD4J44/Grassberger - 2012 - Randomness, Information, and Complexity.pdf}
}

@article{greenNPCompleteProblemsCellular,
  title = {{{NP-Complete Problems}} in {{Cellular Automata}}},
  author = {Green, Frederic},
  abstract = {An example of a cellular autom at on (CA) is given in whi ch the following problems are NP -complete: (i) determining if a given subconfi gurati cn s can b e gener at ed af t er [s] t ime steps , (ii) determining if a given eubccnfigurat ion 8 will rec ur afte r lsi time steps, (iii) deter mining if a given te mporal seque nce of state s 8 can be generated in lsi ti me steps. It is also found that the CA constructed bas an NP -hard limit language.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/7Y43V9VX/Green - NP-Complete Problems in Cellular Automata.pdf}
}

@article{griffithForecastingChaoticSystems2019,
  title = {Forecasting Chaotic Systems with Very Low Connectivity Reservoir Computers},
  author = {Griffith, Aaron and Pomerance, Andrew and Gauthier, Daniel J.},
  year = {2019},
  month = dec,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {29},
  number = {12},
  pages = {123108},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.5120710},
  urldate = {2025-08-12},
  abstract = {We explore the hyperparameter space of reservoir computers used for forecasting of the chaotic Lorenz '63 attractor with Bayesian optimization. We use a new measure of reservoir performance, designed to emphasize learning the global climate of the forecasted system rather than short-term prediction. We find that optimizing over this measure more quickly excludes reservoirs that fail to reproduce the climate. The results of optimization are surprising: the optimized parameters often specify a reservoir network with very low connectivity. Inspired by this observation, we explore reservoir designs with even simpler structure and find well-performing reservoirs that have zero spectral radius and no recurrence. These simple reservoirs provide counterexamples to widely used heuristics in the field and may be useful for hardware implementations of reservoir computers.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/W2TVZHD4/Griffith et al. - 2019 - Forecasting chaotic systems with very low connectivity reservoir computers.pdf}
}

@article{griffithsGriffithQuantumMechanics,
  title = {Griffith {{Quantum Mechanics}} - {{Solutions}}},
  author = {Griffiths, David and Schroeter, Darrell and College, Reed},
  langid = {english},
  file = {/home/elessar/Zotero/storage/WIXPLBGN/Griﬃths et al. - Instructors’ Solution Manual Introduction to Quantum Mechanics, 3rd ed..pdf}
}

@article{grigorieffKolmogorovComplexityNondeterminism2002,
  title = {Kolmogorov Complexity and Non-Determinism},
  author = {Grigorieff, Serge and Marion, Jean-Yves},
  year = {2002},
  month = jan,
  journal = {Theoretical Computer Science},
  volume = {271},
  number = {1-2},
  pages = {151--180},
  publisher = {Elsevier BV},
  issn = {0304-3975},
  doi = {10.1016/s0304-3975(01)00038-x},
  urldate = {2025-07-25},
  abstract = {We are concerned with Kolmogorov complexity of strings produced by non-deterministic algorithms. For this, we consider /ve classes of non-deterministic description modes: (i) non-bounded description modes in which the number of outputs depends on programs, (ii) distributed description modes in which the number of outputs depends on the size of the outputs, (iii) spread description modes in which the number of outputs depends on both programs and the size of the outputs, (iv) description modes for which each stringhas a unique minimal description, and lastly (v) description modes for which the set of minimal length descriptions is a pre/x set. c{\copyright} 2002 Elsevier Science B.V. All rights reserved.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/75GN8BTE/Grigorieff and Marion - 2002 - Kolmogorov complexity and non-determinism.pdf}
}

@article{grigoryevaChaosCompactManifolds2021,
  title = {Chaos on Compact Manifolds: {{Differentiable}} Synchronizations beyond the {{Takens}} Theorem},
  shorttitle = {Chaos on Compact Manifolds},
  author = {Grigoryeva, Lyudmila and Hart, Allen and Ortega, Juan-Pablo},
  year = {2021},
  month = jun,
  journal = {Physical Review E},
  volume = {103},
  number = {6},
  pages = {062204},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.103.062204},
  urldate = {2025-08-12},
  langid = {english},
  file = {/home/elessar/Zotero/storage/ED8YJI8J/Grigoryeva et al. - 2021 - Chaos on compact manifolds Differentiable synchronizations beyond the Takens theorem.pdf}
}

@article{grigoryevaDatadrivenColdStarting2024,
  title = {Data-Driven Cold Starting of Good Reservoirs},
  author = {Grigoryeva, Lyudmila and Hamzi, Boumediene and Kemeth, Felix P. and Kevrekidis, Yannis and Manjunath, G. and Ortega, Juan-Pablo and Steynberg, Matthys J.},
  year = {2024},
  month = dec,
  journal = {Physica D: Nonlinear Phenomena},
  volume = {469},
  pages = {134325},
  issn = {01672789},
  doi = {10.1016/j.physd.2024.134325},
  urldate = {2025-08-12},
  langid = {english}
}

@article{grigoryevaEchoStateNetworks2018,
  title = {Echo State Networks Are Universal},
  author = {Grigoryeva, Lyudmila and Ortega, Juan-Pablo},
  year = {2018},
  month = dec,
  journal = {Neural Networks},
  volume = {108},
  pages = {495--508},
  issn = {08936080},
  doi = {10.1016/j.neunet.2018.08.025},
  urldate = {2023-05-30},
  abstract = {This paper shows that echo state networks are universal uniform approximants in the context of discrete-time fading memory filters with uniformly bounded inputs defined on negative infinite times. This result guarantees that any fading memory input/output system in discrete time can be realized as a simple finite-dimensional neural network-type state-space model with a static linear readout map. This approximation is valid for infinite time intervals. The proof of this statement is based on fundamental results, also presented in this work, about the topological nature of the fading memory property and about reservoir computing systems generated by continuous reservoir maps.},
  langid = {english},
  annotation = {GSCC: 0000245},
  file = {/home/elessar/Zotero/storage/V6F2HCHE/Grigoryeva and Ortega - 2018 - Echo state networks are universal.pdf}
}

@article{grigoryevaLearningStrangeAttractors2023,
  title = {Learning Strange Attractors with Reservoir Systems},
  author = {Grigoryeva, Lyudmila and Hart, Allen and Ortega, Juan-Pablo},
  year = {2023},
  month = sep,
  journal = {Nonlinearity},
  volume = {36},
  number = {9},
  eprint = {2108.05024},
  primaryclass = {math},
  pages = {4674--4708},
  issn = {0951-7715, 1361-6544},
  doi = {10.1088/1361-6544/ace492},
  urldate = {2025-07-28},
  abstract = {This paper shows that the celebrated Embedding Theorem of Takens is a particular case of a much more general statement according to which, randomly generated linear state-space representations of generic observations of an invertible dynamical system carry in their wake an embedding of the phase space dynamics into the chosen Euclidean state space. This embedding coincides with a natural generalized synchronization that arises in this setup and that yields a topological conjugacy between the state-space dynamics driven by the generic observations of the dynamical system and the dynamical system itself. This result provides additional tools for the representation, learning, and analysis of chaotic attractors and sheds additional light on the reservoir computing phenomenon that appears in the context of recurrent neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Dynamical Systems},
  file = {/home/elessar/Zotero/storage/MD5NLTAJ/Grigoryeva et al. - 2023 - Learning strange attractors with reservoir systems.pdf;/home/elessar/Zotero/storage/HVR4G6JM/2108.html}
}

@article{grimmettRandomProcessesGraphs,
  title = {Random {{Processes}} on {{Graphs}} and {{Lattices}}},
  author = {Grimmett, Geoffrey},
  langid = {english},
  file = {/home/elessar/Zotero/storage/IG78HNK9/Grimmett - Random Processes on Graphs and Lattices.pdf}
}

@article{gubernatisMulticanonicalMonteCarlo2000,
  title = {The Multicanonical {{Monte Carlo}} Method},
  author = {Gubernatis, Jim and Hatano, Naomichi},
  year = {2000},
  month = mar,
  journal = {Computing in Science \& Engineering},
  volume = {2},
  number = {2},
  pages = {95--102},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {1521-9615},
  doi = {10.1109/mcise.2000.5427643},
  urldate = {2025-07-25},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/WUW9KBHD/Gubernatis and Hatano - 2000 - The multicanonical Monte Carlo method.PDF}
}

@article{gurzadyanKolmogorovComplexityDescriptor1999,
  title = {Kolmogorov Complexity as a Descriptor of Cosmic Microwave Background Maps},
  author = {Gurzadyan, V. G},
  year = {1999},
  month = apr,
  journal = {Europhysics Letters (EPL)},
  volume = {46},
  number = {1},
  pages = {114--117},
  publisher = {IOP Publishing},
  issn = {0295-5075, 1286-4854},
  doi = {10.1209/epl/i1999-00544-3},
  urldate = {2025-07-25},
  abstract = {The information theory approach is suggested to the Cosmic Microwave Background (CMB) problem for negatively curved homogeneous and isotropic Universe. Namely, the Kolmogorov complexity of anisotropy spots of Cosmic Microwave Background (CMB) radiation sky maps is proposed as a new descriptor for revealing crucial cosmological information, particularly on the curvature of the Universe. Such profound descriptor can be especially valuable while analyzing the data of forthcoming space- and ground-based experiments MAP, Planck surveyor, CAT, etc.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/3I5KSKSL/Gurzadyan - 1999 - Kolmogorov complexity as a descriptor of cosmic microwave background maps.pdf}
}

@article{guttmanEnumerationsStatisticalMechanics2001,
  title = {Enumerations in Statistical Mechanics and Combinatorics},
  author = {Guttman, A.},
  year = {2001},
  journal = {Computing in Science \& Engineering},
  volume = {3},
  number = {3},
  pages = {42--47},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {1521-9615},
  doi = {10.1109/5992.919265},
  urldate = {2025-07-25},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/JS669SPL/Guttman - 2001 - Enumerations in statistical mechanics and combinatorics.PDF}
}

@article{halevyUnreasonableEffectivenessData2009,
  title = {The {{Unreasonable Effectiveness}} of {{Data}}},
  author = {Halevy, Alon and Norvig, Peter and Pereira, Fernando},
  year = {2009},
  month = mar,
  journal = {IEEE Intelligent Systems},
  volume = {24},
  number = {2},
  pages = {8--12},
  issn = {1541-1672},
  doi = {10.1109/MIS.2009.36},
  urldate = {2023-05-30},
  langid = {english},
  annotation = {GSCC: 0002348},
  file = {/home/elessar/Zotero/storage/ZUIGFYGS/Halevy et al. - 2009 - The Unreasonable Effectiveness of Data.pdf}
}

@article{hallDenjoyCounterexample1981,
  title = {A {{C}}{\textsuperscript{{$\infty$}}} {{Denjoy}} Counterexample},
  author = {Hall, Glen Richard},
  year = {1981},
  month = sep,
  journal = {Ergodic Theory and Dynamical Systems},
  volume = {1},
  number = {3},
  pages = {261--272},
  issn = {0143-3857, 1469-4417},
  doi = {10.1017/S0143385700001243},
  urldate = {2025-07-29},
  abstract = {In this paper we construct an example of a homeomorphism of the circle onto itself which is C{$^\circ^\circ$}, has no periodic points and no dense orbits. Moreover, the homeomorphism will have no more than two points of zero derivative. We alter this example to form a C{$^\circ^\circ$} map of an interval to itself which has homtervals.},
  copyright = {https://www.cambridge.org/core/terms},
  langid = {english},
  file = {/home/elessar/Zotero/storage/ZMXXNTXX/Hall - 1981 - A C∞ Denjoy counterexample.pdf}
}

@article{hallTopologicalEntropyHenon,
  title = {Topological {{Entropy}} in the {{H{\'e}non}} Map},
  author = {Hall, Natalie},
  abstract = {Compared to other areas of physics and math, chaos theory is a relatively new field that has many new areas to be explored. Although chaotic systems are everywhere in nature, many real systems are too complex to be modeled on regular computers. The H{\'e}non map is a simple iterated map that displays chaotic behavior in two dimensions that is easily computed. Topological entropy is a measure of the complexity of a system that can be used to compare different configurations of the H{\'e}non map. Changes in topological entropy over varying k values in the H{\'e}non map may lead to clues at how changes in k affect the dynamics of the system. Generalizing the effects of changing k may be possible, giving us new theories about chaotic systems that can be tested on more realistic systems.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/N85FXJVD/Hall - Topological Entropy in the Hénon map.pdf}
}

@article{haluszczynskiGoodBadPredictions2019,
  title = {Good and Bad Predictions: {{Assessing}} and Improving the Replication of Chaotic Attractors by Means of Reservoir Computing},
  shorttitle = {Good and Bad Predictions},
  author = {Haluszczynski, Alexander and R{\"a}th, Christoph},
  year = {2019},
  month = oct,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {29},
  number = {10},
  pages = {103143},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.5118725},
  urldate = {2025-08-12},
  abstract = {The prediction of complex nonlinear dynamical systems with the help of machine learning techniques has become increasingly popular. In particular, reservoir computing turned out to be a very promising approach especially for the reproduction of the long-term properties of a nonlinear system. Yet, a thorough statistical analysis of the forecast results is missing. Using the Lorenz and R{\"o}ssler system, we statistically analyze the quality of prediction for different parametrizations---both the exact short-term prediction as well as the reproduction of the long-term properties (the ``climate'') of the system as estimated by the correlation dimension and largest Lyapunov exponent. We find that both short- and long-term predictions vary significantly among the realizations. Thus, special care must be taken in selecting the good predictions as realizations, which deliver better short-term prediction also tend to better resemble the long-term climate of the system. Instead of only using purely random Erd{\"o}s-Renyi networks, we also investigate the benefit of alternative network topologies such as small world or scale-free networks and show which effect they have on the prediction quality. Our results suggest that the overall performance with respect to the reproduction of the climate of both the Lorenz and R{\"o}ssler system is worst for scale-free networks. For the Lorenz system, there seems to be a slight benefit of using small world networks, while for the R{\"o}ssler system, small world and Erd{\"o}s-Renyi networks performed equivalently well. In general, the observation is that reservoir computing works for all network topologies investigated here.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/B7E5BP8X/Haluszczynski and Räth - 2019 - Good and bad predictions Assessing and improving the replication of chaotic attractors by means of.pdf}
}

@article{haluszczynskiReducingNetworkSize2020,
  title = {Reducing Network Size and Improving Prediction Stability of Reservoir Computing},
  author = {Haluszczynski, Alexander and Aumeier, Jonas and Herteux, Joschka and R{\"a}th, Christoph},
  year = {2020},
  month = jun,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {30},
  number = {6},
  eprint = {2003.03178},
  primaryclass = {physics},
  pages = {063136},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0006869},
  urldate = {2025-08-12},
  abstract = {Reservoir computing is a very promising approach for the prediction of complex nonlinear dynamical systems. Besides capturing the exact short-term trajectories of nonlinear systems, it has also proved to reproduce its characteristic long-term properties very accurately. However, predictions do not always work equivalently well. It has been shown that both short- and long-term predictions vary significantly among different random realizations of the reservoir. In order to gain an understanding on when reservoir computing works best, we investigate some differential properties of the respective realization of the reservoir in a systematic way. We find that removing nodes that correspond to the largest weights in the output regression matrix reduces outliers and improves overall prediction quality. Moreover, this allows to effectively reduce the network size and, therefore, increase computational efficiency. In addition, we use a nonlinear scaling factor in the hyperbolic tangent of the activation function. This adjusts the response of the activation function to the range of values of the input variables of the nodes. As a consequence, this reduces the number of outliers significantly and increases both the short- and long-term prediction quality for the nonlinear systems investigated in this study. Our results demonstrate that a large optimization potential lies in the systematical refinement of the differential reservoir properties for a given dataset.},
  archiveprefix = {arXiv},
  keywords = {Physics - Data Analysis Statistics and Probability},
  file = {/home/elessar/Zotero/storage/KXZ4UQR9/Haluszczynski et al. - 2020 - Reducing network size and improving prediction stability of reservoir computing.pdf;/home/elessar/Zotero/storage/48SM6FRU/2003.html}
}

@inproceedings{hammerInequalitiesShannonEntropies,
  title = {Inequalities for {{Shannon}} Entropies and {{Kolmogorov}} Complexities},
  booktitle = {Proceedings of {{Computational Complexity}}. {{Twelfth Annual IEEE Conference}}},
  author = {Hammer, D. and Romashchenko, A.E. and Shen, A. and Vereshchagin, N.K.},
  pages = {13--23},
  publisher = {IEEE Comput. Soc},
  address = {Ulm, Germany},
  doi = {10.1109/ccc.1997.612296},
  urldate = {2025-07-25},
  langid = {english},
  file = {/home/elessar/Zotero/storage/3YYGH99D/Hammer et al. - Inequalities for Shannon entropies and Kolmogorov complexities.pdf}
}

@article{hammerInequalitiesShannonEntropy2000,
  title = {Inequalities for {{Shannon Entropy}} and {{Kolmogorov Complexity}}},
  author = {Hammer, Daniel and Romashchenko, Andrei and Shen, Alexander and Vereshchagin, Nikolai},
  year = {2000},
  month = apr,
  journal = {Journal of Computer and System Sciences},
  volume = {60},
  number = {2},
  pages = {442--464},
  publisher = {Elsevier BV},
  issn = {0022-0000},
  doi = {10.1006/jcss.1999.1677},
  urldate = {2025-07-25},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/3JUUFDWJ/Hammer et al. - 2000 - Inequalities for Shannon Entropy and Kolmogorov Complexity.pdf}
}

@article{hanMultistepNetworkTraffic2022,
  title = {Multi-Step Network Traffic Prediction Using Echo State Network with a Selective Error Compensation Strategy},
  author = {Han, Ying and Jing, Yuanwei and Dimirovski, Georgi M and Zhang, Li},
  year = {2022},
  month = may,
  journal = {Transactions of the Institute of Measurement and Control},
  volume = {44},
  number = {8},
  pages = {1656--1668},
  issn = {0142-3312, 1477-0369},
  doi = {10.1177/01423312211050296},
  urldate = {2025-03-11},
  abstract = {Communication networks grow exponentially in this globalization era; thus, the network traffic modelling and prediction plays a crucial role in network management and security warning. Solely, the multi-step network traffic prediction may involve greater errors hence worsening prediction performance. To overcome this problem, an optimized echo state network model with selective error compensation is proposed. In the optimized echo state network-based multi-step prediction model, an improved fruit--fly optimization algorithm based on cloud model (named LVCMFOA) is used to select optimum values of four key parameters of the model. The proposed LVCMFOA algorithm uses the levy-flight function to redefine the generation of the fruit--fly population, which can randomly change the search radius and help getting out of a possible local optimal solution and prevent local optimum. To reduce the calculation time but improve the prediction accuracy simultaneously, a sophisticated selective error compensation strategy employing the variable sliding window technology is proposed so as to avoid the error accumulation problem in the multi-step prediction. The effectiveness of the proposed method is verified by applying it to Henon mapping chaotic series, Mackey--Glass chaotic series and two public network traffic data sets all known in the literature.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/U2QXPD3K/Han et al. - 2022 - Multi-step network traffic prediction using echo state network with a selective error compensation s.pdf}
}

@article{haraLearningDynamicsReservoir2024,
  title = {Learning {{Dynamics}} by {{Reservoir Computing}} ({{In Memory}} of {{Prof}}. {{Pavol Brunovsk{\'y}}})},
  author = {Hara, Masato and Kokubu, Hiroshi},
  year = {2024},
  month = feb,
  journal = {Journal of Dynamics and Differential Equations},
  volume = {36},
  number = {1},
  pages = {515--540},
  issn = {1572-9222},
  doi = {10.1007/s10884-022-10159-w},
  urldate = {2025-08-12},
  abstract = {We study reservoir computing, a machine learning method, from the viewpoint of learning dynamics. We present numerical results of learning the dynamics of the logistic map, one of the typical examples of chaotic dynamical systems, using a 30-node reservoir and a three-node reservoir. When the learning is successful, an attractor that is smoothly conjugate to the logistic map to be learned is observed in the phase space of the reservoir. Inspired by this numerical result, we introduce a degenerate reservoir system and use it to mathematically confirm this observation. We also show that reservoir computing can learn information about dynamics not included in the training data, which we believe is a remarkable feature of reservoir computing compared to other machine learning methods. We discuss this feature in connection with the above observation that there is a smooth conjugacy between the attractor in the reservoir and the dynamics to be learned.},
  langid = {english},
  keywords = {37D10,37E05,37M05,37N99,Attractor,Bifurcation,Conjugacy,Degenerate reservoir,Logistic map,Reservoir computing},
  file = {/home/elessar/Zotero/storage/HRF5K5Q4/Hara and Kokubu - 2024 - Learning Dynamics by Reservoir Computing (In Memory of Prof. Pavol Brunovský).pdf}
}

@book{harderSolvingPartialDifferential2024,
  title = {Solving {{Partial Differential Equations}} with {{Equivariant Extreme Learning Machines}}},
  author = {Harder, Hans and Rabault, J. and Vinuesa, Ricardo and Mortensen, Mikael and Peitz, Sebastian},
  year = {2024},
  month = may,
  doi = {10.48550/arXiv.2404.18530},
  abstract = {We utilize extreme-learning machines for the prediction of partial differential equations (PDEs). Our method splits the state space into multiple windows that are predicted individually using a single model. Despite requiring only few data points (in some cases, our method can learn from a single full-state snapshot), it still achieves high accuracy and can predict the flow of PDEs over long time horizons. Moreover, we show how additional symmetries can be exploited to increase sample efficiency and to enforce equivariance.},
  file = {/home/elessar/Zotero/storage/ZJ29MZRG/Harder et al. - 2024 - Solving Partial Differential Equations with Equivariant Extreme Learning Machines.pdf}
}

@article{hartAttractorReconstructionReservoir2024,
  title = {Attractor Reconstruction with Reservoir Computers: {{The}} Effect of the Reservoir's Conditional {{Lyapunov}} Exponents on Faithful Attractor Reconstruction},
  shorttitle = {Attractor Reconstruction with Reservoir Computers},
  author = {Hart, Joseph D.},
  year = {2024},
  month = apr,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {34},
  number = {4},
  pages = {043123},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0196257},
  urldate = {2025-08-12},
  abstract = {Reservoir computing is a machine learning framework that has been shown to be able to replicate the chaotic attractor, including the fractal dimension and the entire Lyapunov spectrum, of the dynamical system on which it is trained. We quantitatively relate the generalized synchronization dynamics of a driven reservoir during the training stage to the performance of the trained reservoir computer at the attractor reconstruction task. We show that, in order to obtain successful attractor reconstruction and Lyapunov spectrum estimation, the maximal conditional Lyapunov exponent of the driven reservoir must be significantly more negative than the most negative Lyapunov exponent of the target system. We also find that the maximal conditional Lyapunov exponent of the reservoir depends strongly on the spectral radius of the reservoir adjacency matrix; therefore, for attractor reconstruction and Lyapunov spectrum estimation, small spectral radius reservoir computers perform better in general. Our arguments are supported by numerical examples on well-known chaotic systems.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/C7VUVEN6/Hart - 2024 - Attractor reconstruction with reservoir computers The effect of the reservoir’s conditional Lyapuno.pdf}
}

@article{hartEchoStateNetworks2021,
  title = {Echo {{State Networks}} Trained by {{Tikhonov}} Least Squares Are {{L}} 2 ( {$\mu$} ) Approximators of Ergodic Dynamical Systems},
  author = {Hart, Allen G. and Hook, James L. and Dawes, Jonathan H.P.},
  year = {2021},
  month = jul,
  journal = {Physica D: Nonlinear Phenomena},
  volume = {421},
  pages = {132882},
  issn = {01672789},
  doi = {10.1016/j.physd.2021.132882},
  urldate = {2025-08-12},
  langid = {english}
}

@article{hartEmbeddingApproximationTheorems2020,
  title = {Embedding and {{Approximation Theorems}} for {{Echo State Networks}}},
  author = {Hart, Allen G. and Hook, James L. and Dawes, Jonathan H. P.},
  year = {2020},
  month = aug,
  journal = {Neural Networks},
  volume = {128},
  eprint = {1908.05202},
  primaryclass = {nlin},
  pages = {234--247},
  issn = {08936080},
  doi = {10.1016/j.neunet.2020.05.013},
  urldate = {2024-02-13},
  abstract = {Echo State Networks (ESNs) are a class of single-layer recurrent neural networks that have enjoyed recent attention. In this paper we prove that a suitable ESN, trained on a series of measurements of an invertible dynamical system, induces a C1 map from the dynamical system's phase space to the ESN's reservoir space. We call this the Echo State Map. We then prove that the Echo State Map is generically an embedding with positive probability.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematics - Dynamical Systems,Nonlinear Sciences - Chaotic Dynamics},
  annotation = {GSCC: 0000118},
  file = {/home/elessar/Zotero/storage/IK3NYMWG/Hart et al. - 2020 - Embedding and approximation theorems for echo state networks.pdf;/home/elessar/Zotero/storage/NQTGTJM6/Hart et al_2020_Embedding and Approximation Theorems for Echo State Networks.pdf;/home/elessar/Zotero/storage/QGBBN4RT/Hart et al. - 2020 - Embedding and Approximation Theorems for Echo Stat.pdf}
}

@article{hassanibesheliLongtermENSOPrediction2022,
  title = {Long-Term {{ENSO}} Prediction with Echo-State Networks},
  author = {Hassanibesheli, Forough and Kurths, J{\"u}rgen and Boers, Niklas},
  year = {2022},
  month = sep,
  journal = {Environmental Research: Climate},
  volume = {1},
  number = {1},
  pages = {011002},
  issn = {2752-5295},
  doi = {10.1088/2752-5295/ac7f4c},
  urldate = {2025-08-12},
  abstract = {Abstract             The El Ni{\~n}o-Southern Oscillation (ENSO) is a climate phenomenon that profoundly impacts weather patterns and extreme events worldwide. Here we develop a method based on a recurrent neural network, called echo state network (ESN), which can be trained efficiently to predict different ENSO indices despite their relatively high noise levels. To achieve this, we train the ESN model on the low-frequency variability of ENSO indices and estimate the potential future high-frequency variability from specific samples of its past history. Our method reveals the importance of cross-scale interactions in the mechanisms underlying ENSO and skilfully predicts its variability and especially El Ni{\~n}o events at lead times up to 21\,months. This study considers forecasts skillful if the correlation coefficients are above 0.5. Our results show that the low-frequency component of ENSO carries substantial predictive power, which can be exploited by training our model on single scalar time series. The proposed machine learning method for data-driven modeling can be readily applied to other time series, e.g. finance and physiology. However, it should be noted that our approach cannot straightforwardly be turned into a real-time operational forecast because of the decomposition of the original time series into the slow and fast components using low-pass filter techniques.}
}

@article{henaffRecurrentOrthogonalNetworks,
  title = {Recurrent {{Orthogonal Networks}} and {{Long-Memory Tasks}}},
  author = {Henaff, Mikael and Szlam, Arthur and LeCun, Yann},
  abstract = {Although RNNs have been shown to be powerful tools for processing sequential data, finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets originally outlined in (Hochreiter \& Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illuminate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions furthermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/8L76ITU8/Henaff et al. - Recurrent Orthogonal Networks and Long-Memory Tasks.pdf}
}

@article{henriknevermannMappingDynamicalSystems2023,
  title = {Mapping Dynamical Systems with Distributed Time Delays to Sets of Ordinary Differential Equations},
  author = {Henrik Nevermann, Daniel and Gros, Claudius},
  year = {2023},
  month = aug,
  journal = {Journal of Physics A: Mathematical and Theoretical},
  volume = {56},
  number = {34},
  pages = {345702},
  issn = {1751-8113, 1751-8121},
  doi = {10.1088/1751-8121/acea06},
  urldate = {2025-03-20},
  abstract = {Abstract                            Real-world dynamical systems with retardation effects are described in general not by a single, precisely defined time delay, but by a range of delay times. An exact mapping onto a set of               N               \,+\,1 ordinary differential equations exists when the respective delay distribution is given in terms of a gamma distribution with discrete exponents. The number of auxiliary variables one needs to introduce,               N               , is inversely proportional to the variance of the delay distribution. The case of a single delay is therefore recovered when                                                                                                        N                   {$\rightarrow$}                   {$\infty$}                                                                . Using this approach, denoted here the `kernel series framework', we examine systematically how the bifurcation phase diagram of the Mackey--Glass system changes under the influence of distributed delays. We find that local properties, f.i. the locus of a Hopf bifurcation, are robust against the introduction of broadened memory kernels. Period-doubling transitions and the onset of chaos, which involve non-local properties of the flow, are found in contrast to be more sensitive to distributed delays. In general, the observed effects are found to scale as                                                                                                        1                                        /                                      N                                                                . Furthermore, we consider time-delayed systems exhibiting chaotic diffusion, which is present in particular for sinusoidal flows. We find that chaotic diffusion is substantially more pronounced for distributed delays. Our results indicate in consequence that modeling approaches of real-world processes should take the effects of distributed delay times into account.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/Q6CAL55D/Henrik Nevermann and Gros - 2023 - Mapping dynamical systems with distributed time delays to sets of ordinary differential equations.pdf}
}

@inproceedings{hermansTrainingAnalysingDeep2013,
  title = {Training and {{Analysing Deep Recurrent Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hermans, Michiel and Schrauwen, Benjamin},
  year = {2013},
  volume = {26},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-03-07},
  abstract = {Time series often have a temporal hierarchy, with information that is spread out over multiple time scales. Common recurrent neural networks, however, do not explicitly accommodate such a hierarchy, and most research on them has been focusing on training algorithms rather than on their basic architecture. In this pa- per we study the effect of a hierarchy of recurrent neural networks on processing time series. Here, each layer is a recurrent network which receives the hidden state of the previous layer as input. This architecture allows us to perform hi- erarchical processing on difficult temporal tasks, and more naturally capture the structure of time series. We show that they reach state-of-the-art performance for recurrent networks in character-level language modelling when trained with sim- ple stochastic gradient descent. We also offer an analysis of the different emergent time scales.},
  file = {/home/elessar/Zotero/storage/FW98FUNK/Hermans and Schrauwen - 2013 - Training and Analysing Deep Recurrent Neural Networks.pdf}
}

@article{hermoCompressibilityUniformComplexity1997,
  title = {Compressibility and Uniform Complexity},
  author = {Hermo, Montserrat},
  year = {1997},
  month = jun,
  journal = {Information Processing Letters},
  volume = {62},
  number = {5},
  pages = {259--264},
  publisher = {Elsevier BV},
  issn = {0020-0190},
  doi = {10.1016/s0020-0190(97)00070-7},
  urldate = {2025-07-25},
  abstract = {We focus on notions of resource-bounded complexity for infinite binary sequences, and compare both, a definition based on Kobayashi's concept of compressibility, and the uniform approach studied by Loveland. It is known that for constant bounds on the complexity these definitions exactly coincide, and characterize the polynomial-time computable sequences when the running time is bounded by a polynomial, together with the recursive sequences when there is no time bound. We show here how for complexity functions that are monotonic, and recursive, the Kobayashi and Loveland complexity concepts are equivalent under a small constant factor. This also works under time bounds if instead of bounding functions that are recursive, those that are computed within the allowed time are considered. @ 1997 Elsevier Science B.V.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/5JPTTU66/Hermo - 1997 - Compressibility and uniform complexity.pdf}
}

@book{hernandezherediaProgressArtificialIntelligence2024,
  title = {Progress in {{Artificial Intelligence}} and {{Pattern Recognition}}: 8th {{International Congress}} on {{Artificial Intelligence}} and {{Pattern Recognition}}, {{IWAIPR}} 2023, {{Varadero}}, {{Cuba}}, {{September}} 27--29, 2023, {{Proceedings}}},
  shorttitle = {Progress in {{Artificial Intelligence}} and {{Pattern Recognition}}},
  editor = {Hern{\'a}ndez Heredia, Yanio and Mili{\'a}n N{\'u}{\~n}ez, Vladimir and Ruiz Shulcloper, Jos{\'e}},
  year = {2024},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {14335},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-49552-6},
  urldate = {2024-08-06},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-3-031-49551-9 978-3-031-49552-6},
  langid = {english},
  annotation = {GSCC: 0000000},
  file = {/home/elessar/Zotero/storage/HAX2HIRC/Hernández Heredia et al. - 2024 - Progress in Artificial Intelligence and Pattern Re.pdf}
}

@incollection{hinautWhichHypeMy2021,
  title = {Which {{Hype}} for {{My New Task}}? {{Hints}} and {{Random Search}} for {{Echo State Networks Hyperparameters}}},
  shorttitle = {Which {{Hype}} for {{My New Task}}?},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} -- {{ICANN}} 2021},
  author = {Hinaut, Xavier and Trouvain, Nathan},
  editor = {Farka{\v s}, Igor and Masulli, Paolo and Otte, Sebastian and Wermter, Stefan},
  year = {2021},
  volume = {12895},
  pages = {83--97},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-86383-8_7},
  urldate = {2025-02-16},
  abstract = {In learning systems, hyperparameters are parameters that are not learned but need to be set a priori. In Reservoir Computing, there are several parameters that needs to be set a priori depending on the task. Newcomers to Reservoir Computing cannot have a good intuition on which hyperparameters to tune and how to tune them. For instance, beginners often explore the reservoir sparsity, but in practice this parameter is not of high influence on performance for ESNs. Most importantly, many authors keep doing suboptimal hyperparameter searches: using grid search as a tool to explore more than two hyperparameters, while restraining the spectral radius to be below unity. In this short paper, we give some suggestions, intuitions, and give a general method to find robust hyperparameters while understanding their influence on performance. We also provide a graphical interface (included in ReservoirPy) in order to make this hyperparameter search more intuitive. Finally, we discuss some potential refinements of the proposed method.},
  isbn = {978-3-030-86382-1 978-3-030-86383-8},
  langid = {english},
  file = {/home/elessar/Zotero/storage/IHRTQCYP/Hinaut and Trouvain - 2021 - Which Hype for My New Task Hints and Random Search for Echo State Networks Hyperparameters.pdf}
}

@article{hitchcockDimensionEntropyRates2006,
  title = {Dimension, Entropy Rates, and Compression},
  author = {Hitchcock, John M. and Vinodchandran, N.V.},
  year = {2006},
  month = jun,
  journal = {Journal of Computer and System Sciences},
  volume = {72},
  number = {4},
  pages = {760--782},
  publisher = {Elsevier BV},
  issn = {0022-0000},
  doi = {10.1016/j.jcss.2005.10.002},
  urldate = {2025-07-25},
  abstract = {This paper develops new relationships between resource-bounded dimension, entropy rates, and compression. New tools for calculating dimensions are given and used to improve previous results about circuit-size complexity classes.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/I5P6Y94K/Hitchcock and Vinodchandran - 2006 - Dimension, entropy rates, and compression.pdf}
}

@article{hoffmannComputationalLimitationsSelforganization1997,
  title = {On Computational Limitations of Self-Organization},
  author = {Hoffmann, Achim G.},
  year = {1997},
  month = apr,
  journal = {Neurocomputing},
  volume = {15},
  number = {1},
  pages = {69--87},
  publisher = {Elsevier BV},
  issn = {0925-2312},
  doi = {10.1016/s0925-2312(96)00042-2},
  urldate = {2025-07-25},
  abstract = {The idea of self-organization has attracted a lot of interest from scientists in different disciplines, including Computer Science, Neuroscience, Cognitive Science, and Philosophy. The complex dynamics occurring in Neural Network-style self-organizing systems pose an extraordinary difficulty for an analysis of the potential and limitations of such systems. However, this paper provides rigorous bounds on the complexity of structures which may be emerging in self-organizing systems. It is shown how the notion of algorithmic information theory can elegantly be used to analyze the complex dynamics of parallel and distributed systems. First, results using this method are presented. Conditions are identified under which the complexity of a meaningful structure that can possibly emerge, is severely limited by the complexity of the system before it is exposed to training data! On the other hand, conditions are identified under which the acquisition of complex functions is possible. The results provide insight into the factors which may enable successful learning from unclassified data. They also indicate that the conditions under which a self-organizing system can be successfully employed to construct truly complex and useful classification functions require careful examination through further research.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/CSWG5WT8/Hoffmann - 1997 - On computational limitations of self-organization.pdf}
}

@misc{hoffmanRobustLearningJacobian2019,
  title = {Robust {{Learning}} with {{Jacobian Regularization}}},
  author = {Hoffman, Judy and Roberts, Daniel A. and Yaida, Sho},
  year = {2019},
  month = aug,
  number = {arXiv:1908.02729},
  eprint = {1908.02729},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1908.02729},
  urldate = {2025-03-20},
  abstract = {Design of reliable systems must guarantee stability against input perturbations. In machine learning, such guarantee entails preventing overfitting and ensuring robustness of models against corruption of input data. In order to maximize stability, we analyze and develop a computationally efficient implementation of Jacobian regularization that increases classification margins of neural networks. The stabilizing effect of the Jacobian regularizer leads to significant improvements in robustness, as measured against both random and adversarial input perturbations, without severely degrading generalization properties on clean data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/elessar/Zotero/storage/RBZYK7SA/Hoffman et al. - 2019 - Robust Learning with Jacobian Regularization.pdf}
}

@article{horibeNoteKolmogorovComplexity2003,
  title = {A Note on {{Kolmogorov}} Complexity and Entropy},
  author = {Horibe, Yasuichi},
  year = {2003},
  month = oct,
  journal = {Applied Mathematics Letters},
  volume = {16},
  number = {7},
  pages = {1129--1130},
  publisher = {Elsevier BV},
  issn = {0893-9659},
  doi = {10.1016/s0893-9659(03)90105-4},
  urldate = {2025-07-25},
  abstract = {It is shown that the Kolmogorov complexity per symbol of an n-sequence from a stationary ergodic source of finite alphabet approaches the entropy rate of the source in probability as n becomes large. @ 2003 Elsevier Ltd. All rights reserved.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/CZBAXWHJ/Horibe - 2003 - A note on Kolmogorov complexity and entropy.pdf}
}

@article{horneroComplexityAnalysisArterial2008,
  title = {Complexity {{Analysis}} of {{Arterial Pressure During Periods}} of {{Abrupt Hemodynamic Changes}}},
  author = {Hornero, R. and Aboy, M. and Gomez, C. and Hagg, D.S. and Phillips, C.R.},
  year = {2008},
  month = feb,
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {55},
  number = {2},
  pages = {797--801},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9294, 1558-2531},
  doi = {10.1109/tbme.2007.901037},
  urldate = {2025-07-15},
  abstract = {In this communication, we estimated the Lempel--Ziv complexity (LZC) on over 40 h of arterial blood pressure (ABP) recordings corresponding to 18 mechanically ventilated animal subjects. In this study, all subjects underwent a period of abrupt hemodynamic changes after an induced injury involving severe blood loss leading to hemorrhagic shock, followed by fluid resuscitation using either lactated ringers or 0.9\% normal saline. The LZC metric experienced a statistically significant increase ( 0 01) immediately following the induced injury and a statistically significant reduction following the administration of fluid therapy ( 0 01). These results indicate that LZC of ABP may be useful as a dynamic metric to assess fluid responsiveness.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/BLRXZFFQ/Hornero et al. - 2008 - Complexity Analysis of Arterial Pressure During Periods of Abrupt Hemodynamic Changes.pdf}
}

@article{horneroVariabilityRegularityComplexity2006,
  title = {Variability, {{Regularity}}, and {{Complexity}} of {{Time Series Generated}} by {{Schizophrenic Patients}} and {{Control Subjects}}},
  author = {Hornero, R. and Abasolo, D. and Jimeno, N. and Sanchez, C.I. and Poza, J. and Aboy, M.},
  year = {2006},
  month = feb,
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {53},
  number = {2},
  pages = {210--218},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9294},
  doi = {10.1109/tbme.2005.862547},
  urldate = {2025-07-15},
  abstract = {We analyzed time series generated by 20 schizophrenic patients and 20 sex- and age-matched control subjects using three nonlinear methods of time series analysis as test statistics: central tendency measure (CTM) from the scatter plots of first differences of data, approximate entropy (ApEn), and Lempel-Ziv (LZ) complexity. We divided our data into a training set (10 patients and 10 control subjects) and a test set (10 patients and 10 control subjects). The training set was used for algorithm development and optimum threshold selection. Each method was assessed prospectively using the test dataset. We obtained 80\% sensitivity and 90\% specificity with LZ complexity, 90\% sensitivity, and 60\% specificity with ApEn, and 70\% sensitivity and 70\% specificity with CTM. Our results indicate that there exist differences in the ability to generate random time series between schizophrenic subjects and controls, as estimated by the CTM, ApEn, and LZ. This finding agrees with most previous results showing that schizophrenic patients are characterized by less complex neurobehavioral and neuropsychologic measurements.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/U54H27CH/Hornero et al. - 2006 - Variability, Regularity, and Complexity of Time Series Generated by Schizophrenic Patients and Contr.pdf}
}

@article{huangTrendsExtremeLearning2015,
  title = {Trends in Extreme Learning Machines: {{A}} Review},
  shorttitle = {Trends in Extreme Learning Machines},
  author = {Huang, Gao and Huang, Guang-Bin and Song, Shiji and You, Keyou},
  year = {2015},
  month = jan,
  journal = {Neural Networks},
  volume = {61},
  pages = {32--48},
  issn = {08936080},
  doi = {10.1016/j.neunet.2014.10.001},
  urldate = {2025-03-07},
  abstract = {Extreme learning machine (ELM) has gained increasing interest from various research fields recently. In this review, we aim to report the current state of the theoretical research and practical advances on this subject. We first give an overview of ELM from the theoretical perspective, including the interpolation theory, universal approximation capability, and generalization ability. Then we focus on the various improvements made to ELM which further improve its stability, sparsity and accuracy under general or specific conditions. Apart from classification and regression, ELM has recently been extended for clustering, feature selection, representational learning and many other learning tasks. These newly emerging algorithms greatly expand the applications of ELM. From implementation aspect, hardware implementation and parallel computation techniques have substantially sped up the training of ELM, making it feasible for big data processing and real-time reasoning. Due to its remarkable efficiency, simplicity, and impressive generalization performance, ELM have been applied in a variety of domains, such as biomedical engineering, computer vision, system identification, and control and robotics. In this review, we try to provide a comprehensive view of these advances in ELM together with its future perspectives.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/H9LUFQRX/Huang et al. - 2015 - Trends in extreme learning machines A review.pdf}
}

@article{hublerOrderDisorderOpen2010,
  title = {Order and Disorder in Open Systems},
  author = {H{\"u}bler, Alfred and Crutchfield, James P.},
  year = {2010},
  month = sep,
  journal = {Complexity},
  volume = {16},
  number = {1},
  pages = {6--9},
  publisher = {Wiley},
  issn = {1076-2787, 1099-0526},
  doi = {10.1002/cplx.20344},
  urldate = {2025-07-15},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  file = {/home/elessar/Zotero/storage/CAYID7JY/Hübler and Crutchfield - 2010 - Order and disorder in open systems.pdf}
}

@article{hutterSequentialPredictionsBased2006,
  title = {Sequential Predictions Based on Algorithmic Complexity},
  author = {Hutter, Marcus},
  year = {2006},
  month = feb,
  journal = {Journal of Computer and System Sciences},
  volume = {72},
  number = {1},
  pages = {95--117},
  publisher = {Elsevier BV},
  issn = {0022-0000},
  doi = {10.1016/j.jcss.2005.07.001},
  urldate = {2025-07-25},
  abstract = {This paper studies sequence prediction based on the monotone Kolmogorov complexity Km = - log m, i.e. based on universal deterministic/one-part MDL. m is extremely close to Solomonoff's universal prior M, the latter being an excellent predictor in deterministic as well as probabilistic environments, where performance is measured in terms of convergence of posteriors or losses. Despite this closeness to M, it is difficult to assess the prediction quality of m, since little is known about the closeness of their posteriors, which are the important quantities for prediction. We show that for deterministic computable environments, the ``posterior'' and losses of m converge, but rapid convergence could only be shown on-sequence; the off-sequence convergence can be slow. In probabilistic environments, neither the posterior nor the losses converge, in general.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/UX2FIIXU/Hutter - 2006 - Sequential predictions based on algorithmic complexity.pdf}
}

@article{ichifujiiKolmogorovsComplexityPositive2002,
  title = {Kolmogorov's Complexity for Positive Definite Matrices},
  author = {Ichi Fujii, Jun and Fujii, Masatoshi},
  year = {2002},
  month = jan,
  journal = {Linear Algebra and its Applications},
  volume = {341},
  number = {1-3},
  pages = {171--180},
  publisher = {Elsevier BV},
  issn = {0024-3795},
  doi = {10.1016/s0024-3795(01)00354-8},
  urldate = {2025-07-25},
  abstract = {Based on Kolmogorov's idea, complexity of positive definite matrices with respect to a unit vector is defined. We show that the range of the complexity coincides with the logarithm of its spectrum and the order induced by the complexity is equivalent to the spectral one. This order implies the reversed one induced by the operator entropy. {\copyright} 2002 Elsevier Science Inc. All rights reserved.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/QLRJ9PPI/Ichi Fujii and Fujii - 2002 - Kolmogorov's complexity for positive definite matrices.pdf}
}

@article{iiiNUMERICALSTUDYLIKELIHOOD,
  title = {A {{NUMERICAL STUDY OF THE LIKELIHOOD OF PHASE LOCKING}}},
  author = {Iii, E Lanford},
  langid = {english},
  file = {/home/elessar/Zotero/storage/54QPJG8N/Iii - A NUMERICAL STUDY OF THE LIKELIHOOD OF PHASE LOCKING.pdf}
}

@article{ilieWORDCOMPLEXITYREPETITIONS2004,
  title = {{{WORD COMPLEXITY AND REPETITIONS IN WORDS}}},
  author = {Ilie, Lucian and Yu, Sheng and Zhang, Kaizhong},
  year = {2004},
  month = feb,
  journal = {International Journal of Foundations of Computer Science},
  volume = {15},
  number = {01},
  pages = {41--55},
  publisher = {World Scientific Pub Co Pte Lt},
  issn = {0129-0541, 1793-6373},
  doi = {10.1142/s0129054104002297},
  urldate = {2025-07-24},
  abstract = {With ideas from data compression and combinatorics on words, we introduce a complexity measure for words, called repetition complexity, which quantifies the amount of repetition in a word. The repetition complexity of w, r(w), is defined as the smallest amount of space needed to store w when reduced by repeatedly applying the following procedure: n consecutive occurrences uu . . . u of the same subword u of w are stored as (u, n). The repetition complexity has interesting relations with well-known complexity measures, such as subword complexity, sub, and Lempel-Ziv complexity, lz. We have always r(w) {$\geq$} lz(w) and could even be that the former is linear while the latter is only logarithmic; e.g., this happens for prefixes of certain infinite words obtained by iterated morphisms. An infinite word {$\alpha$} being ultimately periodic is equivalent to: (i) sub(prefn({$\alpha$})) = O(n), (ii) lz(prefn({$\alpha$})) = O(1), and (iii) r(prefn({$\alpha$})) = lg n + O(1). De Bruijn words, well known for their high subword complexity, are shown to have almost highest repetition complexity; the precise complexity remains open. r(w) can be computed in time O(n3(log n)2) and it is open, and probably very difficult, to find fast algorithms.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/3DRKE8DR/Ilie et al. - 2004 - WORD COMPLEXITY AND REPETITIONS IN WORDS.pdf}
}

@article{ismailApplicationReservoirComputing2019,
  title = {Application of {{Reservoir Computing}} for the {{Modeling}} of {{Nano-Contact Vortex Oscillator}}},
  author = {Ismail, Ali Rida and Jovanovic, Slavisa and {Petit-Watelot}, S{\'e}bastien and Rabah, Hassan},
  year = {2019},
  month = nov,
  journal = {Electronics},
  volume = {8},
  number = {11},
  pages = {1315},
  issn = {2079-9292},
  doi = {10.3390/electronics8111315},
  urldate = {2023-05-30},
  abstract = {The Nano-Contact Vortex Oscillator (NCVO) is a highly nonlinear spintronic device that can depict chaotic and nonchaotic behaviors according to the current flowing through it. The potential use of such a device in the future-generation computing systems requires the knowledge of a realistic model capable of describing its exact dynamics. In this paper, we firstly investigate the behavior of NCVO based on the power spectral analysis. Furthermore, we propose and demonstrate two efficient approaches of reservoir computing for the modeling of such a device. The performances of the proposed models are addressed in two ways. First, the generated time-varying signals are compared with the simulated magnetizations of the NCVO at different operating currents. Then, the power spectral analysis of one of the two models is carried out to examine its overall behavior over the complete DC current operating range and its ability to diagnose chaotic and non-chaotic regimes. The proposed models show quite promising results that can be counted on for further research.},
  langid = {english},
  annotation = {GSCC: 0000004},
  file = {/home/elessar/Zotero/storage/XYP6TWRW/Ismail et al. - 2019 - Application of Reservoir Computing for the Modelin.pdf}
}

@article{itoRotationSetsAre,
  title = {Rotation Sets Are Closed},
  author = {Ito, Ryuichi},
  langid = {english},
  file = {/home/elessar/Zotero/storage/6LWIUZGE/Ito - Rotation sets are closed.pdf}
}

@article{jacobsonNanotechnologyMeetsMarine2002,
  title = {Nanotechnology Meets Marine Biology},
  author = {Jacobson, A.},
  year = {2002},
  month = jul,
  journal = {Computing in Science \& Engineering},
  volume = {4},
  number = {4},
  pages = {10--11},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {1521-9615},
  doi = {10.1109/mcise.2002.1014973},
  urldate = {2025-07-25},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/Y2XPDGTU/Jacobson - 2002 - Nanotechnology meets marine biology.PDF}
}

@inproceedings{jacquetAsymptoticBehaviorLempelZiv,
  title = {Asymptotic Behavior of the {{Lempel-Ziv}} Parsing Scheme and Digital Trees},
  booktitle = {Proceedings of 1995 {{IEEE International Symposium}} on {{Information Theory}}},
  author = {Jacquet, P. and Szpankowski, W.},
  pages = {14},
  publisher = {IEEE},
  address = {Whistler, BC, Canada},
  doi = {10.1109/isit.1995.531116},
  urldate = {2025-07-15},
  abstract = {In this work, for the memoryless source with unequal probabilities of symbols generation we derive the limiting distribution for number of phrases in the Lempel-Ziv parsing scheme. This proves a long standing open problem. In order to establish it we had to solve another open problem, namely, that of deriving the limiting distribution of the internal path length in a digital search tree.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/PRG7UX9I/Jacquet and Szpankowski - Asymptotic behavior of the Lempel-Ziv parsing scheme and digital trees.pdf}
}

@article{jacquetAsymptoticBehaviorLempelZiv1995,
  title = {Asymptotic Behavior of the {{Lempel-Ziv}} Parsing Scheme and Digital Search Trees},
  author = {Jacquet, Philippe and Szpankowski, Wojciech},
  year = {1995},
  month = jun,
  journal = {Theoretical Computer Science},
  volume = {144},
  number = {1-2},
  pages = {161--197},
  publisher = {Elsevier BV},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(94)00298-w},
  urldate = {2025-07-15},
  abstract = {The Lempel-Ziv parsing scheme finds a wide range of applications, most notably in data compression and algorithms on words. It partitions a sequence of length n into variable phrases such that a new phrase is the shortest substring not seen in the past as a phrase. The parameter of interest is the number M n of phrases that one can construct from a sequence of length n. In this paper, for the memoryless source with unequal probabilities of symbols generation we derive the limiting distribution of M n which turns out to be normal. This proves a long standing open problem. In fact, to obtain this result we solved another open problem, namely, that of establishing the limiting distribution of the internal path length in a digital search tree. The latter is a consequence of an asymptotic solution of a multiplicative differentialfunctional equation often arising in the analysis of algorithms on words. Interestingly enough, our findings are proved by a combination of probabilistic techniques such as renewal equation and uniform integrability, and analytical techniques such as Mellin transform, difIerentialfunctional equations, de-Poissonlzation, and so forth. In concluding remarks we indicate a possibility of extending our results to Markovian models.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/55VF7L9H/Jacquet and Szpankowski - 1995 - Asymptotic behavior of the Lempel-Ziv parsing scheme and digital search trees.pdf}
}

@inproceedings{jacquetLimitingDistributionLempel2011,
  title = {Limiting Distribution of {{Lempel Ziv}}'78 Redundancy},
  booktitle = {2011 {{IEEE International Symposium}} on {{Information Theory Proceedings}}},
  author = {Jacquet, Philippe and Szpankowski, Wojciech},
  year = {2011},
  month = jul,
  pages = {1509--1513},
  publisher = {IEEE},
  address = {St. Petersburg, Russia},
  doi = {10.1109/isit.2011.6033794},
  urldate = {2025-07-15},
  abstract = {We show that the Lempel Ziv'78 redundancy rate tends to a Gaussian distribution for memoryless sources. We accomplish it by extending findings from our 1995 paper [3]. We present a new simplified proof of the Central Limit Theorem for the number of phrases in the LZ'78 algorithm. As in our 1995 paper, here we first analyze the asymptotic behavior of the total path length in a digital search tree (a DST) built from independent sequences. Then we present simplified proofs and extend our analysis of LZ'78 algorithm to include new results on the convergence of moments, moderate and large deviations, and redundancy analysis.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/UGWI97UA/Jacquet and Szpankowski - 2011 - Limiting distribution of Lempel Ziv'78 redundancy.pdf}
}

@article{jaegerAdaptiveNonlinearSystem,
  title = {Adaptive {{Nonlinear System Identification}} with {{Echo State Networks}}},
  author = {Jaeger, Herbert},
  abstract = {Echo state networks (ESN) are a novel approach to recurrent neural network training. An ESN consists of a large, fixed, recurrent "reservoir" network, from which the desired output is obtained by training suitable output connection weights. Determination of optimal output weights becomes a linear, uniquely solvable task of MSE minimization. This article reviews the basic ideas and describes an online adaptation scheme based on the RLS algorithm known from adaptive linear systems. As an example, a 10-th order NARMA system is adaptively identified. The known benefits of the RLS algorithms carryover from linear systems to nonlinear ones; specifically, the convergence rate and misadjustment can be determined at design time.},
  langid = {english},
  annotation = {GSCC: 0000849},
  file = {/home/elessar/Zotero/storage/RVFQAUDR/Jaeger - Adaptive Nonlinear System Identification with Echo.pdf}
}

@article{jaegerDiscoveringMultiscaleDynamical,
  title = {Discovering Multiscale Dynamical Features with Hierarchical {{Echo State Networks}}},
  author = {Jaeger, Herbert},
  abstract = {Many time series of practical relevance data have multi-scale characteristics. Prime examples are speech, texts, writing, or gestures. If one wishes to learn models of such systems, the models must be capable to represent dynamical features on different temporal and/or spatial scales. One natural approach to this end is hierarchical models, where higher processing layers are responsible for processing longer-range (slower, coarser) dynamical features of the input signal. This report introduces a hierarchical architecture where the core ingredient of each layer is an echo state network. In a bottom-up flow of information, throughout the architecture increasingly coarse features are extracted from the input signal. In a top-down flow of information, feature expectations are passed down. The architecture as a whole is trained on a one-step input prediction task by stochastic error gradient descent. The report presents a formal specification of these hierarchical systems and illustrates important aspects of its functioning in a case study with synthetic data.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/K96GNCLY/Jaeger - Discovering multiscale dynamical features with hierarchical Echo State Networks.pdf}
}

@article{jaegerEchoStateApproach2001,
  title = {The ``Echo State'' Approach to Analysing and Training Recurrent Neural Networks -- with an {{Erratum}} Note},
  author = {Jaeger, Herbert},
  year = {2001},
  langid = {english},
  annotation = {GSCC: 0004281},
  file = {/home/elessar/Zotero/storage/BVB9EK8H/Jaeger - The “echo state” approach to analysing and trainin.pdf}
}

@article{jaegerHarnessingNonlinearityPredicting2004,
  title = {Harnessing {{Nonlinearity}}, {{Predicting Chaotic Systems}} and {{Saving Energy}} in {{Wireless Communication}}},
  shorttitle = {Harnessing {{Nonlinearity}}},
  author = {Jaeger, Herbert and Haas, Harald},
  year = {2004},
  month = apr,
  journal = {Science},
  volume = {304},
  number = {5667},
  pages = {78--80},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1091277},
  urldate = {2023-05-30},
  abstract = {We present a method for learning nonlinear systems, echo state networks (ESNs). ESNs employ artificial recurrent neural networks in a way that has recently been proposed independently as a learning mechanism in biological brains. The learning method is computationally efficient and easy to use. On a benchmark task of predicting a chaotic time series, accuracy is improved by a factor of 2400 over previous techniques. The potential for engineering applications is illustrated by equalizing a communication channel, where the signal error rate is improved by two orders of magnitude.},
  langid = {english},
  annotation = {GSCC: 0003987},
  file = {/home/elessar/Zotero/storage/CMK4EL37/Jaeger - Supporting Online Material.pdf;/home/elessar/Zotero/storage/G7KWCJT6/Jaeger and Haas - 2004 - Harnessing Nonlinearity Predicting Chaotic Systems and Saving Energy in Wireless Communication.pdf;/home/elessar/Zotero/storage/GL58E3W6/Jaeger_Haas_2004_Harnessing Nonlinearity, Predicting Chaotic Systems and Saving Energy in.pdf}
}

@article{jaegerOptimizationApplicationsEcho2007,
  title = {Optimization and Applications of Echo State Networks with Leaky- Integrator Neurons},
  author = {Jaeger, Herbert and Luko{\v s}evi{\v c}ius, Mantas and Popovici, Dan and Siewert, Udo},
  year = {2007},
  month = apr,
  journal = {Neural Networks},
  volume = {20},
  number = {3},
  pages = {335--352},
  issn = {08936080},
  doi = {10.1016/j.neunet.2007.04.016},
  urldate = {2023-05-30},
  langid = {english},
  annotation = {GSCC: 0001030},
  file = {/home/elessar/Zotero/storage/EIZVG8HI/Jaeger et al. - 2007 - Optimization and applications of echo state networ.pdf}
}

@article{jaegerOverviewReservoirRecipes,
  title = {Overview of {{Reservoir Recipes}}},
  author = {Jaeger, Herbert and Luko{\v s}evi{\v c}ius, Mantas},
  abstract = {Echo State Networks (ESNs) and Liquid State Machines (LSMs) introduced a simple new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir ) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, made RNNs accessible for practical applications as never before and outperformed classical fully trained RNNs in many tasks. The latter, however, does not imply that random reservoirs are optimal, but rather that adequate training methods for them are yet to be developed. Thus much of the current research in reservoir computing is done on reservoir adaptation, redefining the paradigm as using different methods for training the reservoir and the readout. This report motivates the new definition of the paradigm and surveys the reservoir generation/adaptation techniques, offering a natural conceptual classification which transcends boundaries of the current ``brand-names'' of reservoir methods. The survey focuses more on methods relevant to practical applications of RNNs rather than modeling biological brains.},
  langid = {english},
  annotation = {GSCC: 0000021},
  file = {/home/elessar/Zotero/storage/3BIZQSF7/Overview of Reservoir Recipes.pdf}
}

@article{jaegerReservoirComputingApproaches2009,
  title = {Reservoir Computing Approaches to Recurrent Neural Network Training},
  author = {Jaeger, Herbert and Luko{\v s}evi{\v c}ius, Mantas},
  year = {2009},
  month = aug,
  journal = {Computer Science Review},
  volume = {3},
  number = {3},
  pages = {127--149},
  issn = {15740137},
  doi = {10.1016/j.cosrev.2009.03.005},
  urldate = {2023-05-30},
  abstract = {Echo State Networks and Liquid State Machines introduced a new paradigm in artificial recurrent neural network (RNN) training, where an RNN (the reservoir) is generated randomly and only a readout is trained. The paradigm, becoming known as reservoir computing, greatly facilitated the practical application of RNNs and outperformed classical fully trained RNNs in many tasks. It has lately become a vivid research field with numerous extensions of the basic idea, including reservoir adaptation, thus broadening the initial paradigm to using different methods for training the reservoir and the readout. This review systematically surveys both current ways of generating/adapting the reservoirs and training different types of readouts. It offers a natural conceptual classification of the techniques, which transcends boundaries of the current ``brand-names'' of reservoir methods, and thus aims to help in unifying the field and providing the reader with a detailed ``map'' of it.},
  langid = {english},
  annotation = {GSCC: 0003105},
  file = {/home/elessar/Zotero/storage/U9XH3JPX/Lukoševičius and Jaeger - 2009 - Reservoir computing approaches to recurrent neural.pdf}
}

@article{jaegerReservoirComputingTrends2012,
  title = {Reservoir {{Computing Trends}}},
  author = {Jaeger, Herbert and Luko{\v s}evi{\v c}ius, Mantas and Schrauwen, Benjamin},
  year = {2012},
  month = nov,
  journal = {KI - K{\"u}nstliche Intelligenz},
  volume = {26},
  number = {4},
  pages = {365--371},
  issn = {0933-1875, 1610-1987},
  doi = {10.1007/s13218-012-0204-5},
  urldate = {2023-05-30},
  abstract = {Reservoir Computing (RC) is a paradigm of understanding and training Recurrent Neural Networks (RNNs) based on treating the recurrent part (the reservoir ) differently than the readouts from it. It started ten years ago and is currently a prolific research area, giving important insights into RNNs, practical machine learning tools, as well as enabling computation with non-conventional hardware. Here we give a brief introduction into basic concepts, methods, insights, current developments, and highlight some applications of RC.},
  langid = {english},
  annotation = {GSCC: 0000458},
  file = {/home/elessar/Zotero/storage/B9A9ST2R/Lukoševičius et al. - 2012 - Reservoir Computing Trends.pdf}
}

@article{jaegerRevisitingEchoState2012,
  title = {Re-Visiting the Echo State Property},
  author = {Jaeger, Herbert and Yildiz, Izzet B. and Kiebel, Stefan J.},
  year = {2012},
  month = nov,
  journal = {Neural Networks},
  volume = {35},
  pages = {1--9},
  issn = {08936080},
  doi = {10.1016/j.neunet.2012.07.005},
  urldate = {2023-05-30},
  abstract = {An echo state network (ESN) consists of a large, randomly connected neural network, the reservoir, which is driven by an input signal and projects to output units. During training, only the connections from the reservoir to these output units are learned. A key requisite for output-only training is the echo state property (ESP), which means that the effect of initial conditions should vanish as time passes. In this paper, we use analytical examples to show that a widely used criterion for the ESP, the spectral radius of the weight matrix being smaller than unity, is not sufficient to satisfy the echo state property. We obtain these examples by investigating local bifurcation properties of the standard ESNs. Moreover, we provide new sufficient conditions for the echo state property of standard sigmoid and leaky integrator ESNs. We furthermore suggest an improved technical definition of the echo state property, and discuss what practicians should (and should not) observe when they optimize their reservoirs for specific tasks.},
  langid = {english},
  annotation = {GSCC: 0000514},
  file = {/home/elessar/Zotero/storage/VL8K27D8/Yildiz et al. - 2012 - Re-visiting the echo state property.pdf}
}

@article{jaegerShortTermMemory,
  title = {Short Term Memory in Echo State Networks},
  author = {Jaeger, H.},
  publisher = {Fraunhofer-Gesellschaft},
  doi = {10.24406/PUBLICA-FHG-291107},
  urldate = {2025-05-01},
  file = {/home/elessar/Zotero/storage/832BPWF2/Jaeger - Short term memory in echo state networks.pdf}
}

@article{jaegerSpecialIssueEcho2007,
  title = {Special Issue on Echo State Networks and Liquid State Machines},
  author = {Jaeger, Herbert and Maass, Wolfgang and Principe, Jose},
  year = {2007},
  month = apr,
  journal = {Neural Networks},
  volume = {20},
  number = {3},
  pages = {287--289},
  issn = {08936080},
  doi = {10.1016/j.neunet.2007.04.001},
  urldate = {2023-05-30},
  langid = {english},
  annotation = {GSCC: 0000215},
  file = {/home/elessar/Zotero/storage/HLX4ZQQ2/Jaeger et al. - 2007 - Special issue on echo state networks and liquid st.pdf}
}

@article{jaegerTimeWarpingInvariant,
  title = {Time {{Warping Invariant Echo State Networks}}},
  author = {Jaeger, Herbert and Luko{\v s}evi{\v c}ius, Mantas and Popovici, Dan and Siewert, Udo},
  abstract = {Echo State Networks (ESNs) is a recent simple and powerful approach to training recurrent neural networks (RNNs). In this report we present a modification of ESNs - time warping invariant echo state networks (TWIESNs) that can effectively deal with time warping in dynamic pattern recognition. The standard approach to classify time warped input signals is to align them to candidate prototype patterns by a dynamic programming method and use the alignment cost as a classification criterion. In contrast, we feed the original input signal into specifically designed ESNs which intrinsically are invariant to time warping in the input. For this purpose, ESNs with leaky integrator neurons are required, which are here presented for the first time, too. We then explain the TWIESN architecture and demonstrate their functioning on very strongly warped, synthetic data sets.},
  langid = {english},
  annotation = {GSCC: 0000058},
  file = {/home/elessar/Zotero/storage/XYDGZJE4/Lukoˇseviˇcius et al. - Time Warping Invariant Echo State Networks.pdf}
}

@article{jaegerTutorialTrainingRecurrent,
  title = {A Tutorial on Training Recurrent Neural Networks, Covering {{BPPT}}, {{RTRL}}, {{EKF}} and the "Echo State Network" Approach},
  author = {Jaeger, Herbert},
  abstract = {This tutorial is a worked-out version of a 5-hour course originally held at AIS in September/October 2002. It has two distinct components. First, it contains a mathematically-oriented crash course on traditional training methods for recurrent neural networks, covering back-propagation through time (BPTT), real-time recurrent learning (RTRL), and extended Kalman filtering approaches (EKF). This material is covered in Sections 2 -- 5. The remaining sections 1 and 6 -- 9 are much more gentle, more detailed, and illustrated with simple examples. They are intended to be useful as a stand-alone tutorial for the echo state network (ESN) approach to recurrent neural network training.},
  langid = {english},
  annotation = {GSCC: 0001463},
  file = {/home/elessar/Zotero/storage/N2PJKU94/Jaeger - A tutorial on training recurrent neural networks, .pdf}
}

@article{jamesInformationFlowsCritique2016,
  title = {Information {{Flows}}? {{A Critique}} of {{Transfer Entropies}}},
  shorttitle = {Information {{Flows}}?},
  author = {James, Ryan G. and Barnett, Nix and Crutchfield, James P.},
  year = {2016},
  month = jun,
  journal = {Physical Review Letters},
  volume = {116},
  number = {23},
  eprint = {1512.06479},
  primaryclass = {cond-mat},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.116.238701},
  urldate = {2025-07-15},
  abstract = {A central task in analyzing complex dynamics is to determine the loci of information storage and the communication topology of information flows within a system. Over the last decade and a half, diagnostics for the latter have come to be dominated by the transfer entropy. Via straightforward examples, we show that it and a derivative quantity, the causation entropy, do not, in fact, quantify the flow of information. At one and the same time they can overestimate flow or underestimate influence. We isolate why this is the case and propose alternate measures for information flow. An auxiliary consequence reveals that the proliferation of networks as a now-common theoretical model for large-scale systems in concert with the use of transfer-like entropies has shoehorned dyadic relationships into our structural interpretation of the organization and behavior of complex systems, despite the occurrence of polyadic dependencies. The net result is that much of the sophisticated organization of complex systems goes undetected.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Condensed Matter - Statistical Mechanics,Mathematics - Information Theory,Mathematics - Statistics Theory,Nonlinear Sciences - Adaptation and Self-Organizing Systems,Quantitative Biology - Molecular Networks,Statistics - Statistics Theory},
  file = {/home/elessar/Zotero/storage/QPH6X764/James et al. - 2016 - Information Flows A Critique of Transfer Entropies.pdf}
}

@article{jankeInformationGeometrySpherical2003,
  title = {Information Geometry of the Spherical Model},
  author = {Janke, W. and Johnston, D. A. and Kenna, R.},
  year = {2003},
  month = apr,
  journal = {Physical Review E},
  volume = {67},
  number = {4},
  publisher = {American Physical Society (APS)},
  issn = {1063-651X, 1095-3787},
  doi = {10.1103/physreve.67.046106},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/C4ZEHTDS/Janke et al. - 2003 - Information geometry of the spherical model.pdf}
}

@article{janzingComplexityMeasureContinuoustime2001,
  title = {Complexity Measure for Continuous-Time Quantum Algorithms},
  author = {Janzing, D. and Beth, {\relax Th}.},
  year = {2001},
  month = jul,
  journal = {Physical Review A},
  volume = {64},
  number = {2},
  publisher = {American Physical Society (APS)},
  issn = {1050-2947, 1094-1622},
  doi = {10.1103/physreva.64.022301},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/NUQ3D64M/Janzing and Beth - 2001 - Complexity measure for continuous-time quantum algorithms.pdf}
}

@article{jaurigueChaoticAttractorReconstruction2024,
  title = {Chaotic Attractor Reconstruction Using Small Reservoirs---the Influence of Topology},
  author = {Jaurigue, Lina},
  year = {2024},
  month = sep,
  journal = {Machine Learning: Science and Technology},
  volume = {5},
  number = {3},
  pages = {035058},
  issn = {2632-2153},
  doi = {10.1088/2632-2153/ad6ee8},
  urldate = {2025-08-12},
  abstract = {Abstract             Forecasting timeseries based upon measured data is needed in a wide range of applications and has been the subject of extensive research. A particularly challenging task is the forecasting of timeseries generated by chaotic dynamics. In recent years reservoir computing has been shown to be an effective method of forecasting chaotic dynamics and reconstructing chaotic attractors from data. In this work strides are made toward smaller and lower complexity reservoirs with the goal of improved hardware implementability and more reliable production of adequate surrogate models. We show that a reservoir of uncoupled nodes more reliably produces long term timeseries predictions than more complex reservoir topologies. We then link the improved attractor reconstruction of the uncoupled reservoir with smaller spectral radii of the resulting surrogate systems. These results indicate that, the node degree plays an important role in determining whether the desired dynamics will be stable in the autonomous surrogate system which is attained via closed-loop operation of the trained reservoir. In terms of hardware implementability, uncoupled nodes would allow for greater freedom in the hardware architecture because no complex coupling setups are needed and because, for uncoupled nodes, the system response is equivalent for space and time multiplexing.}
}

@article{jensenCompleteDevilsStaircase1983,
  title = {Complete {{Devil}}'s {{Staircase}}, {{Fractal Dimension}}, and {{Universality}} of {{Mode- Locking Structure}} in the {{Circle Map}}},
  author = {Jensen, M. H{\o}gh and Bak, Per and Bohr, Tomas},
  year = {1983},
  month = may,
  journal = {Physical Review Letters},
  volume = {50},
  number = {21},
  pages = {1637--1639},
  issn = {0031-9007},
  doi = {10.1103/PhysRevLett.50.1637},
  urldate = {2025-07-29},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/LUPZB78I/Jensen et al. - 1983 - Complete Devil's Staircase, Fractal Dimension, and Universality of Mode- Locking Structure in the Ci.pdf}
}

@article{jiangModelfreePredictionSpatiotemporal2019,
  title = {Model-Free Prediction of Spatiotemporal Dynamical Systems with Recurrent Neural Networks: {{Role}} of Network Spectral Radius},
  shorttitle = {Model-Free Prediction of Spatiotemporal Dynamical Systems with Recurrent Neural Networks},
  author = {Jiang, Junjie and Lai, Ying-Cheng},
  year = {2019},
  month = oct,
  journal = {Physical Review Research},
  volume = {1},
  number = {3},
  pages = {033056},
  issn = {2643-1564},
  doi = {10.1103/PhysRevResearch.1.033056},
  urldate = {2025-08-12},
  langid = {english},
  file = {/home/elessar/Zotero/storage/5D2LU9GB/Jiang and Lai - 2019 - Model-free prediction of spatiotemporal dynamical systems with recurrent neural networks Role of ne.pdf}
}

@inproceedings{jiayangTdecompositionAlgorithmLog2005,
  title = {A {{T-decomposition}} Algorithm with {{O}}(n Log n) Time and Space Complexity},
  booktitle = {Proceedings. {{International Symposium}} on {{Information Theory}}, 2005. {{ISIT}} 2005.},
  author = {{Jia Yang} and Speidel, U.},
  year = {2005},
  pages = {23--27},
  publisher = {IEEE},
  address = {Adelaide, Australia},
  doi = {10.1109/isit.2005.1523285},
  urldate = {2025-07-15},
  abstract = {T-decomposition maps a finite string into a series of parameters for a recursive string construction algorithm. Initially developed for the communication of coding trees [19], [4], T-decomposition has since been studied within the context of information measures. This involves the parsing of potentially very large strings, which in turn requires algorithms with good time complexity. This paper presents a T-decomposition algorithm with O(n log n) time and space complexity.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/QKTD7UD4/Jia Yang and Speidel - 2005 - A T-decomposition algorithm with O(n log n) time and space complexity.pdf}
}

@article{jinghuAnalysisBiomedicalSignals2006,
  title = {Analysis of {{Biomedical Signals}} by the {{Lempel-Ziv Complexity}}: The {{Effect}} of {{Finite Data Size}}},
  shorttitle = {Analysis of {{Biomedical Signals}} by the {{Lempel-Ziv Complexity}}},
  author = {{Jing Hu} and {Jianbo Gao} and Principe, J.C.},
  year = {2006},
  month = dec,
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {53},
  number = {12},
  pages = {2606--2609},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9294, 1558-2531},
  doi = {10.1109/tbme.2006.883825},
  urldate = {2025-07-15},
  abstract = {The Lempel-Ziv (LZ) complexity and its variants are popular metrics for characterizing biological signals. Proper interpretation of such analyses, however, has not been thoroughly addressed. In this letter, we study the the effect of finite data size. We derive analytic expressions for the LZ complexity for regular and random sequences, and employ them to develop a normalization scheme. To gain further understanding, we compare the LZ complexity with the correlation entropy from chaos theory in the context of epileptic seizure detection from EEG data, and discuss advantages of the normalized LZ complexity over the correlation entropy.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/2GBR5DAN/Jing Hu et al. - 2006 - Analysis of Biomedical Signals by the Lempel-Ziv Complexity the Effect of Finite Data Size.pdf;/home/elessar/Zotero/storage/6IYRPFGP/Jing Hu et al. - 2006 - Analysis of Biomedical Signals by the Lempel-Ziv Complexity the Effect of Finite Data Size.pdf}
}

@article{johariExcessEntropyDisordered1980,
  title = {On the Excess Entropy of Disordered Solids},
  author = {Johari, G. P.},
  year = {1980},
  month = jan,
  journal = {Philosophical Magazine B},
  volume = {41},
  number = {1},
  pages = {41--47},
  publisher = {Informa UK Limited},
  issn = {1364-2812, 1463-6417},
  doi = {10.1080/13642818008245368},
  urldate = {2025-07-25},
  abstract = {An analysis of the entropy of several types of disordered solids (glasses, orientationally disordered crystals and vapour-deposited amorphous solids) indicates that small-scale configurational changes make a significant contribution to their thermodynamic properties. The excess entropy of these solids over that of their ordered crystalline forms increases with temperature in a remarkably similar manner, despite the differences between the states of their molecular aggregation. Several implications of this behaviour for the thermodynamics of the disordered solids are pointed out.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/QHAT7SAU/Johari - 1980 - On the excess entropy of disordered solids.pdf}
}

@misc{johnsonEnumeratingFinitaryProcesses2012,
  title = {Enumerating {{Finitary Processes}}},
  author = {Johnson, B. D. and Crutchfield, J. P. and Ellison, C. J. and McTague, C. S.},
  year = {2012},
  month = dec,
  number = {arXiv:1011.0036},
  eprint = {1011.0036},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1011.0036},
  urldate = {2025-07-15},
  abstract = {We show how to efficiently enumerate a class of finite-memory stochastic processes using the causal representation of epsilon-machines. We characterize epsilon-machines in the language of automata theory and adapt a recent algorithm for generating accessible deterministic finite automata, pruning this over-large class down to that of epsilon-machines. As an application, we exactly enumerate topological epsilon-machines up to eight states and six-letter alphabets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Formal Languages and Automata Theory,Mathematics - Combinatorics,Mathematics - Dynamical Systems,Mathematics - Statistics Theory,Nonlinear Sciences - Chaotic Dynamics,Statistics - Statistics Theory},
  file = {/home/elessar/Zotero/storage/M5REYZLK/Johnson et al. - 2012 - Enumerating Finitary Processes.pdf}
}

@misc{johnsonSymmetryShannonsNoiseless2010,
  title = {Symmetry in {{Shannon}}'s {{Noiseless Coding Theorem}}},
  author = {Johnson, L. F.},
  year = {2010},
  month = oct,
  number = {arXiv:1010.6247},
  eprint = {1010.6247},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1010.6247},
  urldate = {2025-07-25},
  abstract = {Statements of Shannon's Noiseless Coding Theorem by various authors, including the original, are reviewed and clarified. Traditional statements of the theorem are often unclear as to when it applies. A new notation is introduced and the domain of application is clarified.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Mathematics - Information Theory},
  file = {/home/elessar/Zotero/storage/SCARWY4F/Johnson - 2010 - Symmetry in Shannon's Noiseless Coding Theorem.pdf}
}

@inproceedings{julianshunPracticalParallelLempelZiv2013,
  title = {Practical {{Parallel Lempel-Ziv Factorization}}},
  booktitle = {2013 {{Data Compression Conference}}},
  author = {{Julian Shun} and {Fuyao Zhao}},
  year = {2013},
  month = mar,
  pages = {123--132},
  publisher = {IEEE},
  address = {Snowbird, UT},
  doi = {10.1109/dcc.2013.20},
  urldate = {2025-07-15},
  abstract = {In the age of big data, the need for efficient data compression algorithms has grown. A widely used data compression method is the Lempel-Ziv-77 (LZ77) method, being a subroutine in popular compression packages such as gzip and PKZIP. There has been a lot of recent effort on developing practical sequential algorithms for Lempel-Ziv factorization (equivalent to LZ77 compression), but research in practical parallel implementations has been less satisfactory. In this work, we present a simple work-efficient parallel algorithm for Lempel-Ziv factorization. We show theoretically that our algorithm requires linear work and runs in O(log2 n) time (randomized) for constant alphabets and O(n ) time ( {$<$} 1) for integer alphabets. We present experimental results showing that our algorithm is efficient and achieves good speedup with respect to the best sequential implementations of Lempel-Ziv factorization.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/HQVWM9KE/Julian Shun and Fuyao Zhao - 2013 - Practical Parallel Lempel-Ziv Factorization.pdf}
}

@book{kacprzykSpringerHandbookComputational2015,
  title = {Springer {{Handbook}} of {{Computational Intelligence}}},
  editor = {Kacprzyk, Janusz and Pedrycz, Witold},
  year = {2015},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-43505-2},
  urldate = {2024-01-10},
  isbn = {978-3-662-43504-5 978-3-662-43505-2},
  langid = {english},
  annotation = {GSCC: 0000372},
  file = {/home/elessar/Zotero/storage/2PT3ACS9/Kacprzyk and Pedrycz - 2015 - Springer Handbook of Computational Intelligence.pdf}
}

@inproceedings{kadotaUseLempelzivAlgorithm,
  title = {Use {{Of Lempel-ziv Algorithm For Nonparametric Discrimination}}: {{Simulation Experiment}}},
  shorttitle = {Use {{Of Lempel-ziv Algorithm For Nonparametric Discrimination}}},
  booktitle = {Proceedings. 1991 {{IEEE International Symposium}} on {{Information Theory}}},
  author = {Kadota, T.T. and Ziv, J.},
  pages = {358--358},
  publisher = {IEEE},
  address = {Budapest, Hungary},
  doi = {10.1109/isit.1991.695414},
  urldate = {2025-07-15},
  abstract = {We investigate the feasibility of using the Lempel-Ziv parsing algorithm for non-parametric discrimination. The algorithm measures "randomness" of a sample sequence. Thus, it may be capable of discriminating a signal-containing sequence from a noise-only sequence since, by definition, the former is less random than the latter. The results of Monte Carlo simulation show that the algorithm can indeed be used for this discrimination as judged by the deflection ratio. We extend the results to the twodimensional data and visually demonstrate that the algorithm can detect a localized coherent object, such as a planar sinusoid, and reasonably discriminate it from random objects, such as Markov noise.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/HJ8VFWYB/Kadota and Ziv - Use Of Lempel-ziv Algorithm For Nonparametric Discrimination Simulation Experiment.pdf}
}

@article{kalnishkanGeneralLinearRelations,
  title = {General Linear Relations between Di!Erent Types of Predictive Complexity},
  author = {Kalnishkan, Yuri},
  abstract = {In this paper we introduce a general method of establishing tight linear inequalities between di!erent types of predictive complexity. Predictive complexity is a generalisation of Kolmogorov complexity and it bounds the ability of an algorithm to predict elements of a sequence. Our methodrelies upon probabilistic consiedrations andallows us to edscribe explicitly the sets of coe-cients which correspondto true inequalities. We apply this methodto two particular types of predictive complexity, namely, logarithmic complexity, which coincides with a variant of Kolmogorov complexity, andsquare-loss complexity, which is interesting for applications. c{\copyright} 2002 Elsevier Science B.V. All rights reserved.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/JNAI7B9Y/Kalnishkan - General linear relations between di!erent types of predictive complexity.pdf}
}

@article{kalnishkanHowManyStrings2005,
  title = {How Many Strings Are Easy to Predict?},
  author = {Kalnishkan, Yuri and Vovk, Vladimir and Vyugin, Michael V.},
  year = {2005},
  month = aug,
  journal = {Information and Computation},
  volume = {201},
  number = {1},
  pages = {55--71},
  publisher = {Elsevier BV},
  issn = {0890-5401},
  doi = {10.1016/j.ic.2005.04.001},
  urldate = {2025-07-25},
  abstract = {It is well known in the theory of Kolmogorov complexity that most strings cannot be compressed; more precisely, only exponentially few ( (2n-m)) binary strings of length n can be compressed by m bits. This paper extends the `incompressibility' property of Kolmogorov complexity to the `unpredictability' property of predictive complexity. The `unpredictability' property states that predictive complexity (defined as the loss suffered by a universal prediction algorithm working infinitely long) of most strings is close to a trivial upper bound (the loss suffered by a trivial minimax constant prediction strategy). We show that only exponentially few strings can be successfully predicted and find the base of the exponent.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/ZNIQWJP3/Kalnishkan et al. - 2005 - How many strings are easy to predict.pdf}
}

@inproceedings{kaltchenkoAlgorithmsEstimatingInformation2004,
  title = {Algorithms for Estimating Information Distance with Application to Bioinformatics and Linguistics},
  booktitle = {Canadian {{Conference}} on {{Electrical}} and {{Computer Engineering}} 2004 ({{IEEE Cat}}. {{No}}.{{04CH37513}})},
  author = {Kaltchenko, A.},
  year = {2004},
  pages = {2255-2258 Vol.4},
  publisher = {IEEE},
  address = {Niagara Falls, ON, Canada},
  doi = {10.1109/ccece.2004.1347695},
  urldate = {2025-07-25},
  abstract = {We review unnormalized and normalized informafion distances based on incomputable notions of Kolmogorov complexity and discuss how Kolmogorov complexi@can be approximated by data compression algorithms. We argue thaf opfimal algorithmsfor dafa compression with side information can be successfilly used fa approximafe the normalized distance. Next, we discuss an alfernative informafion distance, which is based on relafive enfropy rate (also known as Kullback-Leibier divergence). and compression-based algorithm for ifs esfimafion. We conjecture fhaf in Bioinformatics and Computational Linguistics fhis aiternative distance is more reievanf and imporfanf fhan the ones based on Kolmogorov compiexity.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/S8WIVKVT/Kaltchenko - 2004 - Algorithms for estimating information distance with application to bioinformatics and linguistics.pdf}
}

@book{kantzNonlinearTimeSeries2003,
  title = {Nonlinear {{Time Series Analysis}}},
  author = {Kantz, Holger and Schreiber, Thomas},
  year = {2003},
  month = nov,
  edition = {2},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511755798},
  urldate = {2025-08-12},
  abstract = {The paradigm of deterministic chaos has influenced thinking in many fields of science. Chaotic systems show rich and surprising mathematical structures. In the applied sciences, deterministic chaos provides a striking explanation for irregular behaviour and anomalies in systems which do not seem to be inherently stochastic. The most direct link between chaos theory and the real world is the analysis of time series from real systems in terms of nonlinear dynamics. Experimental technique and data analysis have seen such dramatic progress that, by now, most fundamental properties of nonlinear dynamical systems have been observed in the laboratory. Great efforts are being made to exploit ideas from chaos theory wherever the data displays more structure than can be captured by traditional methods. Problems of this kind are typical in biology and physiology but also in geophysics, economics, and many other sciences.},
  copyright = {https://www.cambridge.org/core/terms},
  isbn = {978-0-521-52902-0 978-0-521-82150-6 978-0-511-75579-8}
}

@book{kantzNonlinearTimeSeries2004,
  title = {Nonlinear Time Series Analysis},
  author = {Kantz, Holger and Schreiber, Thomas},
  year = {2004},
  edition = {2nd ed},
  publisher = {Cambridge University Press},
  address = {Cambridge, UK ; New York},
  isbn = {978-0-521-82150-6 978-0-521-52902-0},
  langid = {english},
  lccn = {QA280 .K355 2004},
  keywords = {Nonlinear theories,Time-series analysis},
  annotation = {GSCC: 0008918},
  file = {/home/elessar/Zotero/storage/T92T8XKW/Kantz and Schreiber - 2004 - Nonlinear time series analysis.pdf}
}

@article{kantzProblemSpuriousLyapunov2013,
  title = {The Problem of Spurious {{Lyapunov}} Exponents in Time Series Analysis and Its Solution by Covariant {{Lyapunov}} Vectors},
  author = {Kantz, Holger and Radons, G{\"u}nter and Yang, Hongliu},
  year = {2013},
  month = jun,
  journal = {Journal of Physics A: Mathematical and Theoretical},
  volume = {46},
  number = {25},
  pages = {254009},
  issn = {1751-8113, 1751-8121},
  doi = {10.1088/1751-8113/46/25/254009},
  urldate = {2023-05-30},
  abstract = {We briefly recall the methods for the determination of Lyapunov exponents from time series data and their limitations. One particular problem is given by the fact that reconstructed phase spaces have usually extra dimensions compared to the true phase space of a dynamical system, leading to extra, so called spurious Lyapunov exponents. Several methods to identify the true ones have been proposed which do not give satisfactory results. We show that the geometric information contained in covariant Lyapunov vectors can be used to identify the true exponents. We illustrate its use and its limitations by applying it to experimental NMR laser data.},
  langid = {english},
  annotation = {GSCC: 0000027},
  file = {/home/elessar/Zotero/storage/XNHGZ5FC/Kantz et al. - 2013 - The problem of spurious Lyapunov exponents in time.pdf}
}

@article{karabogaComparativeStudyArtificial2009,
  title = {A Comparative Study of {{Artificial Bee Colony}} Algorithm},
  author = {Karaboga, Dervis and Akay, Bahriye},
  year = {2009},
  month = aug,
  journal = {Applied Mathematics and Computation},
  volume = {214},
  number = {1},
  pages = {108--132},
  publisher = {Elsevier BV},
  issn = {0096-3003},
  doi = {10.1016/j.amc.2009.03.090},
  urldate = {2025-07-15},
  abstract = {Artificial Bee Colony (ABC) algorithm is one of the most recently introduced swarm-based algorithms. ABC simulates the intelligent foraging behaviour of a honeybee swarm. In this work, ABC is used for optimizing a large set of numerical test functions and the results produced by ABC algorithm are compared with the results obtained by genetic algorithm, particle swarm optimization algorithm, differential evolution algorithm and evolution strategies. Results show that the performance of the ABC is better than or similar to those of other population-based algorithms with the advantage of employing fewer control parameters.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/9BYHGGJ2/Karaboga and Akay - 2009 - A comparative study of Artificial Bee Colony algorithm.pdf}
}

@inproceedings{kawabataExactAnalysisLempelZiv,
  title = {Exact {{Analysis}} of the {{Lempel-Ziv Algorithm}} for {{I}}.{{I}}.{{D}}. {{Source}}},
  booktitle = {Proceedings. {{IEEE International Symposium}} on {{Information Theory}}},
  author = {Kawabata, T.},
  pages = {112--112},
  publisher = {IEEE},
  address = {San Antonio, TX},
  doi = {10.1109/isit.1993.748427},
  urldate = {2025-07-15},
  abstract = {A new analysis shows that, when we apply the Lempel-Zivincremental parsing algorithm to i.i.d. source with probabilitieq,, i = 1,...,m, the expected length EIWfI of the t-th parsed segment W t is given by a simple formula. Following this approach we can show a VF(Variab1e to Fixed length) version of Ziv-Lempel universal coding theorem.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/XI5NYAG7/Kawabata - Exact Analysis of the Lempel-Ziv Algorithm for I.I.D. Source.pdf}
}

@article{kawaiSmallworldTopologyEnhances2019,
  title = {A Small-World Topology Enhances the Echo State Property and Signal Propagation in Reservoir Computing},
  author = {Kawai, Yuji and Park, Jihoon and Asada, Minoru},
  year = {2019},
  month = apr,
  journal = {Neural Networks},
  volume = {112},
  pages = {15--23},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.01.002},
  urldate = {2025-03-12},
  abstract = {Cortical neural connectivity has been shown to exhibit a small-world (SW) network topology. However, the role of the topology in neural information processing remains unclear. In this study, we investigated the learning performance of an echo state network (ESN) that includes the SW topology as a reservoir. To elucidate the potential of the SW topology, we limited the numbers of the input and output nodes in the ESN and spatially segregated the output nodes from the input nodes. We tested the ESNs in two benchmark tasks: memory capacity and nonlinear time-series prediction. The SW-ESN exhibited the best learning performance when the spectral radius of the weight matrix was large and when the input and output nodes were segregated. That is, the SW topology provided the ESN with a stable echo state property over a broad range of the weight matrix and efficiently propagated input signals to the output nodes. This result is the same as that of the ESN using a real human cortical connectivity. Thus, the results suggest that the SW topology is essential for maintaining the echo state property, which is the appropriate neural dynamics between input and output brain regions.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/TAIL7IAP/Kawai et al. - 2019 - A small-world topology enhances the echo state property and signal propagation in reservoir computin.pdf}
}

@article{kellyComputationalElectromagneticsMetal2001,
  title = {Computational Electromagnetics of Metal Nanoparticles and Their Aggregates},
  author = {Kelly, K.L. and Lazarides, A.A. and Schatz, G.C.},
  year = {2001},
  journal = {Computing in Science \& Engineering},
  volume = {3},
  number = {4},
  pages = {67--73},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {1521-9615},
  doi = {10.1109/5992.931905},
  urldate = {2025-07-25},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/3IMKA4GB/Kelly et al. - 2001 - Computational electromagnetics of metal nanoparticles and their aggregates.PDF}
}

@article{khoussainovRandomnessComputabilityAlgebraic1998,
  title = {Randomness, Computability and Algebraic Specifications},
  author = {Khoussainov, Bakhadyr},
  year = {1998},
  month = jan,
  journal = {Annals of Pure and Applied Logic},
  volume = {91},
  number = {1},
  pages = {1--15},
  publisher = {Elsevier BV},
  issn = {0168-0072},
  doi = {10.1016/s0168-0072(97)00040-7},
  urldate = {2025-07-15},
  abstract = {This paper shows how the notion of randomness defines, in a natural way, an algebra. It turns out that the algebra is computably enumerable and finitely generated. The paper investigates algebraic and effective properties of this algebra.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/Z3AHX53H/Khoussainov - 1998 - Randomness, computability and algebraic specifications.pdf}
}

@article{kimNeuralMachineCode2023,
  title = {A Neural Machine Code and Programming Framework for the Reservoir Computer},
  author = {Kim, Jason Z. and Bassett, Dani S.},
  year = {2023},
  month = jun,
  journal = {Nature Machine Intelligence},
  volume = {5},
  number = {6},
  pages = {622--630},
  issn = {2522-5839},
  doi = {10.1038/s42256-023-00668-8},
  urldate = {2024-01-10},
  abstract = {Abstract             From logical reasoning to mental simulation, biological and artificial neural systems possess an incredible capacity for computation. Such neural computers offer a fundamentally novel computing paradigm by representing data continuously and processing information in a natively parallel and distributed manner. To harness this computation, prior work has developed extensive training techniques to understand existing neural networks. However, the lack of a concrete and low-level machine code for neural networks precludes us from taking full advantage of a neural computing framework. Here we provide such a machine code along with a programming framework by using a recurrent neural network---a reservoir computer---to decompile, code and compile analogue computations. By decompiling the reservoir's internal representation and dynamics into an analytic basis of its inputs, we define a low-level neural machine code that we use to program the reservoir to solve complex equations and store chaotic dynamical systems as random-access memory. We further provide a fully distributed neural implementation of software virtualization and logical circuits, and even program a playable game of pong inside of a reservoir computer. Importantly, all of these functions are programmed without requiring any example data or sampling of state space. Finally, we demonstrate that we can accurately decompile the analytic, internal representations of a full-rank reservoir computer that has been conventionally trained using data. Taken together, we define an implementation of neural computation that can both decompile computations from existing neural connectivity and compile distributed programs as new connections.},
  langid = {english},
  annotation = {GSCC: 0000020},
  file = {/home/elessar/Zotero/storage/5XI7SIDI/Kim and Bassett - 2023 - A neural machine code and programming framework fo.pdf;/home/elessar/Zotero/storage/G2QUEEF8/Kim and Bassett - 2023 - A neural machine code and programming framework fo.pdf;/home/elessar/Zotero/storage/KSC2ELB6/Kim and Bassett - 2023 - A neural machine code and programming framework fo.pdf;/home/elessar/Zotero/storage/QCGIS9XB/Kim and Bassett - 2023 - A neural machine code and programming framework fo.pdf}
}

@article{kimTimeSeriesPrediction2020,
  title = {Time Series Prediction Using Deep Echo State Networks},
  author = {Kim, Taehwan and King, Brian R.},
  year = {2020},
  month = dec,
  journal = {Neural Computing and Applications},
  volume = {32},
  number = {23},
  pages = {17769--17787},
  issn = {1433-3058},
  doi = {10.1007/s00521-020-04948-x},
  urldate = {2025-03-12},
  abstract = {Artificial neural networks have been used for time series modeling and forecasting in many domains. However, they are often limited in their handling of nonlinear and chaotic data. More recently, reservoir-based recurrent neural net systems, most notably echo state networks (ESN), have made substantial improvements for time series modeling. Their shallow nature lends themselves to an efficient training method, but has limitations on nonstationary, nonlinear chaotic time series, particularly large, multidimensional time series. In this paper, we propose a novel approach for forecasting time series data based on an additive decomposition (AD) applied to the time series as a preprocessor to a deep echo state network. We compare the performance of our method, AD-DeepESN, on popular neural net architectures used for time series prediction. Stationary and nonstationary data sets are used to evaluate the performance of the methods. Our results are compelling, demonstrating that AD-DeepESN has superior performance, particularly on the most challenging time series that exhibit non-stationarity and chaotic behavior compared to existing methods.},
  langid = {english},
  keywords = {Additive decomposition,Artificial Intelligence,Echo state network,Recurrent neural network,Reservoir computing,Time series forecasting},
  file = {/home/elessar/Zotero/storage/Z3G6REUT/Kim and King - 2020 - Time series prediction using deep echo state networks.pdf}
}

@article{kimWhatComplexGraph2008,
  title = {What Is a Complex Graph?},
  author = {Kim, Jongkwang and Wilhelm, Thomas},
  year = {2008},
  month = apr,
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {387},
  number = {11},
  pages = {2637--2652},
  issn = {03784371},
  doi = {10.1016/j.physa.2008.01.015},
  urldate = {2023-05-30},
  langid = {english},
  annotation = {GSCC: 0000282},
  file = {/home/elessar/Zotero/storage/ZQF2ZYWV/Kim and Wilhelm - 2008 - What is a complex graph.pdf}
}

@book{klabnikRustProgrammingLanguage2023,
  title = {The {{Rust Programming Language}}, 2nd {{Edition}}},
  author = {Klabnik, Steve},
  year = {2023},
  publisher = {No Starch Press},
  address = {New York},
  collaborator = {Nichols, Carol},
  isbn = {978-1-7185-0311-3},
  langid = {english},
  file = {/home/elessar/Zotero/storage/S9SSMSWD/Klabnik - 2023 - The Rust Programming Language, 2nd Edition.pdf}
}

@article{kobayashiKolmogorovComplexityUniversal1997,
  title = {The {{Kolmogorov}} Complexity, Universal Distribution, and Coding Theorem for Generalized Length Functions},
  author = {Kobayashi, K.},
  year = {1997},
  month = may,
  journal = {IEEE Transactions on Information Theory},
  volume = {43},
  number = {3},
  pages = {816--826},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9448},
  doi = {10.1109/18.568693},
  urldate = {2025-07-25},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/A9TACV3M/Kobayashi - 1997 - The Kolmogorov complexity, universal distribution, and coding theorem for generalized length functio.pdf}
}

@article{kongReservoircomputingBasedAssociative2024,
  title = {Reservoir-Computing Based Associative Memory and Itinerancy for Complex Dynamical Attractors},
  author = {Kong, Ling-Wei and Brewer, Gene A. and Lai, Ying-Cheng},
  year = {2024},
  month = jun,
  journal = {Nature Communications},
  volume = {15},
  number = {1},
  pages = {4840},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-49190-4},
  urldate = {2024-06-11},
  abstract = {Abstract             Traditional neural network models of associative memories were used to store and retrieve static patterns. We develop reservoir-computing based memories for complex dynamical attractors, under two common recalling scenarios in neuropsychology: location-addressable with an index channel and content-addressable without such a channel. We demonstrate that, for location-addressable retrieval, a single reservoir computing machine can memorize a large number of periodic and chaotic attractors, each retrievable with a specific index value. We articulate control strategies to achieve successful switching among the attractors, unveil the mechanism behind failed switching, and uncover various scaling behaviors between the number of stored attractors and the reservoir network size. For content-addressable retrieval, we exploit multistability with cue signals, where the stored attractors coexist in the high-dimensional phase space of the reservoir network. As the length of the cue signal increases through a critical value, a high success rate can be achieved. The work provides foundational insights into developing long-term memories and itinerancy for complex dynamical patterns.},
  langid = {english},
  annotation = {GSCC: 0000001},
  file = {/home/elessar/Zotero/storage/PKPRXAGP/Kong et al. - 2024 - Reservoir-computing based associative memory and i.pdf;/home/elessar/Zotero/storage/S62XRNYL/Kong et al. - 2024 - Reservoir-computing based associative memory and i.pdf}
}

@incollection{konkoliReservoirComputing2017,
  title = {Reservoir {{Computing}}},
  booktitle = {Encyclopedia of {{Complexity}} and {{Systems Science}}},
  author = {Konkoli, Zoran},
  editor = {Meyers, Robert A.},
  year = {2017},
  pages = {1--12},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-27737-5_683-1},
  urldate = {2023-05-30},
  isbn = {978-3-642-27737-5},
  langid = {english},
  annotation = {GSCC: 0001676},
  file = {/home/elessar/Zotero/storage/S8KHLJTI/Konkoli - 2017 - Reservoir Computing.pdf}
}

@inproceedings{kontoyiannisDistributionRecurrenceTimes,
  title = {On the Distribution of Recurrence Times and the Exact Asymptotics of {{Lempel-Ziv}} Coding},
  booktitle = {Proceedings of {{IEEE International Symposium}} on {{Information Theory}}},
  author = {Kontoyiannis, I.},
  pages = {312},
  publisher = {IEEE},
  address = {Ulm, Germany},
  doi = {10.1109/isit.1997.613236},
  urldate = {2025-07-15},
  abstract = {Let x = (. . . , Z - {\textasciitilde} , Z O , Z I , .. .) be a realization from the discrete, s t a t i o n a r y ergodic source X = \{ X , ; n E Z\}. We investigate the asymptotics of the recuwence time R, defined as the first t i m e that the n-block x;1 = ( X I ,22,.. . ,z.) recurs in the past of Z. We provide a natural framework for deducing the exact a s y m p t o t i c behavior of R,, and w e extract f r o m it precise information about the behavior of the pointwise r e d u n d a n c y of an idealized version of Lempel-Ziv coding.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/WUIUESHJ/Kontoyiannis - On the distribution of recurrence times and the exact asymptotics of Lempel-Ziv coding.pdf}
}

@inproceedings{kontoyiannisSecondorderAnalysisLossless,
  title = {Second-Order Analysis of Lossless and Lossy Versions of {{Lempel-Ziv}} Codes},
  booktitle = {Conference {{Record}} of the {{Thirty-First Asilomar Conference}} on {{Signals}}, {{Systems}} and {{Computers}} ({{Cat}}. {{No}}.{{97CB36136}})},
  author = {Kontoyiannis, I.},
  volume = {2},
  pages = {1349--1353},
  publisher = {IEEE Comput. Soc},
  address = {Pacific Grove, CA, USA},
  doi = {10.1109/acssc.1997.679123},
  urldate = {2025-07-15},
  abstract = {We present an overview of several recent results (some new and some known) on the asymptotic performance of d\$ferent variants of the Lempel-Ziv coding algorithm, in both the lossless case and the lossy case. The results are based on the asymptotic behavior of waiting times, following the general methodology introduced by Wynerand Ziv in 1989. We show that, in this framework, very precise statements can be made about the second-order (asymptotic)properties of the codeword lengths.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/EZPN4FHS/Kontoyiannis - Second-order analysis of lossless and lossy versions of Lempel-Ziv codes.pdf}
}

@article{kraskovEstimatingMutualInformation2004,
  title = {Estimating {{Mutual Information}}},
  author = {Kraskov, Alexander and Stoegbauer, Harald and Grassberger, Peter},
  year = {2004},
  month = jun,
  journal = {Physical Review E},
  volume = {69},
  number = {6},
  eprint = {cond-mat/0305641},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.69.066138},
  urldate = {2025-07-25},
  abstract = {We present two classes of improved estimators for mutual information \$M(X,Y)\$, from samples of random points distributed according to some joint probability density \${\textbackslash}mu(x,y)\$. In contrast to conventional estimators based on binnings, they are based on entropy estimates from \$k\$-nearest neighbour distances. This means that they are data efficient (with \$k=1\$ we resolve structures down to the smallest possible scales), adaptive (the resolution is higher where data are more numerous), and have minimal bias. Indeed, the bias of the underlying entropy estimates is mainly due to non-uniformity of the density at the smallest resolved scale, giving typically systematic errors which scale as functions of \$k/N\$ for \$N\$ points. Numerically, we find that both families become \{{\textbackslash}it exact\} for independent distributions, i.e. the estimator \${\textbackslash}hat M(X,Y)\$ vanishes (up to statistical fluctuations) if \${\textbackslash}mu(x,y) = {\textbackslash}mu(x) {\textbackslash}mu(y)\$. This holds for all tested marginal distributions and for all dimensions of \$x\$ and \$y\$. In addition, we give estimators for redundancies between more than 2 random variables. We compare our algorithms in detail with existing algorithms. Finally, we demonstrate the usefulness of our estimators for assessing the actual independence of components obtained from independent component analysis (ICA), for improving ICA, and for estimating the reliability of blind source separation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics},
  file = {/home/elessar/Zotero/storage/HCWKSNH5/Kraskov et al. - 2004 - Estimating Mutual Information.pdf}
}

@article{kratzerSurfaceKnowledgePredictive2001,
  title = {Surface Knowledge: Toward a Predictive Theory of Materials},
  shorttitle = {Surface Knowledge},
  author = {Kratzer, P. and Scheffler, M.},
  year = {2001},
  journal = {Computing in Science \& Engineering},
  volume = {3},
  number = {6},
  pages = {16--25},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {1521-9615},
  doi = {10.1109/5992.963424},
  urldate = {2025-07-25},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/9RNHK63N/Kratzer and Scheffler - 2001 - Surface knowledge toward a predictive theory of materials.PDF}
}

@article{kreinovichKolmogorovComplexityChaotic2003,
  title = {Kolmogorov Complexity and Chaotic Phenomena},
  author = {Kreinovich, Vladik and Kunin, Isaak A},
  year = {2003},
  month = mar,
  journal = {International Journal of Engineering Science},
  volume = {41},
  number = {3-5},
  pages = {483--493},
  publisher = {Elsevier BV},
  issn = {0020-7225},
  doi = {10.1016/s0020-7225(02)00211-2},
  urldate = {2025-07-25},
  abstract = {Born about three decades ago, Kolmogorov complexity theory (KC) led to important discoveries that, in particular, give a new understanding of the fundamental problem: interrelations between classical continuum mathematics and reality (physics, biology, engineering sciences,. . .).},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/RV7Z4ITF/Kreinovich and Kunin - 2003 - Kolmogorov complexity and chaotic phenomena.pdf}
}

@inbook{krishnamurthyAlgorithmicEntropyPhase2003,
  title = {Algorithmic {{Entropy}}, {{Phase Transition}}, and {{Smart Systems}}},
  booktitle = {Lecture {{Notes}} in {{Computer Science}}},
  year = {2003},
  pages = {333--342},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  issn = {0302-9743},
  doi = {10.1007/3-540-44863-2_33},
  urldate = {2025-07-15},
  abstract = {A smart system exhibits the three important properties: (i) interactive, collective, coordinated and parallel operation (ii) self-organization through emergent properties (iii) adaptive and flexible operation. A hierarchy based on metric entropy is suggested among the computational systems that transcend from the unsmart to the smart system through a phase transition like phenomenon. Understanding smart systems is useful to solve hard-optimization problem inspired by the self-organizing processes found in nature. Such systems will be valuable to create artificial systems made up of exotic matter to solve specific problems in particular domains of interest with a high efficiency.},
  collaborator = {Krishnamurthy, E. V.},
  isbn = {978-3-540-40196-4 978-3-540-44863-1},
  langid = {english},
  file = {/home/elessar/Zotero/storage/8UBJWGI6/2003 - Algorithmic Entropy, Phase Transition, and Smart Systems.PDF}
}

@misc{krivineDiscreteContinuousDynamics2004,
  title = {From Discrete to Continuous Dynamics and Back: {{How}} Large Is 1?},
  shorttitle = {From Discrete to Continuous Dynamics and Back},
  author = {Krivine, Hubert and Lesne, Annick and Treiner, Jacques},
  year = {2004},
  month = jun,
  number = {arXiv:cond-mat/0406432},
  eprint = {cond-mat/0406432},
  publisher = {arXiv},
  doi = {10.48550/arXiv.cond-mat/0406432},
  urldate = {2025-07-25},
  abstract = {Discrete autonomous dynamical systems in dimension 1 can exhibit chaotic behavior, whereas the corresponding continuous evolution equations rule it out, and cannot even possess a nontrivial periodic solution. Therefore the passage from discrete to continuous equations (and conversely) is all but harmless. We address this issue and evidence some caveats on the paradigmatic Verhulst logistic equation, investigating in particular the status and influence of the actual size of the unit time step in discrete modelings, rooted in well-known numerical analysis.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Other Condensed Matter},
  file = {/home/elessar/Zotero/storage/6R3G7S5K/Krivine et al. - 2004 - From discrete to continuous dynamics and back How large is 1.pdf}
}

@article{krivovichevAlgorithmicCrystalChemistry2012,
  title = {Algorithmic Crystal Chemistry: {{A}} Cellular Automata Approach},
  shorttitle = {Algorithmic Crystal Chemistry},
  author = {Krivovichev, S. V.},
  year = {2012},
  month = jan,
  journal = {Crystallography Reports},
  volume = {57},
  number = {1},
  pages = {10--17},
  publisher = {Pleiades Publishing Ltd},
  issn = {1063-7745, 1562-689X},
  doi = {10.1134/s1063774511060149},
  urldate = {2025-07-15},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/home/elessar/Zotero/storage/YYEASFE7/Krivovichev - 2012 - Algorithmic crystal chemistry A cellular automata approach.pdf}
}

@article{krivovichevStructuralComplexityMinerals2013,
  title = {Structural Complexity of Minerals: Information Storage and Processing in the Mineral World},
  shorttitle = {Structural Complexity of Minerals},
  author = {Krivovichev, S. V.},
  year = {2013},
  month = apr,
  journal = {Mineralogical Magazine},
  volume = {77},
  number = {3},
  pages = {275--326},
  publisher = {Mineralogical Society},
  issn = {0026-461X, 1471-8022},
  doi = {10.1180/minmag.2013.077.3.05},
  urldate = {2025-07-25},
  abstract = {Structural complexity of minerals is characterized using information contents of their crystal structures calculated according to the modified Shannon formula. The crystal structure is considered as a message consisting of atoms classified into equivalence classes according to their distribution over crystallographic orbits (Wyckoff sites). The proposed complexity measures combine both size- and symmetry-sensitive aspects of crystal structures. Information-based complexity parameters have been calculated for 3949 structure reports on minerals extracted from the Inorganic Crystal Structure Database. According to the total structural information content, IG,total, mineral structures can be classified into very simple (0 20 bits), simple (20 100 bits), intermediate (100 500 bits), complex (500 1000 bits), and very complex ({$>$} 1000 bits). The average information content for mineral structures is calculated as 228(6) bits per structure and 3.23(2) bits per atom. Twenty most complex mineral structures are (IG,total in bits): paulingite (6766.998), fantappieite (5948.330), sacrofanite (5317.353), mendeleevite-(Ce) (3398.878), bouazzerite (3035.201), megacyclite (2950.928), vandendriesscheite (2835.307), giuseppetite (2723.097), stilpnomelane (2483.819), stavelotite-(La) (2411.498), rogermitchellite (2320.653), parsettensite (2309.820), apjohnite (2305.361), antigorite (m = 17 polysome) (2250.397), tounkite (2187.799), tschoertnerite (2132.228), farneseite (2094.012), kircherite (2052.539), bannisterite (2031.017), and mutinaite (2025.067). The following complexitygenerating mechanisms have been recognized: modularity, misfit relationships between structure elements, and presence of nanoscale units (clusters or tubules). Structural complexity should be distiguished from topological complexity. Structural complexity increases with decreasing temperature and increasing pressure, though at ultra-high pressures, the situation may be different. Quantitative complexity measures can be used to investigate evolution of information in the course of global and local geological processes involving formation and transformation of crystalline phases. The information-based complexity measures can also be used to estimate the `ease of crystallization' from the viewpoint of simplexity principle proposed by J.R. Goldsmith (1953) for understanding of formation of simple and complex mineral phases under both natural and laboratory conditions. According to the proposed quantitative approach, the crystal structure can be viewed as a reservoir of information encoded in its complexity. Complex structures store more information than simple ones. As erasure of information is always associated with dissipation of energy, information stored in crystal structures of minerals must have an important influence upon natural processes. As every process can be viewed as a communication channel, the mineralogical history of our planet on any scale is a story of accumulation, storage, transmission and processing of structural information.},
  copyright = {https://www.cambridge.org/core/terms},
  langid = {english},
  file = {/home/elessar/Zotero/storage/3D7LE6PJ/Krivovichev - 2013 - Structural complexity of minerals information storage and processing in the mineral world.pdf}
}

@article{krusnaPredictingMozartsNext,
  title = {Predicting {{Mozart}}'s {{Next Note}} via {{Echo State Networks}}},
  author = {Kru{\v s}na, {\k A}{\v z}uolas and Luko{\v s}evi{\v c}ius, Mantas},
  abstract = {Even though algorithmic music has been around the world since the old days, it has never attracted as many researchers as in the recent years. To our knowledge it existed in Iran back in the Middle Ages and in Europe during the Age of Enlightenment. Though the form has changed and it has grown layers of complexity, the very foundations of the algorithm that generates musical compositions have not changed, i.e. most of them are based on structures of fortuity. Additionally, models that are able to learn have been discovered allowing us to imitate the music of the incredible artists throughout history. The thought alone is crazy to think of and seems to be from the sci-fi. In this paper, a research trying to find the best model of an echo state network in order to mimic the music of the legendary Wolfgang Amadeus Mozart has been carried out. As it turns out, the best models are the ones that rely on long-term dependencies.},
  langid = {english},
  keywords = {No DOI found},
  annotation = {GSCC: 0000003},
  file = {/home/elessar/Zotero/storage/4RZCWSSY/Krušna and Lukoševičius - Predicting Mozart’s Next Note via Echo State Netwo.pdf}
}

@article{kuramotoCoexistenceCoherenceIncoherence2002,
  title = {Coexistence of {{Coherence}} and {{Incoherence}} in {{Nonlocally Coupled Phase Oscillators}}},
  author = {Kuramoto, Y and Battogtokh, D},
  year = {2002},
  volume = {5},
  number = {4},
  abstract = {The phase oscillator model with global coupling is extended to the case of finite-range nonlocal coupling. Under suitable conditions, peculiar patterns emerge in which a quasi-continuous array of identical oscillators separates sharply into two domains, one composed of mutually synchronized oscillators with unique frequency and the other composed of desynchronized oscillators with distributed frequencies. We apply a theory similar to the one which successfully explained the onset of collective synchronization in globally coupled phase oscillators with frequency distribution. A space-dependent order parameter is thus introduced, and an exact functional self-consistency equation is derived for this quantity. Its numerical solution is confirmed to reproduce the simulation results accurately.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/3FRIKG8U/Kuramoto and Battogtokh - 2002 - Coexistence of Coherence and Incoherence in Nonlocally Coupled Phase Oscillators.pdf}
}

@misc{kuramotoDiffusionInducedChaosReaction1978,
  title = {Diffusion-{{Induced Chaos}} in {{Reaction Systems}}},
  shorttitle = {Diffusion-{{Induced Chaos}} in {{Reaction Systems}}},
  author = {Kuramoto, Yoshiki},
  year = {1978},
  month = feb,
  publisher = {Progress of Theoretical Physics Supplement},
  abstract = {A turbulent state in a distributed chemical reaction system is studied theoretically. The chaotic behavior here is basically due to the unstable growth of a spatial inhomogeneity taking place in an oscillating medium. It is argued that phase turbulence and amplitude turbulence have to be discriminated from each other according to their distinct origins. Two prototype equations describing respective types of turbulence are derived by means of some asymptotic methods, and their solutions turn out toe exhibit successive bifurcations. It is found that the phase turbulence arises essentially from the interaction among a few unstable phase modes, while the amplitude turbulence may well appear in the presence of only one unstable mode with is a mixed mode of the phase and amplitude. In particular, some similarity of the amplitude turbulence to the Lorenz chaos is pointed out. Throughout the present paper, the chaotic behavior is discucssed in connection with spatial pattern, so that no discrete approximations, such as a box-model or mode-truncation, are employed.},
  langid = {english},
  annotation = {GSCC: 0000747},
  file = {/home/elessar/Zotero/storage/JERMHPCX/Kuramoto - 1978 - Diffusion-Induced Chaos in Reaction Systems.pdf}
}

@misc{lambProfessorForcingNew2016,
  title = {Professor {{Forcing}}: {{A New Algorithm}} for {{Training Recurrent Networks}}},
  shorttitle = {Professor {{Forcing}}},
  author = {Lamb, Alex and Goyal, Anirudh and Zhang, Ying and Zhang, Saizheng and Courville, Aaron and Bengio, Yoshua},
  year = {2016},
  month = oct,
  number = {arXiv:1610.09038},
  eprint = {1610.09038},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-05-30},
  abstract = {The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network's own one-stepahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {GSCC: 0000731},
  file = {/home/elessar/Zotero/storage/UINJZ5WA/Lamb et al. - 2016 - Professor Forcing A New Algorithm for Training Re.pdf}
}

@article{langeroudiFDLSTMFuzzyLSTM,
  title = {{{FD-LSTM}}: {{A Fuzzy LSTM Model}} for {{Chaotic Time-Series Prediction}}},
  author = {Langeroudi, Milad Keshtkar and Yamaghani, Mohammad Reza and Khodaparast, Siavash},
  journal = {DEEP LEARNING},
  langid = {english},
  annotation = {GSCC: 0000011},
  file = {/home/elessar/Zotero/storage/LHNKVPWD/Langeroudi et al. - FD-LSTM A Fuzzy LSTM Model for Chaotic Time-Serie.pdf}
}

@article{lara-benitezExperimentalReviewDeep2021,
  title = {An {{Experimental Review}} on {{Deep Learning Architectures}} for {{Time Series Forecasting}}},
  author = {{Lara-Ben{\'i}tez}, Pedro and {Carranza-Garc{\'i}a}, Manuel and Riquelme, Jos{\'e} C.},
  year = {2021},
  month = mar,
  journal = {International Journal of Neural Systems},
  volume = {31},
  number = {03},
  eprint = {2103.12057},
  primaryclass = {cs},
  pages = {2130001},
  issn = {0129-0657, 1793-6462},
  doi = {10.1142/S0129065721300011},
  urldate = {2024-01-10},
  abstract = {In recent years, deep learning techniques have outperformed traditional models in many machine learning tasks. Deep neural networks have successfully been applied to address time series forecasting problems, which is a very important topic in data mining. They have proved to be an effective solution given their capacity to automatically learn the temporal dependencies present in time series. However, selecting the most convenient type of deep neural network and its parametrization is a complex task that requires considerable expertise. Therefore, there is a need for deeper studies on the suitability of all existing architectures for different forecasting tasks. In this work, we face two main challenges: a comprehensive review of the latest works using deep learning for time series forecasting; and an experimental study comparing the performance of the most popular architectures. The comparison involves a thorough analysis of seven types of deep learning models in terms of accuracy and efficiency. We evaluate the rankings and distribution of results obtained with the proposed models under many different architecture configurations and training hyperparameters. The datasets used comprise more than 50000 time series divided into 12 different forecasting problems. By training more than 38000 models on these data, we provide the most extensive deep learning study for time series forecasting. Among all studied models, the results show that long short-term memory (LSTM) and convolutional networks (CNN) are the best alternatives, with LSTMs obtaining the most accurate forecasts. CNNs achieve comparable performance with less variability of results under different parameter configurations, while also being more efficient.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {GSCC: 0000419},
  file = {/home/elessar/Zotero/storage/HUH7DXEQ/Lara-Benítez et al_2021_An Experimental Review on Deep Learning Architectures for Time Series.pdf;/home/elessar/Zotero/storage/L7VUHFKD/Lara-Benítez et al. - 2021 - An Experimental Review on Deep Learning Architectu.pdf}
}

@article{leeComputationalComplexityArt,
  title = {Computational {{Complexity}} of {{Art Gallery Problems}}},
  author = {Lee, D T and Lin, Arthurk},
  langid = {english},
  file = {/home/elessar/Zotero/storage/Y833H9B3/Lee and Lin - Computational Complexity of Art Gallery Problems.pdf}
}

@article{leeResourceBoundedSymmetry2005,
  title = {Resource Bounded Symmetry of Information Revisited},
  author = {Lee, Troy and Romashchenko, Andrei},
  year = {2005},
  month = nov,
  journal = {Theoretical Computer Science},
  volume = {345},
  number = {2-3},
  pages = {386--405},
  publisher = {Elsevier BV},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2005.07.017},
  urldate = {2025-07-25},
  abstract = {The information contained in a string x about a string y is the difference between the Kolmogorov complexity of y and the conditional Kolmogorov complexity of y given x, i.e., I (x : y)=C(y)-C(y{\textbar}x). The Kolmogorov--Levin Theorem says that I (x : y) is symmetric up to a small additive term. We investigate if this property also holds for several versions of polynomial time-bounded Kolmogorov complexity.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/Z37GG6Q8/Lee and Romashchenko - 2005 - Resource bounded symmetry of information revisited.pdf}
}

@article{leeVITGANTRAININGGANS2022,
  title = {{{VITGAN}}: {{TRAINING GANS WITH VISION TRANS- FORMERS}}},
  author = {Lee, Kwonjoon and Chang, Huiwen and Jiang, Lu and Zhang, Han and Tu, Zhuowen and Liu, Ce},
  year = {2022},
  abstract = {Recently, Vision Transformers (ViTs) have shown competitive performance on image recognition while requiring less vision-specific inductive biases. In this paper, we investigate if such performance can be extended to image generation. To this end, we integrate the ViT architecture into generative adversarial networks (GANs). For ViT discriminators, we observe that existing regularization methods for GANs interact poorly with self-attention, causing serious instability during training. To resolve this issue, we introduce several novel regularization techniques for training GANs with ViTs. For ViT generators, we examine architectural choices for latent and pixel mapping layers to facilitate convergence. Empirically, our approach, named ViTGAN, achieves comparable performance to the leading CNNbased GAN models on three datasets: CIFAR-10, CelebA, and LSUN bedroom. Our code is available online1.},
  langid = {english},
  keywords = {No DOI found},
  annotation = {GSCC: 0000219},
  file = {/home/elessar/Zotero/storage/JXS82UJL/Lee et al. - 2022 - VITGAN TRAINING GANS WITH VISION TRANS- FORMERS.pdf}
}

@article{legensteinEdgeChaosPrediction2007,
  title = {Edge of Chaos and Prediction of Computational Performance for Neural Circuit Models},
  author = {Legenstein, Robert and Maass, Wolfgang},
  year = {2007},
  month = apr,
  journal = {Neural Networks},
  volume = {20},
  number = {3},
  pages = {323--334},
  issn = {08936080},
  doi = {10.1016/j.neunet.2007.04.017},
  urldate = {2023-05-30},
  abstract = {We analyze in this article the significance of the edge of chaos for real-time computations in neural microcircuit models consisting of spiking neurons and dynamic synapses. We find that the edge of chaos predicts quite well those values of circuit parameters that yield maximal computational performance. But obviously it makes no prediction of their computational performance for other parameter values. Therefore, we propose a new method for predicting the computational performance of neural microcircuit models. The new measure estimates directly the kernel property and the generalization capability of a neural microcircuit. We validate the proposed measure by comparing its prediction with direct evaluations of the computational performance of various neural microcircuit models. The proposed method also allows us to quantify differences in the computational performance and generalization capability of neural circuits in different dynamic regimes (UP- and DOWN-states) that have been demonstrated through intracellular recordings in vivo.},
  langid = {english},
  annotation = {GSCC: 0000544},
  file = {/home/elessar/Zotero/storage/J9GNMA2H/Legenstein and Maass - 2007 - Edge of chaos and prediction of computational perf.pdf}
}

@article{lempelComplexityFiniteSequences1976,
  title = {On the {{Complexity}} of {{Finite Sequences}}},
  author = {Lempel, A. and Ziv, J.},
  year = {1976},
  month = jan,
  journal = {IEEE Transactions on Information Theory},
  volume = {22},
  number = {1},
  pages = {75--81},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/tit.1976.1055501},
  urldate = {2025-07-15},
  abstract = {A new approach to the problem of evaluating the complexity (``randomness'') of finite sequencesis presented. The proposed complexity measure is related to the number of steps in a self-delimiting production process by which a given sequenceis presumed to be generated. It is further related to the number of distinct substrings and the rate of their occurrence along the sequence.The derived properties of the proposed measure are discussed and motivated in conjunction with other wellestablished complexity criteria.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/SCVSXHAK/Lempel and Ziv - 1976 - On the Complexity of Finite Sequences.pdf}
}

@article{lesneEntropyEstimationVery2009,
  title = {Entropy Estimation of Very Short Symbolic Sequences},
  author = {Lesne, Annick and Blanc, Jean-Luc and Pezard, Laurent},
  year = {2009},
  month = apr,
  journal = {Physical Review E},
  volume = {79},
  number = {4},
  publisher = {American Physical Society (APS)},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/physreve.79.046208},
  urldate = {2025-07-15},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/F5KHY2SL/Lesne et al. - 2009 - Entropy estimation of very short symbolic sequences.pdf}
}

@article{liChaoticTimeSeries2012,
  title = {Chaotic {{Time Series Prediction Based}} on a {{Novel Robust Echo State Network}}},
  author = {Li, Decai and Han, Min and Wang, Jun},
  year = {2012},
  month = may,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {23},
  number = {5},
  pages = {787--799},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2012.2188414},
  urldate = {2025-03-07},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  annotation = {GSCC: 0000375},
  file = {/home/elessar/Zotero/storage/N7GJVVWI/Li et al. - 2012 - Chaotic Time Series Prediction Based on a Novel Robust Echo State Network.pdf}
}

@book{lichtenbergRegularChaoticDynamics1992,
  title = {Regular and Chaotic Dynamics},
  author = {Lichtenberg, Allan J. and Lieberman, M. A. and Lichtenberg, Allan J.},
  year = {1992},
  series = {Applied Mathematical Sciences},
  edition = {2nd ed},
  number = {v. 38},
  publisher = {Springer-Verlag},
  address = {New York},
  isbn = {978-0-387-97745-4},
  lccn = {QA1 QA867.5 .A647 vol. 38 1992},
  keywords = {Hamiltonian systems,Nonlinear oscillations,Stochastic processes},
  file = {/home/elessar/Zotero/storage/ZGAZLA7V/Lichtenberg et al. - 1992 - Regular and chaotic dynamics.pdf}
}

@article{liEchoStateNetwork,
  title = {An {{Echo State Network With Improved Topology}} for {{Time Series Prediction}}},
  author = {Li, Xin and Bi, Fengrong and Yang, Xiao and Bi, Xiaoyang},
  abstract = {An echo state network with improved topology (IESN) is proposed for accurate and efficient time series prediction. In this approach, a tighter bound of the echo state property related to the Lipshitz constant of reservoir activation function and the maximum structured singular value of reservoir is firstly researched to run the model at the edge of chaos. A smooth composite reservoir activation function is then designed to enhance the ESN. The exact echo state property bound is solved by computing the Lipshitz constant of the composite function. Finally, a decoupling matrix with eigenvalues distributing uniformly in the complex plane is built as the reservoir for abundant dynamic characteristics. Six classical benchmarks are employed to test the IESN. Besides, combined with amplitude-frequency separation based on the Hilbert transform, the IESN predicts a set of engine vibration signals in knock. Compared with several popular models, the proposed IESN shows the best performance.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/UXRNLMUI/Li et al. - An Echo State Network With Improved Topology for Time Series Prediction.pdf}
}

@article{liGrowingDeepEcho2022,
  title = {Growing Deep Echo State Network with Supervised Learning for Time Series Prediction},
  author = {Li, Ying and Li, Fanjun},
  year = {2022},
  month = oct,
  journal = {Applied Soft Computing},
  volume = {128},
  pages = {109454},
  issn = {15684946},
  doi = {10.1016/j.asoc.2022.109454},
  urldate = {2025-03-12},
  abstract = {Multilayer echo state networks (ESNs) are powerful on learning hierarchical temporal representation. However, how to determine the depth of multilayer ESNs is still an open issue. In this paper, we propose a novel approach to automatically determine the depth of a multilayer ESN, named growing deep ESN (GD-ESN). First, an incremental hierarchical structure is proposed, where the recurrent layers and the pre-trained feedforward layers are alternately added to the network one by one. Then, a control scheme is designed for the growth of the network based on the newly defined averaged mutual information and the full rank criterion. Finally, the proposed GD-ESN is evaluated on both benchmark datasets and real-world applications. The experimental results show the effectiveness of the proposed method.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/8XURYBQX/Li and Li - 2022 - Growing deep echo state network with supervised learning for time series prediction.pdf}
}

@article{liHigherorderGrangerReservoir2024,
  title = {Higher-Order {{Granger}} Reservoir Computing: Simultaneously Achieving Scalable Complex Structures Inference and Accurate Dynamics Prediction},
  shorttitle = {Higher-Order {{Granger}} Reservoir Computing},
  author = {Li, Xin and Zhu, Qunxi and Zhao, Chengli and Duan, Xiaojun and Zhao, Bolin and Zhang, Xue and Ma, Huanfei and Sun, Jie and Lin, Wei},
  year = {2024},
  month = mar,
  journal = {Nature Communications},
  volume = {15},
  number = {1},
  pages = {2506},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-46852-1},
  urldate = {2024-04-30},
  abstract = {Abstract             Recently, machine learning methods, including reservoir computing (RC), have been tremendously successful in predicting complex dynamics in many fields. However, a present challenge lies in pushing for the limit of prediction accuracy while maintaining the low complexity of the model. Here, we design a data-driven, model-free framework named higher-order Granger reservoir computing (HoGRC), which owns two major missions: The first is to infer the higher-order structures incorporating the idea of Granger causality with the RC, and, simultaneously, the second is to realize multi-step prediction by feeding the time series and the inferred higher-order information into HoGRC. We demonstrate the efficacy and robustness of the HoGRC using several representative systems, including the classical chaotic systems, the network dynamical systems, and the UK power grid system. In the era of machine learning and complex systems, we anticipate a broad application of the HoGRC framework in structure inference and dynamics prediction.},
  langid = {english},
  annotation = {GSCC: 0000008},
  file = {/home/elessar/Zotero/storage/XHWDQRZR/Li et al. - 2024 - Higher-order Granger reservoir computing simultan.pdf}
}

@article{liHYPERBANDBANDITBASEDCONFIGURATION2017,
  title = {{{HYPERBAND}}: {{BANDIT-BASED CONFIGURATION EVAL- UATION FOR HYPERPARAMETER OPTIMIZATION}}},
  author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year = {2017},
  abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian Optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation. We present HYPERBAND, a novel algorithm for hyperparameter optimization that is simple, flexible, and theoretically sound. HYPERBAND is a principled early-stoppping method that adaptively allocates a predefined resource, e.g., iterations, data samples or number of features, to randomly sampled configurations. We compare HYPERBAND with popular Bayesian Optimization methods on several hyperparameter optimization problems. We observe that HYPERBAND can provide more than an order of magnitude speedups over competitors on a variety of neural network and kernel-based learning problems.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/37X3YUAK/Li et al. - 2017 - HYPERBAND BANDIT-BASED CONFIGURATION EVAL- UATION FOR HYPERPARAMETER OPTIMIZATION.pdf}
}

@article{limniotisNonlinearComplexityLempel2007,
  title = {On the {{Nonlinear Complexity}} and {{Lempel}}--{{Ziv Complexity}} of {{Finite Length Sequences}}},
  author = {Limniotis, Konstantinos and Kolokotronis, Nicholas and Kalouptsidis, Nicholas},
  year = {2007},
  month = nov,
  journal = {IEEE Transactions on Information Theory},
  volume = {53},
  number = {11},
  pages = {4293--4302},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9448},
  doi = {10.1109/tit.2007.907442},
  urldate = {2025-07-15},
  abstract = {The nonlinear complexity of binary sequences and its connections with Lempel--Ziv complexity is studied in this paper. A new recursive algorithm is presented, which produces the minimal nonlinear feedback shift register of a given binary sequence. Moreover, it is shown that the eigenvalue profile of a sequence uniquely determines its nonlinear complexity profile, thus establishing a connection between Lempel--Ziv complexity and nonlinear complexity. Furthermore, a lower bound for the Lempel--Ziv compression ratio of a given sequence is proved that depends on its nonlinear complexity.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/GZJ2UPXM/Limniotis et al. - 2007 - On the Nonlinear Complexity and Lempel–Ziv Complexity of Finite Length Sequences.pdf}
}

@inproceedings{linderCausalCodingIndividual,
  title = {Causal Coding of Individual Sequences and the Lempel-Ziv Differential Entropy},
  booktitle = {International {{Symposium onInformation Theory}}, 2004. {{ISIT}} 2004. {{Proceedings}}.},
  author = {Linder, T. and Zamir, R.},
  pages = {560--560},
  publisher = {IEEE},
  address = {Chicago, Illinois, USA},
  doi = {10.1109/isit.2004.1365597},
  urldate = {2025-07-15},
  abstract = {In causal source coding, the reconstruction is restricted to be a function of the present and past source samples, while the variable-length code stream may be non-causal. Neuhoff and Gilbert [1] showed that for memoryless sources, optimum performance among all causal lossy source codes is achieved by time-sharing at most two memoryless codes (scalar quantizers) followed by entropy coding. We extend this result to causal coding of individual sequences in the limit of small distortion. The optimum performance of finite-memory variablerate causal codes in this setting is characterized by a deterministic analogue of differential entropy, which we call ``LempelZiv differential entropy.'' As a by-product, we also provide an individual-sequence version of the Shannon lower bound to the rate-distortion function.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/ZZQKICNG/Linder and Zamir - Causal coding of individual sequences and the lempel-ziv differential entropy.pdf}
}

@article{lindgrenComplexityTwoDimensionalPatterns,
  title = {Complexity of {{Two-Dimensional Patterns}}},
  author = {Lindgren, Kristian and Moore, Cristopher and Nordahl, Mats},
  langid = {english},
  file = {/home/elessar/Zotero/storage/IP9IUD7D/Lindgren et al. - Complexity of Two-Dimensional Patterns.pdf}
}

@article{lindgrenCorrelationsRandomInformation,
  title = {Correlations and {{Random Information}} in {{Cellular Automata}}},
  author = {Lindgren, Kristian},
  langid = {english},
  file = {/home/elessar/Zotero/storage/TIWMR9KC/Lindgren - Correlations and Random Information in Cellular Automata.pdf}
}

@article{linfootInformationalMeasureCorrelation1957,
  title = {An Informational Measure of Correlation},
  author = {Linfoot, E.H.},
  year = {1957},
  month = sep,
  journal = {Information and Control},
  volume = {1},
  number = {1},
  pages = {85--89},
  publisher = {Elsevier BV},
  issn = {0019-9958},
  doi = {10.1016/s0019-9958(57)90116-x},
  urldate = {2025-07-15},
  abstract = {Informational considerations lead to a natural generalization of the classical correlation coefficient of a normal distribution. The generalized coefficient, here called the informational coe{\textasciitilde}cient of correlation, is a function of the joint probability density distribution p(x, y) of the two variables x and y, is invariant under a change of parameterization x' = f(x), y' = g(y), and reduces to the classical correlation coefficient when p(x, y) is normal.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/JWMIRSFV/Linfoot - 1957 - An informational measure of correlation.pdf}
}

@article{lingnauDevilsStaircasesContinuous2020,
  title = {Devil's {{Staircases}} in {{Continuous Systems}} with {{Modulated Forcing}}},
  author = {Lingnau, Benjamin and Shortiss, Kevin and Dubois, Fabien and Peters, Frank H. and Kelleher, Bryan},
  year = {2020},
  month = sep,
  journal = {Physical Review E},
  volume = {102},
  number = {3},
  eprint = {1905.01122},
  primaryclass = {physics},
  pages = {030201},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.102.030201},
  urldate = {2025-07-29},
  abstract = {The discrete circle map is the archetypical example of a driven periodic system, showing a complex resonance structure under a change of the forcing frequency known as the devil's staircase. Adler's equation can be seen as the direct continuous equivalent of the circle map, describing locking effects in periodic systems with continuous forcing. This type of locking produces a single fundamental resonance tongue without higher order resonances, and a devil's staircase is not observed. We show that, with harmonically modulated forcing, nonlinear oscillations close to a Hopf bifurcation generically reproduce the devil's staircase even in the continuous case. Experimental results on a semiconductor laser driven by a modulated optical signal show excellent agreement with our theoretical predictions. The locking appears as a modulation of the oscillation amplitude as well as the angular oscillation frequency. Our results show that by proper implementation of an external drive, additional regions of stable frequency locking can be introduced in systems which originally show only a single Adler-type resonance tongue.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Nonlinear Sciences - Chaotic Dynamics,Physics - Optics},
  file = {/home/elessar/Zotero/storage/99USEVUW/Lingnau et al. - 2020 - Devil's Staircases in Continuous Systems with Modulated Forcing.pdf}
}

@article{linotDeepLearningDiscover2020,
  title = {Deep Learning to Discover and Predict Dynamics on an Inertial Manifold},
  author = {Linot, Alec J. and Graham, Michael D.},
  year = {2020},
  month = jun,
  journal = {Physical Review E},
  volume = {101},
  number = {6},
  pages = {062209},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.101.062209},
  urldate = {2023-05-30},
  langid = {english},
  annotation = {GSCC: 0000082},
  file = {/home/elessar/Zotero/storage/5TDUXEBD/Linot and Graham - 2020 - Deep learning to discover and predict dynamics on .pdf}
}

@article{lipowskiSynchronizationPartialSynchronization2005,
  title = {Synchronization and Partial Synchronization of Linear Maps},
  author = {Lipowski, Adam and Droz, Michel},
  year = {2005},
  month = mar,
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {347},
  eprint = {cond-mat/0312067},
  pages = {38--50},
  issn = {0378-4371},
  doi = {10.1016/j.physa.2004.09.047},
  urldate = {2025-07-25},
  abstract = {We study synchronization of low-dimensional (\$d=2,3,4\$) chaotic piecewise linear maps. For Bernoulli maps we find Lyapunov exponents and locate the synchronization transition, that numerically is found to be discontinuous (despite continuously vanishing Lyapunov exponent(s)). For tent maps, a limit of stability of the synchronized state is used to locate the synchronization transition that numerically is found to be continuous. For nonidentical tent maps at the partial synchronization transition, the probability distribution of the synchronization error is shown to develop highly singular behavior. We suggest that for nonidentical Bernoulli maps (and perhaps some other discontinuous maps) partial synchronization is merely a smooth crossover rather than a well defined transition. More subtle analysis in the \$d=4\$ case locates the point where the synchronized state becomes stable. In some cases, however, a riddled basin attractor appears, and synchronized and chaotic behaviors coexist. We also suggest that similar riddling of a basin of attractor might take place in some extended systems where it is known as stable chaos.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Statistical Mechanics,Nonlinear Sciences - Chaotic Dynamics},
  file = {/home/elessar/Zotero/storage/T46X92I8/Lipowski and Droz - 2005 - Synchronization and partial synchronization of linear maps.pdf}
}

@article{liPredictingChaoticTime2022,
  title = {Predicting Chaotic Time Series and Replicating Chaotic Attractors Based on Two Novel Echo State Network Models},
  author = {Li, Yuting and Li, Yong},
  year = {2022},
  month = jun,
  journal = {Neurocomputing},
  volume = {491},
  pages = {321--332},
  issn = {09252312},
  doi = {10.1016/j.neucom.2022.03.054},
  urldate = {2025-03-12},
  abstract = {Neural network is the inevitable outcome of the rapid development of artificial intelligence. Based on the idea of homotopy and combined activation function, two novel echo state network (ESN) models are proposed. Compared with several activation functions commonly used in the neural network, the proposed models provide intuitive but effective approaches to chaotic forecasting, and the prediction accuracy is higher. Secondly, for the Mackey-Glass (MG) time series and R{\"o}ssler attractor, the prediction errors and prediction step sizes of the two novel models are superior to many pre-existing ESN models, which demonstrates the merit of the proposed models. Moreover, several parameters play key roles in network training, such as spectral radius, sparse degree etc., and their effects on network performance are analyzed. Notably, it is also investigated that the trained network can replicate R{\"o}ssler chaotic attractor well. At the end of the results, the parameters of the proposed models are optimized, and the relatively optimized parameters are obtained after a large number of data experiments.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/UY8A3HQV/Li and Li - 2022 - Predicting chaotic time series and replicating chaotic attractors based on two novel echo state netw.pdf}
}

@article{liSharpeningOccamsRazor2003,
  title = {Sharpening {{Occam}}'s Razor},
  author = {Li, Ming and Tromp, John and Vit{\'a}nyi, Paul},
  year = {2003},
  month = mar,
  journal = {Information Processing Letters},
  volume = {85},
  number = {5},
  pages = {267--274},
  publisher = {Elsevier BV},
  issn = {0020-0190},
  doi = {10.1016/s0020-0190(02)00427-1},
  urldate = {2025-07-25},
  abstract = {We provide a new representation-independent formulation of Occam's razor theorem, based on Kolmogorov complexity. This new formulation allows us to: (i) obtain better sample complexity than both length-based [Blumer et al., Inform. Process. Lett. 24 (1987) 377--380] and VC-based [Blumer et al., J. ACM 35 (1989) 929--965] versions of Occam's razor theorem, in many applications; and (ii) achieve a sharper reverse of Occam's razor theorem than that of Board and Pitt [STOC, 1999, pp. 54--63]. Specifically, we weaken the assumptions made by Board and Pitt [STOC, 1999, pp. 54--63] and extend the reverse to superpolynomial running times.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/H6VNA7GE/Li et al. - 2003 - Sharpening Occam's razor.pdf}
}

@article{liStatisticalPropertiesFinite1994,
  title = {Statistical Properties of Finite Sequences with High {{Kolmogorov}} Complexity},
  author = {Li, Ming and Vit{\dbend}nyi, Paul M. B.},
  year = {1994},
  month = jul,
  journal = {Mathematical Systems Theory},
  volume = {27},
  number = {4},
  pages = {365--376},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {0025-5661, 1433-0490},
  doi = {10.1007/bf01192146},
  urldate = {2025-07-25},
  abstract = {We investigate to what extent finite binary sequences with high Kolmogorov complexity are normal (all blocks of equal length occur equally frequently), and the maximal length of all-zero or all-one runs which occur with certainty.},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/home/elessar/Zotero/storage/MR6MUGEQ/Li and Vit�nyi - 1994 - Statistical properties of finite sequences with high Kolmogorov complexity.pdf}
}

@inproceedings{liTwoDecadesApplied1988,
  title = {Two Decades of Applied {{Kolmogorov}} Complexity: In Memoriam {{Andrei Nikolaevich Kolmogorov}} 1903-87},
  shorttitle = {Two Decades of Applied {{Kolmogorov}} Complexity},
  booktitle = {[1988] {{Proceedings}}. {{Structure}} in {{Complexity Theory Third Annual Conference}}},
  author = {Li, M. and Vitanyi, P.M.B.},
  year = {1988},
  pages = {80--101},
  publisher = {IEEE},
  address = {Washington, DC, USA},
  doi = {10.1109/sct.1988.5265},
  urldate = {2025-07-25},
  langid = {english},
  file = {/home/elessar/Zotero/storage/MMEZTC5C/Li and Vitanyi - 1988 - Two decades of applied Kolmogorov complexity in memoriam Andrei Nikolaevich Kolmogorov 1903-87.pdf}
}

@article{liviDeterminationEdgeCriticality2018,
  title = {Determination of the {{Edge}} of {{Criticality}} in {{Echo State Networks Through Fisher Information Maximization}}},
  author = {Livi, Lorenzo and Bianchi, Filippo Maria and Alippi, Cesare},
  year = {2018},
  month = mar,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {29},
  number = {3},
  pages = {706--717},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2016.2644268},
  urldate = {2025-08-12},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}
}

@article{lokseTrainingEchoState2017,
  title = {Training {{Echo State Networks}} with {{Regularization Through Dimensionality Reduction}}},
  author = {L{\o}kse, Sigurd and Bianchi, Filippo Maria and Jenssen, Robert},
  year = {2017},
  month = jun,
  journal = {Cognitive Computation},
  volume = {9},
  number = {3},
  pages = {364--378},
  issn = {1866-9956, 1866-9964},
  doi = {10.1007/s12559-017-9450-z},
  urldate = {2023-05-30},
  abstract = {In this paper we introduce a new framework to train an Echo State Network to predict real valued time-series. The method consists in projecting the output of the internal layer of the network on a space with lower dimensionality, before training the output layer to learn the target task. Notably, we enforce a regularization constraint that leads to better generalization capabilities. We evaluate the performances of our approach on several benchmark tests, using different techniques to train the readout of the network, achieving superior predictive performance when using the proposed framework. Finally, we provide an insight on the effectiveness of the implemented mechanics through a visualization of the trajectory in the phase space and relying on the methodologies of nonlinear time-series analysis. By applying our method on well known chaotic systems, we provide evidence that the lower dimensional embedding retains the dynamical properties of the underlying system better than the full-dimensional internal states of the network.},
  langid = {english},
  annotation = {GSCC: 0000074},
  file = {/home/elessar/Zotero/storage/4AJT55UM/Løkse et al. - 2017 - Training Echo State Networks with Regularization T.pdf}
}

@article{lopez-ortizExploringDeepEcho2024,
  title = {Exploring Deep Echo State Networks for Image Classification: A Multi-Reservoir Approach},
  shorttitle = {Exploring Deep Echo State Networks for Image Classification},
  author = {{L{\'o}pez-Ortiz}, E. J. and {Perea-Trigo}, M. and {Soria-Morillo}, L. M. and {Sancho-Caparrini}, F. and {Vegas-Olmos}, J. J.},
  year = {2024},
  month = jul,
  journal = {Neural Computing and Applications},
  volume = {36},
  number = {20},
  pages = {11901--11918},
  issn = {1433-3058},
  doi = {10.1007/s00521-024-09656-4},
  urldate = {2024-10-21},
  abstract = {Echo state networks (ESNs) belong to the class of recurrent neural networks and have demonstrated robust performance in time series prediction tasks. In this study, we investigate the capability of different ESN architectures to capture spatial relationships in images without transforming them into temporal sequences. We begin with three pre-existing ESN-based architectures and enhance their design by incorporating multiple output layers, customising them for a classification task. Our investigation involves an examination of the behaviour of these modified networks, coupled with a comprehensive performance comparison against the baseline vanilla ESN architecture. Our experiments on the MNIST data set reveal that a network with multiple independent reservoirs working in parallel outperforms other ESN-based architectures for this task, achieving a classification accuracy of 98.43\%. This improvement on the classical ESN architecture is accompanied by reduced training times. While the accuracy of ESN-based architectures lags behind that of convolutional neural network-based architectures, the significantly lower training times of ESNs with multiple reservoirs operating in parallel make them a compelling choice for learning spatial relationships in scenarios prioritising energy efficiency and rapid training. This multi-reservoir ESN architecture overcomes standard ESN limitations regarding memory requirements and training times for large networks, providing more accurate predictions than other ESN-based models. These findings contribute to a deeper understanding of the potential of ESNs as a tool for image classification.},
  langid = {english},
  keywords = {Artificial Intelligence,ESN,GNN,Image classification,MNIST},
  file = {/home/elessar/Zotero/storage/3NI9ZY79/López-Ortiz et al. - 2024 - Exploring deep echo state networks for image classification a multi-reservoir approach.pdf}
}

@article{lopezTeoriaInformacionCodificacion,
  title = {{Teor{\'i}a de la Informaci{\'o}n y Codificaci{\'o}n}},
  author = {L{\'o}pez, C{\'a}ndido and Veiga, Manuel},
  langid = {galician},
  file = {/home/elessar/Zotero/storage/ZIMJCGV5/López and Veiga - Teoría de la Información y Codiﬁcación.pdf}
}

@misc{loshchilovDecoupledWeightDecay2019,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2019},
  month = jan,
  number = {arXiv:1711.05101},
  eprint = {1711.05101},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-10-21},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by {\textbackslash}emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/home/elessar/Zotero/storage/YH9V88MW/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf}
}

@article{louchardAverageProfileLimiting1995,
  title = {Average Profile and Limiting Distribution for a Phrase Size in the {{Lempel-Ziv}} Parsing Algorithm},
  author = {Louchard, G. and Szpankowski, W.},
  year = {1995},
  month = mar,
  journal = {IEEE Transactions on Information Theory},
  volume = {41},
  number = {2},
  pages = {478--488},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9448},
  doi = {10.1109/18.370149},
  urldate = {2025-07-15},
  abstract = {Consider the parsing algorithm developed by Lem pel and Ziv that partitions a sequence of leugth n into variable phrases (blocks) such that a new block is the shortest substring not seen in the past as a phrase. In practice, the following parameters are of interest: number of phrases, the size of a phrase, the numher of phrases of given size, and so forth. In this paper, we focus on the size of a randomly selected phrase, and the average nwnber of phrases of a given size (the 50called average profile of phrase sizes). These parameters can be efficiently analyzed through a digital search tree representation. For a memoryless source with unequal probabilities of symbols generation (the so-called asymmetric Bernoulli model), we prove that the size of a typical phrase is asymptotically normally distributed with mean and variance explicitly computed. In terms of digital search trees, we prove the normal limiting distribution of the typical depth (i.e" the length of a path from the root to a randomly selected node). The latter finding is proved by a technique that helongs to the toolkit of the "analytical analysis of algorithms," and it seems to be novel in the context of data compression.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/8X62X8DL/Louchard and Szpankowski - 1995 - Average profile and limiting distribution for a phrase size in the Lempel-Ziv parsing algorithm.pdf}
}

@article{lukoseviciusEchoStateNetworks,
  title = {Echo {{State Networks}} with {{Trained Feedbacks}}},
  author = {Luko{\v s}evi{\v c}ius, Mantas},
  abstract = {Echo State Networks (ESNs) is an approach to the recurrent neural network (RNN) training, based on generating a big random network (reservoir) of sparsely interconnected neurons and learning only a single layer of output weights from the reservoir as the target function. Despite many advantages of ESNs over gradient based RNN training techniques, they lack the power of learning some complex functions. New findings in dynamical systems theory state, that fixed neural circuits can obtain universal computational qualities if suitable feedbacks (or intermediate units) can be trained. Unfortunately the theory gives no hint on how this can be done. In this report we explore possible directions in which the theoretical findings could be applied to increase the computational power of ESNs. More specifically, we discuss possible options for defining training targets for the feedbacks, present and discuss some empirical results (positive as well as negative) testing the ideas in practice and analyze some problems pointing out to some intrinsic limitations of ESNs. Another contribution of this report is a discussion of many practical issues of training ESNs in particular the ones having feedback connections. We also propose a modification of ESNs called Layered ESNs. This technical report is based on the author's Master Thesis named ``Improving Echo State Networks by Training Intermediate Units''.},
  langid = {english},
  annotation = {GSCC: 0000035},
  file = {/home/elessar/Zotero/storage/GBJCQPPR/Lukoˇseviˇcius - Echo State Networks with Trained Feedbacks.pdf}
}

@incollection{lukoseviciusEfficientCrossValidationEcho2019,
  title = {Efficient {{Cross-Validation}} of {{Echo State Networks}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} -- {{ICANN}} 2019: {{Workshop}} and {{Special Sessions}}},
  author = {Luko{\v s}evi{\v c}ius, Mantas and Uselis, Arnas},
  editor = {Tetko, Igor V. and K{\r u}rkov{\'a}, V{\v e}ra and Karpov, Pavel and Theis, Fabian},
  year = {2019},
  volume = {11731},
  pages = {121--133},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-30493-5_12},
  urldate = {2023-05-30},
  abstract = {Echo State Networks (ESNs) are known for their fast and precise one-shot learning of time series. But they often need good hyperparameter tuning for best performance. For this good validation is key, but usually, a single validation split is used. In this rather practical contribution we suggest several schemes for cross-validating ESNs and introduce an efficient algorithm for implementing them. The component that dominates the time complexity of the already quite fast ESN training remains constant (does not scale up with k) in our proposed method of doing k-fold cross-validation. The component that does scale linearly with k starts dominating only in some not very common situations. Thus in many situations k-fold cross-validation of ESNs can be done for virtually the same time complexity as a simple single split validation. Space complexity can also remain the same. We also discuss when the proposed validation schemes for ESNs could be beneficial and empirically investigate them on several different real-world datasets.},
  isbn = {978-3-030-30492-8 978-3-030-30493-5},
  langid = {english},
  annotation = {GSCC: 0000022},
  file = {/home/elessar/Zotero/storage/FTA5NMNW/Lukoševičius and Uselis - 2019 - Efficient Cross-Validation of Echo State Networks.pdf}
}

@article{lukoseviciusEfficientImplementationsEcho,
  title = {Efficient Implementations of Echo State Network Cross-Validation},
  author = {Luko{\v s}evi{\v c}ius, Mantas and Uselis, Arnas},
  abstract = {Background/introduction: Cross-Validation (CV) is still uncommon in time series modeling. Echo State Networks (ESNs), as a prime example of Reservoir Computing (RC) models, are known for their fast and precise one-shot learning, that often benefit from good hyper-parameter tuning. This makes them ideal to change the status quo. Methods: We discuss CV of time series for predicting a concrete time interval of interest, suggest several schemes for cross-validating ESNs and introduce an efficient algorithm for implementing them. This algorithm is presented as two levels of optimizations of doing k-fold CV. Training an RC model typically consists of two stages: (i) running the reservoir with the data and (ii) computing the optimal readouts. The first level of our optimization addresses the most computationally expensive part (i) and makes it remain constant irrespective of k. It dramatically reduces reservoir computations in any type of RC system and is enough if k is small. The second level of optimization also makes the (ii) part remain constant irrespective of large k, as long as the dimension of the output is low. We discuss when the proposed validation schemes for ESNs could be beneficial, three options for producing the final model and empirically investigate them on six different real-world datasets, as well as do empirical computation time experiments. We provide the code in an online repository. Results: Proposed CV schemes give better and more stable test performance in all the six different real-world datasets, three task types. Empirical run times confirm our complexity analysis. Conclusions: In most situations, k-fold CV of ESNs and many other RC models can be done for virtually the same time and space complexity as a simple single-split validation. This enables CV to become a standard practice in RC.},
  langid = {english},
  annotation = {GSCC: 0000023},
  file = {/home/elessar/Zotero/storage/JJIIL3TY/Lukosevicius and Uselis - Efficient implementations of echo state network cr.pdf}
}

@incollection{lukoseviciusPracticalGuideApplying2012,
  title = {A {{Practical Guide}} to {{Applying Echo State Networks}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  author = {Luko{\v s}evi{\v c}ius, Mantas},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  volume = {7700},
  pages = {659--686},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-35289-8_36},
  urldate = {2023-05-30},
  abstract = {Reservoir computing has emerged in the last decade as an alternative to gradient descent methods for training recurrent neural networks. Echo State Network (ESN) is one of the key reservoir computing ``flavors''. While being practical, conceptually simple, and easy to implement, ESNs require some experience and insight to achieve the hailed good performance in many tasks. Here we present practical techniques and recommendations for successfully applying ESNs, as well as some more advanced application-specific modifications.},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  langid = {english},
  annotation = {GSCC: 0001044},
  file = {/home/elessar/Zotero/storage/V5M22ANR/Lukoševičius - 2012 - A Practical Guide to Applying Echo State Networks.pdf}
}

@article{lukoseviciusReservoirComputingSelfOrganized,
  title = {Reservoir {{Computing}} and {{Self-Organized Neural Hierarchies}}},
  author = {Lukosevicius, Mantas},
  langid = {english},
  annotation = {GSCC: 0000035},
  file = {/home/elessar/Zotero/storage/Y2KZWUZF/Lukosevicius - Reservoir Computing and Self-Organized Neural Hier.pdf}
}

@incollection{lukoseviciusSelforganizedReservoirsTheir2012,
  title = {Self-Organized {{Reservoirs}} and {{Their Hierarchies}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} -- {{ICANN}} 2012},
  author = {Luko{\v s}evi{\v c}ius, Mantas},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Villa, Alessandro E. P. and Duch, W{\l}odzis{\l}aw and {\'E}rdi, P{\'e}ter and Masulli, Francesco and Palm, G{\"u}nther},
  year = {2012},
  volume = {7552},
  pages = {587--595},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-33269-2_74},
  urldate = {2023-05-30},
  abstract = {We investigate how unsupervised training of recurrent neural networks (RNNs) and their deep hierarchies can benefit a supervised task like temporal pattern detection. The RNNs are fully and fast trained by unsupervised algorithms and only supervised feed-forward readouts are used. The unsupervised RNNs are shown to perform better in a rigorous comparison against state-of-art random reservoir networks. Unsupervised greedy bottom-up trained hierarchies of such RNNs are shown being capable of big performance improvements over single layer setups.},
  isbn = {978-3-642-33268-5 978-3-642-33269-2},
  langid = {english},
  annotation = {GSCC: 0000035},
  file = {/home/elessar/Zotero/storage/CMLNWZFH/Lukoševičius - 2012 - Self-organized Reservoirs and Their Hierarchies.pdf}
}

@article{lukoseviciusTimeAdaptiveRecurrentNeural,
  title = {Time-{{Adaptive Recurrent Neural Networks}}},
  author = {Lukosevicius, Mantas and Uselis, Arnas},
  abstract = {Data are often sampled irregularly in time. Dealing with this using Recurrent Neural Networks (RNNs) traditionally involved ignoring the fact, feeding the time differences as additional inputs, or resampling the data. All these methods have their shortcomings. We propose an elegant alternative approach where instead the RNN is in effect resampled in time to match the time of the data. We use Echo State Network (ESN) and Gated Recurrent Unit (GRU) as the basis for our solution. Such RNNs can be seen as discretizations of continuous-time dynamical systems, which gives a solid theoretical ground for our approach. Similar recent observations have been made in feed-forward neural networks as neural ordinary differential equations. Our Time-Adaptive ESN (TAESN) and GRU (TAGRU) models allow for a direct model time setting and require no additional training, parameter tuning, or computation compared to the regular counterparts, thus retaining their original efficiency. We confirm empirically that our models can effectively compensate for the time-non-uniformity of the data and demonstrate that they compare favorably to data resampling, classical RNN methods, and alternative RNN models proposed to deal with time irregularities on several real-world nonuniform-time datasets.},
  langid = {english},
  annotation = {GSCC: 0000003},
  file = {/home/elessar/Zotero/storage/AYZMV9HP/Lukosevicius and Uselis - Time-Adaptive Recurrent Neural Networks.pdf}
}

@article{lunModifiedSufficientConditions2019,
  title = {The Modified Sufficient Conditions for Echo State Property and Parameter Optimization of Leaky Integrator Echo State Network},
  author = {Lun, Shu-xian and Hu, Hai-feng and Yao, Xian-shuang},
  year = {2019},
  month = apr,
  journal = {Applied Soft Computing},
  volume = {77},
  pages = {750--760},
  issn = {15684946},
  doi = {10.1016/j.asoc.2019.02.005},
  urldate = {2025-03-12},
  abstract = {Leaky integrator echo state network (Leaky-ESN), an improved echo state network, is suitable for learning very slow dynamic systems and replaying the learnt system at different speeds. To ensure the echo state property of Leaky-ESN, spectral radius of reservoir connection weight matrix needs to be less than leaking rate, which means that the output feedback of Leaky-ESN is actually ignored. Leaky-ESN without output feedback is equivalent to being a feedforward type neural network. Therefore, in this paper, we consider Leaky-ESN model with output feedback. Firstly, we give the modified sufficient conditions for echo state property, which are actually some inequality constraints about the control parameters (mainly leaking rate, spectral radius of reservoir connection weight matrix and the scaling of output feedback) and the maximum singular values of feedback and output connection weight matrices. Secondly, to reduce the influence of the initial values of the control parameters on Leaky-ESN, we use the barrier method to optimize the control parameters. The barrier method can convert the constrained optimization problem into the unconstrained optimization problem. Newton's method and its improve method, for example, eigenvalue modification methods, are further used to solve the unconstrained optimization problem and then obtain the control parameters of Leaky-ESN. Finally, Mackey--Glass chaotic time series prediction is selected to validate the method proposed in this paper. Simulation results show that the method proposed in this paper can make Leaky-ESN model more stable and higher approximation precision.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/GHMPW5PU/Lun et al. - 2019 - The modified sufficient conditions for echo state property and parameter optimization of leaky integ.pdf}
}

@article{luqueIntroduccionDinamicaAplicaciones,
  title = {{Introduccio{\textasciiacute}n a la dina{\textasciiacute}mica de aplicaciones del c{\textasciiacute}{\i}rculo y problemas relacionados}},
  author = {Luque, Alejandro},
  langid = {spanish},
  keywords = {No DOI found},
  annotation = {GSCC: 0000002},
  file = {/home/elessar/Zotero/storage/3GG3ZBE3/Luque - Introduccio´n a la dina´mica de aplicaciones del c.pdf}
}

@article{luReservoirObserversModelfree2017,
  title = {Reservoir Observers: {{Model-free}} Inference of Unmeasured Variables in Chaotic Systems},
  shorttitle = {Reservoir Observers},
  author = {Lu, Zhixin and Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Brockett, Roger and Ott, Edward},
  year = {2017},
  month = apr,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {27},
  number = {4},
  pages = {041102},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.4979665},
  urldate = {2025-03-17},
  abstract = {Deducing the state of a dynamical system as a function of time from a limited number of concurrent system state measurements is an important problem of great practical utility. A scheme that accomplishes this is called an ``observer.'' We consider the case in which a model of the system is unavailable or insufficiently accurate, but ``training'' time series data of the desired state variables are available for a short period of time, and a limited number of other system variables are continually measured. We propose a solution to this problem using networks of neuron-like units known as ``reservoir computers.'' The measurements that are continually available are input to the network, which is trained with the limited-time data to output estimates of the desired state variables. We demonstrate our method, which we call a ``reservoir observer,'' using the R{\"o}ssler system, the Lorenz system, and the spatiotemporally chaotic Kuramoto--Sivashinsky equation. Subject to the condition of observability (i.e., whether it is in principle possible, by any means, to infer the desired unmeasured variables from the measured variables), we show that the reservoir observer can be a very effective and versatile tool for robustly reconstructing unmeasured dynamical system variables.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/8F56F54A/Lu et al. - 2017 - Reservoir observers Model-free inference of unmeasured variables in chaotic systems.pdf;/home/elessar/Zotero/storage/N782QIKV/Model-free inference of unmeasured variables in chaotic systems.pdf}
}

@article{lutzDimensionsIndividualStrings2003,
  title = {The Dimensions of Individual Strings and Sequences},
  author = {Lutz, Jack H.},
  year = {2003},
  month = nov,
  journal = {Information and Computation},
  volume = {187},
  number = {1},
  pages = {49--79},
  publisher = {Elsevier BV},
  issn = {0890-5401},
  doi = {10.1016/s0890-5401(03)00187-1},
  urldate = {2025-07-25},
  abstract = {A constructive version of Hausdorff dimension is developed using constructive supergales, which are betting strategies that generalize the constructive supermartingales used in the theory of individual random sequences. This constructive dimension is used to assign every individual (infinite, binary) sequence S a dimension, which is a real number dim(S) in the interval [0, 1]. Sequences that are random (in the sense of Martin-L{\"o}f) have dimension 1, while sequences that are decidable, 10, or 01, have dimension 0. It is shown that for every 02-computable real number {$\alpha$} in [0, 1] there is a 02 sequence S such that dim(S) = {$\alpha$}. A discrete version of constructive dimension is also developed using termgales, which are supergale-like functions that bet on the terminations of (finite, binary) strings as well as on their successive bits. This discrete dimension is used to assign each individual string w a dimension, which is a nonnegative real number dim(w). The dimension of a sequence is shown to be the limit inferior of the dimensions of its prefixes. The Kolmogorov complexity of a string is proven to be the product of its length and its dimension. This gives a new characterization of algorithmic information and a new proof of Mayordomo's recent theorem stating that the dimension of a sequence is the limit inferior of the average Kolmogorov complexity of its first n bits. Every sequence that is random relative to any computable sequence of coin-toss biases that converge to a real number {$\beta$} in (0, 1) is shown to have dimension H({$\beta$}), the binary entropy of {$\beta$}.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/HAVTICHC/Lutz - 2003 - The dimensions of individual strings and sequences.pdf}
}

@inbook{maassLiquidStateMachines2011,
  title = {Liquid {{State Machines}}: {{Motivation}}, {{Theory}}, and {{Applications}}},
  shorttitle = {Liquid {{State Machines}}},
  booktitle = {Computability in {{Context}}},
  author = {Maass, Wolfgang},
  year = {2011},
  month = feb,
  pages = {275--296},
  publisher = {IMPERIAL COLLEGE PRESS},
  doi = {10.1142/9781848162778_0008},
  urldate = {2023-05-30},
  collaborator = {Cooper, S Barry and Sorbi, Andrea},
  isbn = {978-1-84816-245-7 978-1-84816-277-8},
  langid = {english},
  annotation = {GSCC: 0000263},
  file = {/home/elessar/Zotero/storage/3I6BFKBI/Maass - 2011 - Liquid State Machines Motivation, Theory, and App.pdf}
}

@article{maassRealTimeComputingStable2002,
  title = {Real-{{Time Computing Without Stable States}}: {{A New Framework}} for {{Neural Computation Based}} on {{Perturbations}}},
  shorttitle = {Real-{{Time Computing Without Stable States}}},
  author = {Maass, Wolfgang and Natschl{\"a}ger, Thomas and Markram, Henry},
  year = {2002},
  month = nov,
  journal = {Neural Computation},
  volume = {14},
  number = {11},
  pages = {2531--2560},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976602760407955},
  urldate = {2023-05-30},
  abstract = {A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.},
  langid = {english},
  annotation = {GSCC: 0004501},
  file = {/home/elessar/Zotero/storage/QMEV8ALM/Maass et al. - 2002 - Real-Time Computing Without Stable States A New F.pdf}
}

@inproceedings{maatEfficientOptimizationEcho2018,
  title = {Efficient {{Optimization}} of {{Echo State Networks}} for {{Time Series Datasets}}},
  booktitle = {2018 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Maat, Jacob Reinier and Gianniotis, Nikos and Protopapas, Pavlos},
  year = {2018},
  month = jul,
  pages = {1--7},
  publisher = {IEEE},
  address = {Rio de Janeiro},
  doi = {10.1109/IJCNN.2018.8489094},
  urldate = {2023-05-30},
  abstract = {Echo State Networks (ESNs) are recurrent neural networks that only train their output layer, thereby precluding the need to backpropagate gradients through time, and leading to significant computational gains. Nevertheless, a common issue in ESNs is determining its hyperparameters, which are crucial in instantiating a well performing reservoir, but are often set manually or using heuristics. In this work we optimize the ESN hyperparameters using Bayesian optimization which, given a limited budget of function evaluations, outperforms a grid search strategy. In the context of large volumes of time series data, such as light curves in the field of astronomy, we can further reduce the optimization cost of ESNs. In particular, we wish to avoid tuning hyperparameters per time series: we want to find ESNs with hyperparameters that perform well not just over individual time series but rather on families of time series without sacrificing predictive performance significantly. This naturally leads to a notion of clusters, where each cluster is represented by an ESN tuned to model a group of time series of similar temporal behavior. We demonstrate this approach on both synthetic datasets as well as real world light curves from the MACHO survey. We show a significant reduction in the number of ESN models required to model a whole dataset while retaining most predictive performance.},
  isbn = {978-1-5090-6014-6},
  langid = {english},
  annotation = {GSCC: 0000217},
  file = {/home/elessar/Zotero/storage/F43KWLWE/Maat et al_2018_Efficient Optimization of Echo State Networks for Time Series Datasets.pdf;/home/elessar/Zotero/storage/V3DYVT9E/Maat et al. - 2018 - Efficient Optimization of Echo State Networks for .pdf}
}

@article{machtaNaturalComplexityComputational2011,
  title = {Natural Complexity, Computational Complexity and Depth},
  author = {Machta, J.},
  year = {2011},
  month = sep,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {21},
  number = {3},
  publisher = {AIP Publishing},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.3634009},
  urldate = {2025-07-25},
  abstract = {Depth is a complexity measure for natural systems of the kind studied in statistical physics and is defined in terms of computational complexity. Depth quantifies the length of the shortest parallel computation required to construct a typical system state or history starting from simple initial conditions. The properties of depth are discussed and it is compared with other complexity measures. Depth can only be large for systems with embedded computation.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/93JMK6JR/Machta - 2011 - Natural complexity, computational complexity and depth.pdf}
}

@misc{maDeepESNMultipleProjectionencoding2017,
  title = {Deep-{{ESN}}: {{A Multiple Projection-encoding Hierarchical Reservoir Computing Framework}}},
  shorttitle = {Deep-{{ESN}}},
  author = {Ma, Qianli and Shen, Lifeng and Cottrell, Garrison W.},
  year = {2017},
  month = nov,
  number = {arXiv:1711.05255},
  eprint = {1711.05255},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1711.05255},
  urldate = {2025-03-07},
  abstract = {As an efficient recurrent neural network (RNN) model, reservoir computing (RC) models, such as Echo State Networks, have attracted widespread attention in the last decade. However, while they have had great success with time series data [1], [2], many time series have a multiscale structure, which a single-hidden-layer RC model may have difficulty capturing. In this paper, we propose a novel hierarchical reservoir computing framework we call Deep Echo State Networks (Deep-ESNs). The most distinctive feature of a Deep-ESN is its ability to deal with time series through hierarchical projections. Specifically, when an input time series is projected into the high-dimensional echo-state space of a reservoir, a subsequent encoding layer (e.g., a PCA, autoencoder, or a random projection) can project the echo-state representations into a lower-dimensional space. These low-dimensional representations can then be processed by another ESN. By using projection layers and encoding layers alternately in the hierarchical framework, a Deep-ESN can not only attenuate the effects of the collinearity problem in ESNs, but also fully take advantage of the temporal kernel property of ESNs to explore multiscale dynamics of time series. To fuse the multiscale representations obtained by each reservoir, we add connections from each encoding layer to the last output layer. Theoretical analyses prove that stability of a Deep-ESN is guaranteed by the echo state property (ESP), and the time complexity is equivalent to a conventional ESN. Experimental results on some artificial and real world time series demonstrate that Deep-ESNs can capture multiscale dynamics, and outperform both standard ESNs and previous hierarchical ESN-based models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {GSCC: 0000051},
  file = {/home/elessar/Zotero/storage/2HQ3WXTE/Ma et al. - 2017 - Deep-ESN A Multiple Projection-encoding Hierarchical Reservoir Computing Framework.pdf;/home/elessar/Zotero/storage/LAIAXGDI/1711.html}
}

@article{maEfficientForecastingChaotic2023,
  title = {Efficient Forecasting of Chaotic Systems with Block-Diagonal and Binary Reservoir Computing},
  author = {Ma, Haochun and Prosperino, Davide and Haluszczynski, Alexander and R{\"a}th, Christoph},
  year = {2023},
  month = jun,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {33},
  number = {6},
  pages = {063130},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0151290},
  urldate = {2025-07-28},
  abstract = {The prediction of complex nonlinear dynamical systems with the help of machine learning has become increasingly popular in different areas of science. In particular, reservoir computers, also known as echo-state networks, turned out to be a very powerful approach, especially for the reproduction of nonlinear systems. The reservoir, the key component of this method, is usually constructed as a sparse, random network that serves as a memory for the system. In this work, we introduce block-diagonal reservoirs, which implies that a reservoir can be composed of multiple smaller reservoirs, each with its own dynamics. Furthermore, we take out the randomness of the reservoir by using matrices of ones for the individual blocks. This breaks with the widespread interpretation of the reservoir as a single network. In the example of the Lorenz and Halvorsen systems, we analyze the performance of block-diagonal reservoirs and their sensitivity to hyperparameters. We find that the performance is comparable to sparse random networks and discuss the implications with regard to scalability, explainability, and hardware realizations of reservoir computers.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/LIZN752H/Ma et al. - 2023 - Efficient forecasting of chaotic systems with block-diagonal and binary reservoir computing.pdf;/home/elessar/Zotero/storage/LQUWVN9M/Ma et al. - 2023 - Efficient forecasting of chaotic systems with block-diagonal and binary reservoir computing.pdf}
}

@article{malikMultilayeredEchoState2017,
  title = {Multilayered {{Echo State Machine}}: {{A Novel Architecture}} and {{Algorithm}}},
  shorttitle = {Multilayered {{Echo State Machine}}},
  author = {Malik, Zeeshan Khawar and Hussain, Amir and Wu, Qingming Jonathan},
  year = {2017},
  month = apr,
  journal = {IEEE Transactions on Cybernetics},
  volume = {47},
  number = {4},
  pages = {946--959},
  issn = {2168-2267, 2168-2275},
  doi = {10.1109/TCYB.2016.2533545},
  urldate = {2025-03-07},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  file = {/home/elessar/Zotero/storage/LRRB9BEK/Malik et al. - 2017 - Multilayered Echo State Machine A Novel Architecture and Algorithm.pdf}
}

@article{manjunathEchoStateProperty2013,
  title = {Echo {{State Property Linked}} to an {{Input}}: {{Exploring}} a {{Fundamental Characteristic}} of {{Recurrent Neural Networks}}},
  shorttitle = {Echo {{State Property Linked}} to an {{Input}}},
  author = {Manjunath, G. and Jaeger, H.},
  year = {2013},
  month = mar,
  journal = {Neural Computation},
  volume = {25},
  number = {3},
  pages = {671--696},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00411},
  urldate = {2025-04-21},
  abstract = {The echo state property is a key for the design and training of recurrent neural networks within the paradigm of reservoir computing. In intuitive terms this is a passivity condition: a network having this property, when driven by an input signal, will become entrained by the input and develop an internal response signal. This excited internal dynamics can be seen as a high-dimensional, nonlinear, unique transform of the input with a rich memory content. This view has implications for understanding neural dynamics beyond the field of reservoir computing. Available definitions and theorems concerning the echo state property, however, are of little practical use because they do not relate the network response to temporal or statistical properties of the driving input. Here we present a new definition of the echo state property which directly connects it to such properties. We derive a fundamental 0-1 law: if the input comes from an ergodic source, the network response has the echo state property with probability one or zero, independent of the given network. Furthermore we give a sufficient condition for the echo state property which connects statistical characteristics of the input to algebraic properties of the network connection matrix. The mathematical methods that we employ are freshly imported from the young field of nonautonomous dynamical systems theory. Since these methods are not yet well known in neural computation research, we introduce them in some detail. As a side story, we hope to demonstrate the eminent usefulness of these methods.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/6935BYEL/Manjunath and Jaeger - 2013 - Echo State Property Linked to an Input Exploring a Fundamental Characteristic of Recurrent Neural N.pdf;/home/elessar/Zotero/storage/UCMTKBTF/Manjunath and Jaeger - 2013 - Echo State Property Linked to an Input Exploring a Fundamental Characteristic of Recurrent Neural N.pdf}
}

@article{maNovelApproachMinimal2023,
  title = {A Novel Approach to Minimal Reservoir Computing},
  author = {Ma, Haochun and Prosperino, Davide and R{\"a}th, Christoph},
  year = {2023},
  month = aug,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {12970},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-39886-w},
  urldate = {2025-08-12},
  abstract = {Abstract             Reservoir computers are powerful machine learning algorithms for predicting nonlinear systems. Unlike traditional feedforward neural networks, they work on small training data sets, operate with linear optimization, and therefore require minimal computational resources. However, the traditional reservoir computer uses random matrices to define the underlying recurrent neural network and has a large number of hyperparameters that need to be optimized. Recent approaches show that randomness can be taken out by running regressions on a large library of linear and nonlinear combinations constructed from the input data and their time lags and polynomials thereof. However, for high-dimensional and nonlinear data, the number of these combinations explodes. Here, we show that a few simple changes to the traditional reservoir computer architecture further minimizing computational resources lead to significant and robust improvements in short- and long-term predictive performances compared to similar models while requiring minimal sizes of training data sets.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/BDJ4LH2A/Ma et al. - 2023 - A novel approach to minimal reservoir computing.pdf}
}

@article{mansillaAlgorithmicComplexityMinority2000,
  title = {Algorithmic Complexity in the Minority Game},
  author = {Mansilla, R.},
  year = {2000},
  month = oct,
  journal = {Physical Review E},
  volume = {62},
  number = {4},
  pages = {4553--4557},
  publisher = {American Physical Society (APS)},
  issn = {1063-651X, 1095-3787},
  doi = {10.1103/physreve.62.4553},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/329SGV69/Mansilla - 2000 - Algorithmic complexity in the minority game.pdf}
}

@article{margazoglouStabilityAnalysisChaotic2023,
  title = {Stability Analysis of Chaotic Systems from Data},
  author = {Margazoglou, Georgios and Magri, Luca},
  year = {2023},
  month = may,
  journal = {Nonlinear Dynamics},
  volume = {111},
  number = {9},
  pages = {8799--8819},
  issn = {0924-090X, 1573-269X},
  doi = {10.1007/s11071-023-08285-1},
  urldate = {2025-08-12},
  abstract = {Abstract             The prediction of the temporal dynamics of chaotic systems is challenging because infinitesimal perturbations grow exponentially. The analysis of the dynamics of infinitesimal perturbations is the subject of stability analysis. In stability analysis, we linearize the equations of the dynamical system around a reference point and compute the properties of the tangent space (i.e. the Jacobian). The main goal of this paper is to propose a method that infers the Jacobian, thus, the stability properties, from observables (data). First, we propose the echo state network (ESN) with the Recycle validation as a tool to accurately infer the chaotic dynamics from data. Second, we mathematically derive the Jacobian of the echo state network, which provides the evolution of infinitesimal perturbations. Third, we analyse the stability properties of the Jacobian inferred from the ESN and compare them with the benchmark results obtained by linearizing the equations. The ESN correctly infers the nonlinear solution and its tangent space with negligible numerical errors. In detail, we compute from data only (i) the long-term statistics of the chaotic state; (ii) the covariant Lyapunov vectors; (iii) the Lyapunov spectrum; (iv) the finite-time Lyapunov exponents; (v) and the angles between the stable, neutral, and unstable splittings of the tangent space (the degree of hyperbolicity of the attractor). This work opens up new opportunities for the computation of stability properties of nonlinear systems from data, instead of equations.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/7PKCT5RJ/Margazoglou and Magri - 2023 - Stability analysis of chaotic systems from data.pdf}
}

@misc{margolusCrystallineComputation1998,
  title = {Crystalline {{Computation}}},
  author = {Margolus, Norman},
  year = {1998},
  month = nov,
  number = {arXiv:comp-gas/9811002},
  eprint = {comp-gas/9811002},
  publisher = {arXiv},
  doi = {10.48550/arXiv.comp-gas/9811002},
  urldate = {2025-07-25},
  abstract = {Discrete lattice systems have had a long and productive history in physics. Examples range from exact theoretical models studied in statistical mechanics to approximate numerical treatments of continuum models. There has, however, been relatively little attention paid to exact lattice models which obey an invertible dynamics: from any state of the dynamical system you can infer the previous state. This kind of microscopic reversibility is an important property of all microscopic physical dynamics. Invertible lattice systems become even more physically realistic if we impose locality of interaction and exact conservation laws. In fact, some invertible and momentum conserving lattice dynamics---in which discrete particles hop between neighboring lattice sites at discrete times---accurately reproduce hydrodynamics in the macroscopic limit.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Cellular Automata and Lattice Gases,Nonlinear Sciences - Cellular Automata and Lattice Gases},
  file = {/home/elessar/Zotero/storage/4DJANEEZ/Margolus - 1998 - Crystalline Computation.pdf}
}

@article{markovichInformationSpreadingEvolution2022,
  title = {Information {{Spreading}} and {{Evolution}} of {{Non-Homogeneous Networks}}},
  author = {Markovich, Natalia M and Ryzhov, Maksim S},
  year = {2022},
  abstract = {Information spreading among nodes of directed random networks by means of the linear preferential attachment (PA) schemes and the well-known SPREAD algorithm is considered. The novelty of the paper is that schemes of the linear preferential attachment proposed in Wan et al. (2020) for the network evolution are also used here for the information spreading. The SPREAD algorithm proposed for undirected random graphs is adapted to directed graphs. Moreover, we deal with non-homogeneous directed networks consisting of nodes whose inand out-degrees have different power law distributions that is realistic for practice and we find communities in a network that spread the information faster. We compare the minimum number of evolution steps K{$\ast$} required for the preferential attachment schemes and the well-known algorithm SPREAD to spread a message among a fixed number of nodes. The evolution of the network in time starts from a seed set of nodes. We study the impact of the seed network and parameters of the preferential attachment on K{$\ast$} for simulated graphs. Real temporal graphs are also investigated in the same way. The PA may be a better spreader than the SPREAD algorithm. This is valid for the sets of the PA parameters with dominating proportions of created new edges from existing nodes to newly appending ones or between the existing nodes only. It is shown both for simulated and real graphs that the communities with the smallest tail indices of the out-degrees and PageRanks may spread the message faster than other communities.},
  langid = {english},
  annotation = {GSCC: 0000005},
  file = {/home/elessar/Zotero/storage/J7XC8G9P/Markovich and Ryzhov - 2022 - Information Spreading and Evolution of Non-Homogen.pdf}
}

@article{marshIntroductionContinuousEntropy,
  title = {Introduction to {{Continuous Entropy}}},
  author = {Marsh, Charles},
  abstract = {Classically, Shannon entropy was formalized over discrete probability distributions. However, the concept of entropy can be extended to continuous distributions through a quantity known as continuous (or differential ) entropy. The most common definition for continuous entropy is seemingly straightforward; however, further analysis reveals a number of shortcomings that render it far less useful than it appears. Instead, relative entropy (or KL divergence) proves to be the key to information theory in the continuous case, as the notion of comparing entropy across probability distributions retains value. Expanding off this notion, we present several results in the field of maximum entropy and, in particular, conclude with an information-theoretic proof of the Central Limit Theorem using continuous relative entropy.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/57CF78QY/Marsh - Introduction to Continuous Entropy.pdf}
}

@article{marsiliLearningCoordinateComplex2001,
  title = {Learning to {{Coordinate}} in a {{Complex}} and {{Nonstationary World}}},
  author = {Marsili, M. and Mulet, R. and {Ricci-Tersenghi}, F. and Zecchina, R.},
  year = {2001},
  month = oct,
  journal = {Physical Review Letters},
  volume = {87},
  number = {20},
  publisher = {American Physical Society (APS)},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/physrevlett.87.208701},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/CAN3RXNE/Marsili et al. - 2001 - Learning to Coordinate in a Complex and Nonstationary World.PDF}
}

@article{marszalekPropertiesMemristiveCircuits2015,
  title = {Properties of Memristive Circuits with Mixed-mode Oscillations},
  author = {Marszalek, W. and Trzaska, Z.W.},
  year = {2015},
  month = jan,
  journal = {Electronics Letters},
  volume = {51},
  number = {2},
  pages = {140--141},
  issn = {0013-5194, 1350-911X},
  doi = {10.1049/el.2014.3235},
  urldate = {2024-04-30},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  annotation = {GSCC: 0000019},
  file = {/home/elessar/Zotero/storage/INRLS4MK/marszalek2015.pdf.pdf;/home/elessar/Zotero/storage/UF6RKCGC/Marszalek and Trzaska - 2015 - Properties of memristive circuits with mixed‐mode .pdf}
}

@article{martinEntropyFixedPoint2006,
  title = {Entropy as a Fixed Point},
  author = {Martin, Keye},
  year = {2006},
  month = feb,
  journal = {Theoretical Computer Science},
  volume = {350},
  number = {2-3},
  pages = {292--324},
  publisher = {Elsevier BV},
  issn = {0304-3975},
  doi = {10.1016/j.tcs.2005.10.026},
  urldate = {2025-07-25},
  abstract = {We study complexity and information and introduce the idea that while complexity is relative to a given class of processes, information is process independent: Information is complexity relative to the class of all conceivable processes. In essence, the idea is that information is an extension of the concept `algorithmic complexity' from a class of desirable and concrete processes, such as those represented by binary decision trees, to a class more general that can only in pragmatic terms be regarded as existing in the conception. It is then precisely the fact that information is defined relative to such a large class of processes that it becomes an effective tool for analyzing phenomena in a wide range of disciplines.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/RTK9NSK7/Martin - 2006 - Entropy as a fixed point.pdf}
}

@article{martinUMLTutorialFinite,
  title = {{{UML Tutorial}}: {{Finite State Machines}}},
  author = {Martin, Robert C},
  langid = {english},
  file = {/home/elessar/Zotero/storage/EGG2MTPP/Martin - UML Tutorial Finite State Machines.pdf}
}

@article{martinuzziLearningExtremeVegetation2024,
  title = {Learning Extreme Vegetation Response to Climate Drivers with Recurrent Neural Networks},
  author = {Martinuzzi, Francesco and Mahecha, Miguel D. and {Camps-Valls}, Gustau and Montero, David and Williams, Tristan and Mora, Karin},
  year = {2024},
  month = nov,
  journal = {Nonlinear Processes in Geophysics},
  volume = {31},
  number = {4},
  pages = {535--557},
  issn = {1607-7946},
  doi = {10.5194/npg-31-535-2024},
  urldate = {2025-08-12},
  abstract = {Abstract. The spectral signatures of vegetation are indicative of ecosystem states and health. Spectral indices used to monitor vegetation are characterized by long-term trends, seasonal fluctuations, and responses to weather anomalies. This study investigates the potential of neural networks in learning and predicting vegetation response, including extreme behavior from meteorological data. While machine learning methods, particularly neural networks, have significantly advanced in modeling nonlinear dynamics, it has become standard practice to approach the problem using recurrent architectures capable of capturing nonlinear effects and accommodating both long- and short-term memory. We compare four recurrent-based learning models, which differ in their training and architecture for predicting spectral indices at different forest sites in Europe: (1) recurrent neural networks (RNNs), (2) long short-term memory networks (LSTMs), (3) gated recurrent unit networks (GRUs), and (4) echo state networks (ESNs). While our results show minimal quantitative differences in their performances, ESNs exhibit slightly superior results across various metrics. Overall, we show that recurrent network architectures prove generally suitable for vegetation state prediction yet exhibit limitations under extreme conditions. This study highlights the potential of recurrent network architectures for vegetation state prediction, emphasizing the need for further research to address limitations in modeling extreme conditions within ecosystem dynamics.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english}
}

@misc{martinuzziMinimalDeterministicEcho2025,
  title = {Minimal {{Deterministic Echo State Networks Outperform Random Reservoirs}} in {{Learning Chaotic Dynamics}}},
  author = {Martinuzzi, Francesco},
  year = {2025},
  month = jul,
  number = {arXiv:2507.06050},
  eprint = {2507.06050},
  primaryclass = {nlin},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.06050},
  urldate = {2025-08-12},
  abstract = {Machine learning (ML) is widely used to model chaotic systems. Among ML approaches, echo state networks (ESNs) have received considerable attention due to their simple construction and fast training. However, ESN performance is highly sensitive to hyperparameter choices and to its random initialization. In this work, we demonstrate that ESNs constructed using deterministic rules and simple topologies (MESNs) outperform standard ESNs in the task of chaotic attractor reconstruction. We use a dataset of more than 90 chaotic systems to benchmark 10 different minimal deterministic reservoir initializations. We find that MESNs obtain up to a 41\% reduction in error compared to standard ESNs. Furthermore, we show that the MESNs are more robust, exhibiting less inter-run variation, and have the ability to reuse hyperparameters across different systems. Our results illustrate how structured simplicity in ESN design can outperform stochastic complexity in learning chaotic dynamics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Nonlinear Sciences - Chaotic Dynamics},
  file = {/home/elessar/Zotero/storage/VXULIMTX/Martinuzzi - 2025 - Minimal Deterministic Echo State Networks Outperform Random Reservoirs in Learning Chaotic Dynamics.pdf;/home/elessar/Zotero/storage/K3SZFFLL/2507.html}
}

@article{marzenInformationAnatomyStochastic2014,
  title = {Information {{Anatomy}} of {{Stochastic Equilibria}}},
  author = {Marzen, Sarah and Crutchfield, James},
  year = {2014},
  month = aug,
  journal = {Entropy},
  volume = {16},
  number = {9},
  pages = {4713--4748},
  publisher = {MDPI AG},
  issn = {1099-4300},
  doi = {10.3390/e16094713},
  urldate = {2025-07-25},
  abstract = {A stochastic nonlinear dynamical system generates information, as measured by its entropy rate. Some---the ephemeral information---is dissipated and some---the bound information---is actively stored and so affects future behavior. We derive analytic expressions for the ephemeral and bound information in the limit of infinitesimal time discretization for two classical systems that exhibit dynamical equilibria: first-order Langevin equations (i) where the drift is the gradient of an analytic potential function and the diffusion matrix is invertible and (ii) with a linear drift term (Ornstein--Uhlenbeck), but a noninvertible diffusion matrix. In both cases, the bound information is sensitive to the drift and diffusion, while the ephemeral information is sensitive only to the diffusion matrix and not to the drift. Notably, this information anatomy changes discontinuously as any of the diffusion coefficients vanishes, indicating that it is very sensitive to the noise structure. We then calculate the information anatomy of the stochastic cusp catastrophe and of particles diffusing in a heat bath in the overdamped limit, both examples of stochastic gradient descent on a potential landscape. Finally, we use our methods to calculate and compare approximations for the time-local predictive information for adaptive agents.},
  copyright = {https://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Computer Science - Information Theory,Condensed Matter - Statistical Mechanics,Mathematics - Dynamical Systems,Mathematics - Information Theory,Nonlinear Sciences - Chaotic Dynamics},
  file = {/home/elessar/Zotero/storage/58R2MBVW/Marzen and Crutchfield - 2014 - Information Anatomy of Stochastic Equilibria.pdf;/home/elessar/Zotero/storage/NGRWT46A/Marzen and Crutchfield - 2014 - Information Anatomy of Stochastic Equilibria.pdf}
}

@article{massarMeanFieldTheory2013,
  title = {Mean {{Field Theory}} of {{Dynamical Systems Driven}} by {{External Signals}}},
  author = {Massar, Marc and Massar, Serge},
  year = {2013},
  month = apr,
  journal = {Physical Review E},
  volume = {87},
  number = {4},
  eprint = {1210.8260},
  primaryclass = {nlin},
  pages = {042809},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.87.042809},
  urldate = {2025-03-11},
  abstract = {Dynamical systems driven by strong external signals are ubiquituous in nature and engineering. Here we study "echo state networks", networks of a large number of randomly connected nodes, which represent a simple model of a neural network, and have important applications in machine learning. We develop a mean field theory of echo state networks. The dynamics of the network is captured by the evolution law, similar to a logistic map, for a single collective variable. When the network is driven by many independent external signals, this collective variable reaches a steady state. But when the network is driven by a single external signal, the collective variable is nonstationnary but can be characterised by its time averaged distribution. The predictions of the mean field theory, including the value of the largest Lyaponuov exponent, are compared with the numerical integration of the equations of motion.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Condensed Matter - Disordered Systems and Neural Networks,Nonlinear Sciences - Chaotic Dynamics},
  file = {/home/elessar/Zotero/storage/MPW3E4LD/Massar and Massar - 2013 - Mean Field Theory of Dynamical Systems Driven by External Signals.pdf;/home/elessar/Zotero/storage/XYHRJCNS/1210.html}
}

@article{mateosMedidasComplejidadInformacion,
  title = {{Medidas de Complejidad y de informaci{\'o}n como herramientas para el an{\'a}lisis de series temporales.}},
  author = {Mateos, Diego M},
  langid = {spanish},
  keywords = {No DOI found},
  annotation = {GSCC: 0000002},
  file = {/home/elessar/Zotero/storage/9W2JV6P3/Mateos - Medidas de Complejidad y de información como herra.pdf}
}

@article{maTimeDomainGeneralizationRandomCoupling2023,
  title = {Time-{{Domain Generalization}} of the {{Random-Coupling Model}} and {{Experimental Verification}} in a {{Complex Scattering System}}},
  author = {Ma, Shukai and Antonsen, Thomas M. and Anlage, Steven M.},
  year = {2023},
  month = jun,
  journal = {Physical Review Applied},
  volume = {19},
  number = {6},
  pages = {064052},
  issn = {2331-7019},
  doi = {10.1103/PhysRevApplied.19.064052},
  urldate = {2024-01-10},
  langid = {english},
  annotation = {GSCC: 0000001},
  file = {/home/elessar/Zotero/storage/BGJHSGJS/Ma et al. - 2023 - Time-Domain Generalization of the Random-Coupling .pdf}
}

@article{melchertComputationalMechanicsApproach,
  title = {A Computational Mechanics Approach to Estimate Entropy and (Approximate) Complexity for the Dynamics of the {{2D Ising Ferromagnet}}},
  author = {Melchert, O and Hartmann, A K},
  abstract = {We present a numerical analysis of the entropy rate and statistical complexity related to the spin flip dynamics of the 2D Ising Ferromagnet at different temperatures T . We follow an information theoretic approach and test three different entropy estimation algorithms to asses entropy rate and statistical complexity of binary sequences. The latter are obtained by monitoring the orientation of a single spin on a square lattice of side-length L = 256 at a given temperature parameter over time. The different entropy estimation procedures are based on the M -block Shannon entropy (a well established method that yields results for benchmarking purposes), non-sequential recursive pair substitution (providing an elaborate and an approximate estimator) and a convenient data compression algorithm contained in the zlib-library (providing an approximate estimator only). We propose an approximate measure of statistical complexity that emphasizes on correlations within the sequence and which is easy to implement, even by means of black-box data compression algorithms. Regarding the 2D Ising Ferromagnet simulated using Metropolis dynamics and for binary sequences of finite length, the proposed approximate complexity measure is peaked close to the critical temperature. For the approximate estimators, a finite-size scaling analysis reveals that the peak approaches the critical temperature as the sequence length increases. Results obtained using different spin-flip dynamics are briefly discussed. The suggested complexity measure can be extended to non-binary sequences in a straightforward manner.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/RBPTQVEN/Melchert and Hartmann - A computational mechanics approach to estimate entropy and (approximate) complexity for the dynamics.pdf}
}

@article{mendesErgodicParametersDynamical2011,
  title = {Ergodic Parameters and Dynamical Complexity},
  author = {Mendes, Rui Vilela},
  year = {2011},
  month = sep,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {21},
  number = {3},
  publisher = {AIP Publishing},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.3634008},
  urldate = {2025-07-25},
  abstract = {Using a cocycle formulation, old and new ergodic parameters beyond the Lyapunov exponent are rigorously characterized. Dynamical Renyi entropies and fluctuations of the local expansion rate are related by a generalization of the Pesin formula. How the ergodic parameters may be used to characterize the complexity of dynamical systems is illustrated by some examples: clustering and synchronization, self-organized criticality and the topological structure of networks.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/XBXFFHW6/Mendes - 2011 - Ergodic parameters and dynamical complexity.pdf}
}

@article{merkleKolmogorovLovelandRandomness2006,
  title = {Kolmogorov--{{Loveland}} Randomness and Stochasticity},
  author = {Merkle, Wolfgang and Miller, Joseph S. and Nies, Andr{\'e} and Reimann, Jan and Stephan, Frank},
  year = {2006},
  month = mar,
  journal = {Annals of Pure and Applied Logic},
  volume = {138},
  number = {1-3},
  pages = {183--210},
  publisher = {Elsevier BV},
  issn = {0168-0072},
  doi = {10.1016/j.apal.2005.06.011},
  urldate = {2025-07-25},
  abstract = {An infinite binary sequence X is Kolmogorov--Loveland (or KL-) random if there is no computable non-monotonic betting strategy that succeeds on X in the sense of having an unbounded gain in the limit while betting successively on bits of X. A sequence X is KL-stochastic if there is no computable non-monotonic selection rule that selects from X an infinite, biased sequence.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/QBCTE8NZ/Merkle et al. - 2006 - Kolmogorov–Loveland randomness and stochasticity.pdf}
}

@article{mertensComputationalComplexityPhysicists2002,
  title = {Computational Complexity for Physicists},
  author = {Mertens, S.},
  year = {2002},
  journal = {Computing in Science \& Engineering},
  volume = {4},
  number = {3},
  pages = {31--47},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {1521-9615},
  doi = {10.1109/5992.998639},
  urldate = {2025-07-25},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/SNSGBZJH/Mertens - 2002 - Computational complexity for physicists.PDF}
}

@misc{meyerHarnessingDiscreteRepresentations2024,
  title = {Harnessing {{Discrete Representations For Continual Reinforcement Learning}}},
  author = {Meyer, Edan and White, Adam and Machado, Marlos C.},
  year = {2024},
  month = jul,
  number = {arXiv:2312.01203},
  eprint = {2312.01203},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.01203},
  urldate = {2025-03-20},
  abstract = {Reinforcement learning (RL) agents make decisions using nothing but observations from the environment, and consequently, heavily rely on the representations of those observations. Though some recent breakthroughs have used vector-based categorical representations of observations, often referred to as discrete representations, there is little work explicitly assessing the significance of such a choice. In this work, we provide a thorough empirical investigation of the advantages of representing observations as vectors of categorical values within the context of reinforcement learning. We perform evaluations on world-model learning, model-free RL, and ultimately continual RL problems, where the benefits best align with the needs of the problem setting. We find that, when compared to traditional continuous representations, world models learned over discrete representations accurately model more of the world with less capacity, and that agents trained with discrete representations learn better policies with less data. In the context of continual RL, these benefits translate into faster adapting agents. Additionally, our analysis suggests that the observed performance improvements can be attributed to the information contained within the latent vectors and potentially the encoding of the discrete representation itself.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/elessar/Zotero/storage/QFXQ6G5A/Meyer et al. - 2024 - Harnessing Discrete Representations For Continual Reinforcement Learning.pdf}
}

@article{milleaExplorationsEchoState,
  title = {Explorations in {{Echo State Networks}}},
  author = {Millea, Adrian and Thesis, Master's},
  langid = {english},
  annotation = {GSCC: 0000012},
  file = {/home/elessar/Zotero/storage/M4NH3489/Millea and Thesis - Explorations in Echo State Networks.pdf}
}

@article{millerReconstructionMutilatedEnglish1957,
  title = {The Reconstruction of Mutilated English Texts},
  author = {Miller, George A. and Friedman, Elizabeth A.},
  year = {1957},
  month = sep,
  journal = {Information and Control},
  volume = {1},
  number = {1},
  pages = {38--55},
  publisher = {Elsevier BV},
  issn = {0019-9958},
  doi = {10.1016/s0019-9958(57)90061-x},
  urldate = {2025-07-25},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/IZ3NVIMB/Miller and Friedman - 1957 - The reconstruction of mutilated english texts.pdf}
}

@misc{mnihPlayingAtariDeep2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  month = dec,
  number = {arXiv:1312.5602},
  eprint = {1312.5602},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-10},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  annotation = {GSCC: 0015733},
  file = {/home/elessar/Zotero/storage/HP8H9V2S/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf}
}

@article{moddemeijerEstimationEntropyMutual1989,
  title = {On Estimation of Entropy and Mutual Information of Continuous Distributions},
  author = {Moddemeijer, R.},
  year = {1989},
  month = mar,
  journal = {Signal Processing},
  volume = {16},
  number = {3},
  pages = {233--248},
  issn = {01651684},
  doi = {10.1016/0165-1684(89)90132-1},
  urldate = {2025-07-29},
  abstract = {Mutual information is used in a procedure to estimate time-delays between recordings of electroencephalogram (EEG) signals originating from epileptic animals and patients. We present a simple and reliable histogram-based method to estimate mutual information. The accuracies of this mutual information estimator and of a similar entropy estimator are discussed. The bias and variance calculations presented can also be applied to discrete valued systems. Finally, we present some simulation results, which are compared with earlier work.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/ZWCY9PII/Moddemeijer - 1989 - On estimation of entropy and mutual information of continuous distributions.pdf}
}

@article{montanaKolmogorovComplexityReal1998,
  title = {On {{Kolmogorov}} Complexity in the Real {{Turing}} Machine Setting},
  author = {Monta{\~n}a, J.L. and Pardo, Luis M.},
  year = {1998},
  month = jul,
  journal = {Information Processing Letters},
  volume = {67},
  number = {2},
  pages = {81--86},
  publisher = {Elsevier BV},
  issn = {0020-0190},
  doi = {10.1016/s0020-0190(98)00089-1},
  urldate = {2025-07-25},
  abstract = {We extend notion of complexity to computational model real Turing Following the lines of we show Invariance Theorem explore the of incompressibility. explicitly show infinite strings real numbers. of these are so that the complexity notion, on real machines, has in commom ordinary randomness. 1998 Elsevier B.V. All reserved.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/MT7GCQDC/Montaña and Pardo - 1998 - On Kolmogorov complexity in the real Turing machine setting.pdf}
}

@book{montavonNeuralNetworksTricks2012,
  title = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}: {{Second Edition}}},
  shorttitle = {Neural {{Networks}}},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {7700},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-35289-8},
  urldate = {2023-05-30},
  isbn = {978-3-642-35288-1 978-3-642-35289-8},
  langid = {english},
  annotation = {GSCC: 0000557},
  file = {/home/elessar/Zotero/storage/MPQI6D9D/Montavon et al. - 2012 - Neural Networks Tricks of the Trade Second Editi.pdf}
}

@article{mooreQuantumAutomataQuantum2000,
  title = {Quantum Automata and Quantum Grammars},
  author = {Moore, Cristopher and Crutchfield, James P.},
  year = {2000},
  month = apr,
  journal = {Theoretical Computer Science},
  volume = {237},
  number = {1-2},
  pages = {275--306},
  publisher = {Elsevier BV},
  issn = {0304-3975},
  doi = {10.1016/s0304-3975(98)00191-1},
  urldate = {2025-07-15},
  abstract = {To study quantum computation, it might be helpful to generalize structures from language and automata theory to the quantum case. To that end, we propose quantum versions of nite-state and push-down automata, and regular and context-free grammars. We nd analogs of several classical theorems, including pumping lemmas, closure properties, rational and algebraic generating functions, and Greibach normal form. We also show that there are quantum context-free languages that are not context-free, so QCFL 6= CFL. c{\copyright} 2000 Elsevier Science B.V. All rights reserved.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/YH3D79NV/Moore and Crutchfield - 2000 - Quantum automata and quantum grammars.pdf}
}

@article{muchnikAlmostPeriodicSequences2003,
  title = {Almost Periodic Sequences},
  author = {Muchnik, {\relax An}. and Semenov, A. and Ushakov, M.},
  year = {2003},
  month = jul,
  journal = {Theoretical Computer Science},
  volume = {304},
  number = {1-3},
  pages = {1--33},
  publisher = {Elsevier BV},
  issn = {0304-3975},
  doi = {10.1016/s0304-3975(02)00847-2},
  urldate = {2025-07-15},
  abstract = {This paper studies properties of almost periodic sequences (also known as uniformly recursive). A sequence is almost periodic if for every 0nite string that ccurs in0nitely many times in the sequence there exists a number m such that every segment of length m contains an ccurrence of the word.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/8UJ8MFBP/Muchnik et al. - 2003 - Almost periodic sequences.pdf}
}

@article{muchnikConditionalComplexityCodes2002,
  title = {Conditional Complexity and Codes},
  author = {Muchnik, Andrej A.},
  year = {2002},
  month = jan,
  journal = {Theoretical Computer Science},
  volume = {271},
  number = {1-2},
  pages = {97--109},
  publisher = {Elsevier BV},
  issn = {0304-3975},
  doi = {10.1016/s0304-3975(01)00033-0},
  urldate = {2025-07-15},
  abstract = {Let x and y be binarystrings. We prove that there exists a program p of size about K(x{\textbar}y) that maps y to x and has small complexitywhen x is known (K(p{\textbar}x) {$\approx$} 0). Having in mind the parallelism between Shannon information theoryand algorithmic information theor,y one can saythat this result is parallel to Wolf--Slepian and K.orner--Csiszar--Marton theorems, see (I. Csiszar and J. Ko.rner, Information theory, Coding Theorems for Discrete Memoryless Systems, Akad3emiai Kiad3o, Budapest, 1981).},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/E6R5CPDE/Muchnik - 2002 - Conditional complexity and codes.pdf}
}

@article{muchnikKolmogorovEntropyContext2002,
  title = {Kolmogorov Entropy in the Context of Computability Theory},
  author = {Muchnik, Andrej A. and Positselsky, Semen Ye.},
  year = {2002},
  month = jan,
  journal = {Theoretical Computer Science},
  volume = {271},
  number = {1-2},
  pages = {15--35},
  publisher = {Elsevier BV},
  issn = {0304-3975},
  doi = {10.1016/s0304-3975(01)00028-7},
  urldate = {2025-07-15},
  abstract = {We consider the overgraph of the Kolmogorov entropy function and study whether it is a complete enumerable set with respect to di.erent types of reductions. It turns out that (for any type of entropy) the overgraph of the conditional entropy function is m-complete, but the overgraph of the unconditional entropy function is not m-complete (and also not bT -complete). For tt-completeness, the situation is more subtle: the overgraph of the unconditional pre3x entropy may be tt-complete or incomplete dependingon the optimal prorgammingsystem used in the de3nition of entropy. To prove these results we use the notion of r-separability and its e.ective version introduced in this article for the 3rst time. c{\copyright} 2002 Elsevier Science B.V. All rights reserved.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/PQYYSCIC/Muchnik and Positselsky - 2002 - Kolmogorov entropy in the context of computability theory.pdf}
}

@article{nagarajanQuantifyingPhysiologicalData2002,
  title = {Quantifying Physiological Data with {{Lempel-Ziv}} Complexity-Certain Issues},
  author = {Nagarajan, R.},
  year = {2002},
  month = nov,
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {49},
  number = {11},
  pages = {1371--1373},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9294},
  doi = {10.1109/tbme.2002.804582},
  urldate = {2025-07-15},
  abstract = {The oscillations observed in physiological data can be attributed largely to the presence of either a nonlinear deterministic or a nondeterministic component. The Lempel--Ziv [1] complexity and its variants have been used successfully to quantify the regularity of these oscillations. The decrease in the complexity can be observed in the case of nontrivial deterministic patterns as well as correlated noise. Thus, any conclusion on the nature of the pattern based solely on the value of the Lempel--Ziv complexity is incomplete. In this paper, the use of the surrogate data technique is suggested to avoid spurious interpretation of this measure of complexity. The data sets considered include the uterine contraction obtained during active labor. The surrogates are generated using the amplitude adjusted fourier transform and the iterated amplitude adjusted fourier transform. The approximate entropy [2] is used as an alternate measure to verify the results obtained.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/NQI73F3M/Nagarajan - 2002 - Quantifying physiological data with Lempel-Ziv complexity-certain issues.pdf}
}

@article{naHierarchicalDelaymemoryEcho2021,
  title = {Hierarchical Delay-Memory Echo State Network: {{A}} Model Designed for Multi-Step Chaotic Time Series Prediction},
  shorttitle = {Hierarchical Delay-Memory Echo State Network},
  author = {Na, Xiaodong and Ren, Weijie and Xu, Xinghan},
  year = {2021},
  month = jun,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {102},
  pages = {104229},
  issn = {09521976},
  doi = {10.1016/j.engappai.2021.104229},
  urldate = {2025-03-12},
  abstract = {Predicting for long-term dynamics of complex systems from observations is a challenging topic in the field of time series modeling and analysis, and is continually under research. Noteworthily, multi-step prediction requires accurate learning of dynamics and correlations between historical data for predicting future behavior. In this paper, we proposed a modified recurrent neural network named hierarchical delay-memory echo state network (HDESN) for solving the task of multi-step chaotic time series prediction. The HDESN uses multiple reservoirs with delay-memory capabilities, which can simultaneously discover and explore the information of short-term and long-term memory hidden in the historical sequence, and extract the valuable evolution patterns through deep topology and hierarchical processing. Moreover, to ensure high-quality prediction results and reduce the computational burden as much as possible, we further design a phase-space representation strategy which can calculate a compact topology and delay-memory coefficient according to the chaotic characteristics of the data. Compared with other improved ESN-based models, the proposed HDESN does not have a larger memory capacity to capture potential evolution law hidden in the complex system layer by layer, but can also adaptively determine a suitable network architecture to reflect the mapping relations in chaotic phase space. The experimental results on two benchmark chaotic systems and a real-world meteorological dataset demonstrate that the proposed HDESN model obtains satisfactory performance in multi-step chaotic time series prediction.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/PU8IB2ZK/Na et al. - 2021 - Hierarchical delay-memory echo state network A model designed for multi-step chaotic time series pr.pdf}
}

@book{nakajimaReservoirComputingTheory2021,
  title = {Reservoir {{Computing}}: {{Theory}}, {{Physical Implementations}}, and {{Applications}}},
  shorttitle = {Reservoir {{Computing}}},
  editor = {Nakajima, Kohei and Fischer, Ingo},
  year = {2021},
  series = {Natural {{Computing Series}}},
  publisher = {Springer Singapore},
  address = {Singapore},
  doi = {10.1007/978-981-13-1687-6},
  urldate = {2023-05-30},
  isbn = {978-981-13-1686-9 978-981-13-1687-6},
  langid = {english},
  annotation = {GSCC: 0000273},
  file = {/home/elessar/Zotero/storage/LCUZ8MM8/Nakajima and Fischer - 2021 - Reservoir Computing Theory, Physical Implementati.pdf}
}

@article{nakajimaScalableReservoirComputing2021,
  title = {Scalable Reservoir Computing on Coherent Linear Photonic Processor},
  author = {Nakajima, Mitsumasa and Tanaka, Kenji and Hashimoto, Toshikazu},
  year = {2021},
  month = feb,
  journal = {Communications Physics},
  volume = {4},
  number = {1},
  pages = {20},
  issn = {2399-3650},
  doi = {10.1038/s42005-021-00519-1},
  urldate = {2025-08-12},
  abstract = {Abstract                            Photonic neuromorphic computing is of particular interest due to its significant potential for ultrahigh computing speed and energy efficiency. The advantage of photonic computing hardware lies in its ultrawide bandwidth and parallel processing utilizing inherent parallelism. Here, we demonstrate a scalable on-chip photonic implementation of a simplified recurrent neural network, called a reservoir computer, using an integrated coherent linear photonic processor. In contrast to previous approaches, both the input and recurrent weights are encoded in the spatiotemporal domain by photonic linear processing, which enables scalable and ultrafast computing beyond the input electrical bandwidth. As the device can process multiple wavelength inputs over the telecom C-band simultaneously, we can use ultrawide optical bandwidth ({\textasciitilde}5 terahertz) as a computational resource. Experiments for the standard benchmarks showed good performance for chaotic time-series forecasting and image classification. The device is considered to be able to perform 21.12 tera multiplication--accumulation operations per second (MAC\,{$\bullet$}\,s               -1               ) for each wavelength and can reach petascale computation speed on a single photonic chip by using wavelength division multiplexing. Our results are challenging for conventional Turing--von Neumann machines, and they confirm the great potential of photonic neuromorphic processing towards peta-scale neuromorphic super-computing on a photonic chip.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/FIXY4DGN/Nakajima et al. - 2021 - Scalable reservoir computing on coherent linear photonic processor.pdf}
}

@article{nakanoMultiscaleSimulationNanosystems2001,
  title = {Multiscale Simulation of Nanosystems},
  author = {Nakano, A. and Bachlechner, M.E. and Kalia, R.K. and Lidorikis, E. and Vashishta, P. and Voyiadjis, G.Z. and Campbell, T.J. and Ogata, S. and Shimojo, F.},
  year = {2001},
  journal = {Computing in Science \& Engineering},
  volume = {3},
  number = {4},
  pages = {56--66},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {1521-9615},
  doi = {10.1109/5992.931904},
  urldate = {2025-07-25},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/INXQBA5Q/Nakano et al. - 2001 - Multiscale simulation of nanosystems.PDF}
}

@article{natheReservoirComputingNoise2023a,
  title = {Reservoir Computing with Noise},
  author = {Nathe, Chad and Pappu, Chandra and Mecholsky, Nicholas A. and Hart, Joe and Carroll, Thomas and Sorrentino, Francesco},
  year = {2023},
  month = apr,
  journal = {Chaos},
  volume = {33},
  number = {4},
  pages = {041101},
  issn = {1054-1500},
  doi = {10.1063/5.0130278},
  urldate = {2025-08-12},
  abstract = {This paper investigates in detail the effects of measurement noise on the performance of reservoir computing. We focus on an application in which reservoir computers are used to learn the relationship between different state variables of a chaotic system. We recognize that noise can affect the training and testing phases differently. We find that the best performance of the reservoir is achieved when the strength of the noise that affects the input signal in the training phase equals the strength of the noise that affects the input signal in the testing phase. For all the cases we examined, we found that a good remedy to noise is to low-pass filter the input and the training/testing signals; this typically preserves the performance of the reservoir, while reducing the undesired effects of noise.},
  pmcid = {PMC10132850},
  pmid = {37097967},
  file = {/home/elessar/Zotero/storage/4SRSPYYE/Nathe et al. - 2023 - Reservoir computing with noise.pdf}
}

@misc{nazaretVariationalInferenceInfinitely2022,
  title = {Variational {{Inference}} for {{Infinitely Deep Neural Networks}}},
  author = {Nazaret, Achille and Blei, David},
  year = {2022},
  month = sep,
  number = {arXiv:2209.10091},
  eprint = {2209.10091},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-05-30},
  abstract = {We introduce the unbounded depth neural network (UDN), an infinitely deep probabilistic model that adapts its complexity to the training data. The UDN contains an infinite sequence of hidden layers and places an unbounded prior on a truncation , the layer from which it produces its data. Given a dataset of observations, the posterior UDN provides a conditional distribution of both the parameters of the infinite neural network and its truncation. We develop a novel variational inference algorithm to approximate this posterior, optimizing a distribution of the neural network weights and of the truncation depth , and without any upper limit on . To this end, the variational family has a special structure: it models neural network weights of arbitrary depth, and it dynamically creates or removes free variational parameters as its distribution of the truncation is optimized. (Unlike heuristic approaches to model search, it is solely through gradient-based optimization that this algorithm explores the space of truncations.) We study the UDN on real and synthetic data. We find that the UDN adapts its posterior depth to the dataset complexity; it outperforms standard neural networks of similar computational complexity; and it outperforms other approaches to infinite-depth neural networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {GSCC: 0000005},
  file = {/home/elessar/Zotero/storage/MP7V5HC6/Nazaret and Blei - 2022 - Variational Inference for Infinitely Deep Neural N.pdf}
}

@article{nealeAllopticalControlMicrofluidic2005,
  title = {All-Optical Control of Microfluidic Components Using Form Birefringence},
  author = {Neale, Steven L. and MacDonald, Michael P. and Dholakia, Kishan and Krauss, Thomas F.},
  year = {2005},
  month = jul,
  journal = {Nature Materials},
  volume = {4},
  number = {7},
  pages = {530--533},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {1476-1122, 1476-4660},
  doi = {10.1038/nmat1411},
  urldate = {2025-07-15},
  copyright = {https://www.springer.com/tdm},
  langid = {english},
  file = {/home/elessar/Zotero/storage/RHWW52V8/Neale et al. - 2005 - All-optical control of microfluidic components using form birefringence.pdf}
}

@article{newhouseBifurcationsStabilityFamilies1983,
  title = {Bifurcations and Stability of Families of Diffeomorphisms},
  author = {Newhouse, S. and Palis, J. and Takens, F.},
  year = {1983},
  month = dec,
  journal = {Publications math{\'e}matiques de l'IH{\'E}S},
  volume = {57},
  number = {1},
  pages = {5--71},
  issn = {0073-8301, 1618-1913},
  doi = {10.1007/BF02698773},
  urldate = {2025-07-29},
  abstract = {We consider one parameter families or arcs of diffeomorphisms. For families starting with Morse-Smale diffeomorphisms we characterize various types of (structural) stability at or near the first bifurcation point. We also give a complete description of the stable arcs of diffeomorphisms whose limit sets consist of finitely many orbits. Universal models for the local unfoldings of the bifurcating periodic orbits (especially saddlenodes) are established, as well as several results on the global dynamical structure of the bifurcating diffeomorphisms. Moduli of stability related to saddle-connections are introduced.},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/home/elessar/Zotero/storage/LLK62E3L/Newhouse et al. - 1983 - Bifurcations and stability of families of diffeomorphisms.pdf}
}

@book{newmanNetworks2018,
  title = {Networks},
  author = {Newman, Mark},
  year = {2018},
  month = oct,
  volume = {1},
  publisher = {Oxford University Press},
  doi = {10.1093/oso/9780198805090.001.0001},
  urldate = {2025-02-10},
  abstract = {The study of networks, including computer networks, social networks, and biological networks, has attracted enormous interest in recent years. The rise of the Internet and the wide availability of inexpensive computers have made it possible to gather and analyse network data on an unprecendented scale, and the development of new theoretical tools has allowed us to extract knowledge from networks of many different kinds. The study of networks is broadly interdisciplinary and developments have occurred in many fields, including mathematics, physics, computer and information sciences, biology, and the social science. This book brings together the most important breakthroughts in each of these fields and presents them in a unified fashion, highlighting the strong interconnections between work in different areas. Topics covered include the measurement of networks; methods for analysing network data, including methods developed in physics, statistics, and sociology; fundamentals of graph theory; computer algorithms, including spectral algorithms and community detection; mathematical models of networks such as random graph models and generative models; and models of processes taking place on networks.},
  isbn = {978-0-19-880509-0},
  langid = {english},
  file = {/home/elessar/Zotero/storage/A6IN39B3/Newman - 2018 - Networks.pdf}
}

@misc{nguyenVariationalDeepLearning2021,
  title = {Variational {{Deep Learning}} for the {{Identification}} and {{Reconstruction}} of {{Chaotic}} and {{Stochastic Dynamical Systems}} from {{Noisy}} and {{Partial Observations}}},
  author = {Nguyen, Duong and Ouala, Said and Drumetz, Lucas and Fablet, Ronan},
  year = {2021},
  month = feb,
  number = {arXiv:2009.02296},
  eprint = {2009.02296},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-05-30},
  abstract = {The data-driven recovery of the unknown governing equations of dynamical systems has recently received an increasing interest. However, the identification of governing equations remains challenging when dealing with noisy and partial observations. Here, we address this challenge and investigate variational deep learning schemes. Within the proposed framework, we jointly learn an inference model to reconstruct the true states of the system and the governing laws of these states from series of noisy and partial data. In doing so, this framework bridges classical data assimilation and state-of-the-art machine learning techniques. We also demonstrate that it generalises stateof-the-art methods. Importantly, both the inference model and the governing model embed stochastic components to account for stochastic variabilities, model errors, and reconstruction uncertainties. Various experiments on chaotic and stochastic dynamical systems support the relevance of our scheme w.r.t. state-of-the-art approaches.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {GSCC: 0000016},
  file = {/home/elessar/Zotero/storage/49WCE8EJ/Nguyen et al. - 2021 - Variational Deep Learning for the Identification a.pdf}
}

@article{nobeReversibilityCellularAutomata2004,
  title = {On Reversibility of Cellular Automata with Periodic Boundary Conditions},
  author = {Nobe, Atsushi and Yura, Fumitaka},
  year = {2004},
  month = jun,
  journal = {Journal of Physics A: Mathematical and General},
  volume = {37},
  number = {22},
  pages = {5789--5804},
  publisher = {IOP Publishing},
  issn = {0305-4470, 1361-6447},
  doi = {10.1088/0305-4470/37/22/006},
  urldate = {2025-07-25},
  abstract = {Reversibility of one-dimensional cellular automata with periodic boundary conditions is discussed. It is shown that there exist exactly 16 reversible elementary cellular automaton rules for infinitely many cell sizes by means of a correspondence between elementary cellular automaton and the de Bruijn graph. In addition, a sufficient condition for reversibility of three-valued and two-neighbour cellular automaton is given.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/MHFH7NZU/Nobe and Yura - 2004 - On reversibility of cellular automata with periodic boundary conditions.pdf}
}

@article{nortonAllShookFluctuations2013,
  title = {All {{Shook Up}}: {{Fluctuations}}, {{Maxwell}}'s {{Demon}} and the {{Thermodynamics}} of {{Computation}}},
  shorttitle = {All {{Shook Up}}},
  author = {Norton, John},
  year = {2013},
  month = oct,
  journal = {Entropy},
  volume = {15},
  number = {10},
  pages = {4432--4483},
  publisher = {MDPI AG},
  issn = {1099-4300},
  doi = {10.3390/e15104432},
  urldate = {2025-07-25},
  abstract = {The most successful exorcism of Maxwell's demon is Smoluchowski's 1912 observation that thermal fluctuations would likely disrupt the operation of any molecular-scale demonic machine. A later tradition sought to exorcise Maxwell's demon by assessing the entropic cost of the demon's processing of information. This later tradition fails since these same thermal fluctuations invalidate the molecular-scale manipulations upon which the thermodynamics of computation is based. A new argument concerning conservation of phase space volume shows that all Maxwell's demons must fail.},
  copyright = {https://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/TR9DVXTC/Norton - 2013 - All Shook Up Fluctuations, Maxwell’s Demon and the Thermodynamics of Computation.pdf}
}

@book{nortonBeginningPython2005,
  title = {Beginning {{Python}}},
  editor = {Norton, Peter},
  year = {2005},
  series = {Programmer to Programmer},
  publisher = {Wrox},
  address = {Indianapolis, Ind},
  isbn = {978-0-7645-9654-4},
  langid = {english},
  annotation = {GSCC: 0000152},
  file = {/home/elessar/Zotero/storage/39CGBQC7/Norton - 2005 - Beginning Python.pdf}
}

@article{ohernJammingZeroTemperature2003,
  title = {Jamming at Zero Temperature and Zero Applied Stress: {{The}} Epitome of Disorder},
  shorttitle = {Jamming at Zero Temperature and Zero Applied Stress},
  author = {O'Hern, Corey S. and Silbert, Leonardo E. and Liu, Andrea J. and Nagel, Sidney R.},
  year = {2003},
  month = jul,
  journal = {Physical Review E},
  volume = {68},
  number = {1},
  publisher = {American Physical Society (APS)},
  issn = {1063-651X, 1095-3787},
  doi = {10.1103/physreve.68.011306},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/FCEUE6M4/O’Hern et al. - 2003 - Jamming at zero temperature and zero applied stress The epitome of disorder.pdf}
}

@inproceedings{ojaIndependentComponentAnalysis,
  title = {Independent Component Analysis for Financial Time Series},
  booktitle = {Proceedings of the {{IEEE}} 2000 {{Adaptive Systems}} for {{Signal Processing}}, {{Communications}}, and {{Control Symposium}} ({{Cat}}. {{No}}.{{00EX373}})},
  author = {Oja, E. and Kiviluoto, K. and Malaroiu, S.},
  pages = {111--116},
  publisher = {IEEE},
  address = {Lake Louise, Alta., Canada},
  doi = {10.1109/asspcc.2000.882456},
  urldate = {2025-07-25},
  langid = {english},
  file = {/home/elessar/Zotero/storage/4TDFIRTQ/Oja et al. - Independent component analysis for financial time series.pdf}
}

@misc{omidshafieiScalableAcceleratedDecentralized2017,
  title = {Scalable {{Accelerated Decentralized Multi-Robot Policy Search}} in {{Continuous Observation Spaces}}},
  author = {Omidshafiei, Shayegan and Amato, Christopher and Liu, Miao and Everett, Michael and How, Jonathan P. and Vian, John},
  year = {2017},
  month = mar,
  number = {arXiv:1703.05626},
  eprint = {1703.05626},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-10},
  abstract = {This paper presents the first ever approach for solving continuous-observation Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) and their semiMarkovian counterparts, Dec-POSMDPs. This contribution is especially important in robotics, where a vast number of sensors provide continuous observation data. A continuous-observation policy representation is introduced using Stochastic Kernelbased Finite State Automata (SK-FSAs). An SK-FSA search algorithm titled Entropy-based Policy Search using Continuous Kernel Observations (EPSCKO) is introduced and applied to the first ever continuous-observation Dec-POMDP/DecPOSMDP domain, where it significantly outperforms stateof-the-art discrete approaches. This methodology is equally applicable to Dec-POMDPs and Dec-POSMDPs, though the empirical analysis presented focuses on Dec-POSMDPs due to their higher scalability. To improve convergence, an entropy injection policy search acceleration approach for both continuous and discrete observation cases is also developed and shown to improve convergence rates without degrading policy quality.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Multiagent Systems,Computer Science - Robotics},
  annotation = {GSCC: 0000009},
  file = {/home/elessar/Zotero/storage/9JT2TWAE/Omidshafiei et al. - 2017 - Scalable Accelerated Decentralized Multi-Robot Pol.pdf}
}

@article{ornsteinEntropyDataCompression1993,
  title = {Entropy and Data Compression Schemes},
  author = {Ornstein, D.S. and Weiss, B.},
  year = {1993},
  journal = {IEEE Transactions on Information Theory},
  volume = {39},
  number = {1},
  pages = {78--83},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9448},
  doi = {10.1109/18.179344},
  urldate = {2025-07-15},
  abstract = {Some new ways of defining the entropy of a process by observing a single typical output sequence as well as a new kind of Shannon-McMillan-Breiman theorem are presented. Here are two sample results: 1) For a stationary ergodic process let R, ( E ) = inf\{lc 2 n : Ek+lEk+2 . ..Ekfn = E I E Z ...E,, 1, then a.s. limn-- (logR,({$<$}))/n = entropy of the process. 2) In the Lempel-Ziv parsing, a.s. for n sufficiently large most of (1 ...En has been parsed into blocks of size roughly, ( l o g n ) / h , ,where h is the entropy of the process.\vphantom\}},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/THIMYF9N/Ornstein and Weiss - 1993 - Entropy and data compression schemes.pdf}
}

@article{orvietoResurrectingRecurrentNeural,
  title = {Resurrecting {{Recurrent Neural Networks}} for {{Long Sequences}}},
  author = {Orvieto, Antonio and Smith, Samuel L and Gu, Albert and Fernando, Anushan and Gulcehre, Caglar and Pascanu, Razvan and De, Soham},
  abstract = {Recurrent Neural Networks (RNNs) offer fast inference on long sequences but are hard to optimize and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably well on long sequence modeling tasks, and have the added benefits of fast parallelizable training and RNN-like fast inference. However, while SSMs are superficially similar to RNNs, there are important differences that make it unclear where their performance boost over RNNs comes from. We show that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, while matching their training speed. To achieve this, we analyze and ablate a series of changes to standard RNNs including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring careful normalization of the forward pass. Our results provide new insights on the origins of the impressive performance of deep SSMs, and introduce an RNN block called the Linear Recurrent Unit (or LRU) that matches both their performance on the Long Range Arena benchmark and their computational efficiency.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/JW8VHXGS/Orvieto et al. - Resurrecting Recurrent Neural Networks for Long Sequences.pdf}
}

@article{ostruszkaDynamicalEntropySystems2000,
  title = {Dynamical Entropy for Systems with Stochastic Perturbation},
  author = {Ostruszka, Andrzej and Pako{\'n}ski, Prot and S{\l}omczy{\'n}ski, Wojciech and {\.Z}yczkowski, Karol},
  year = {2000},
  month = aug,
  journal = {Physical Review E},
  volume = {62},
  number = {2},
  pages = {2018--2029},
  publisher = {American Physical Society (APS)},
  issn = {1063-651X, 1095-3787},
  doi = {10.1103/physreve.62.2018},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/UKPMFQM6/Ostruszka et al. - 2000 - Dynamical entropy for systems with stochastic perturbation.pdf}
}

@article{ottAttractorReconstructionMachine2018,
  title = {Attractor Reconstruction by Machine Learning},
  author = {Ott, Edward and Lu, Zhixin and Hunt, Brian R.},
  year = {2018},
  month = jun,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {28},
  number = {6},
  pages = {061104},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.5039508},
  urldate = {2023-05-30},
  langid = {english},
  annotation = {GSCC: 0000357},
  file = {/home/elessar/Zotero/storage/CIUZ7MBR/Lu et al. - 2018 - Attractor reconstruction by machine learning.pdf;/home/elessar/Zotero/storage/TKVRDVQ6/Lu et al. - 2018 - Attractor reconstruction by machine learning.pdf}
}

@book{ottChaosDynamicalSystems2002,
  title = {Chaos in {{Dynamical Systems}}},
  author = {Ott, Edward},
  year = {2002},
  month = aug,
  edition = {2},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511803260},
  urldate = {2025-08-12},
  abstract = {Over the past two decades scientists, mathematicians, and engineers have come to understand that a large variety of systems exhibit complicated evolution with time. This complicated behavior is known as chaos. In the new edition of this classic textbook Edward Ott has added much new material and has significantly increased the number of homework problems. The most important change is the addition of a completely new chapter on control and synchronization of chaos. Other changes include new material on riddled basins of attraction, phase locking of globally coupled oscillators, fractal aspects of fluid advection by Lagrangian chaotic flows, magnetic dynamos, and strange nonchaotic attractors. This new edition will be of interest to advanced undergraduates and graduate students in science, engineering, and mathematics taking courses in chaotic dynamics, as well as to researchers in the subject.},
  copyright = {https://www.cambridge.org/core/terms},
  isbn = {978-0-521-81196-5 978-0-521-01084-9 978-0-511-80326-0}
}

@article{ottDIMENSIONCHAOTICATTRACTORS,
  title = {{{THE DIMENSION OF CHAOTIC ATTRACTORS}}},
  author = {Ott, Edward and Farmer, J Doyne},
  langid = {english},
  annotation = {GSCC: 0001841},
  file = {/home/elessar/Zotero/storage/TVM6E5QB/Farmer and Ott - THE DIMENSION OF CHAOTIC ATTRACTORS.pdf}
}

@article{ottSeparationChaoticSignals2020,
  title = {Separation of Chaotic Signals by Reservoir Computing},
  author = {Ott, Edward and Krishnagopal, Sanjukta and Girvan, Michelle and Hunt, Brian R.},
  year = {2020},
  month = feb,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {30},
  number = {2},
  pages = {023123},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.5132766},
  urldate = {2023-05-30},
  abstract = {We demonstrate the utility of machine learning in the separation of superimposed chaotic signals using a technique called reservoir computing. We assume no knowledge of the dynamical equations that produce the signals and require only training data consisting of finite-time samples of the component signals. We test our method on signals that are formed as linear combinations of signals from two Lorenz systems with different parameters. Comparing our nonlinear method with the optimal linear solution to the separation problem, the Wiener filter, we find that our method significantly outperforms the Wiener filter in all the scenarios we study. Furthermore, this difference is particularly striking when the component signals have similar frequency spectra. Indeed, our method works well when the component frequency spectra are indistinguishable---a case where a Wiener filter performs essentially no separation.},
  langid = {english},
  annotation = {GSCC: 0000048},
  file = {/home/elessar/Zotero/storage/5V9WKHIG/Krishnagopal et al. - 2020 - Separation of chaotic signals by reservoir computi.pdf;/home/elessar/Zotero/storage/GKWGHAFD/Krishnagopal et al_2020_Separation of chaotic signals by reservoir computing.pdf}
}

@article{ottUsingMachineLearning2017,
  title = {Using {{Machine Learning}} to {{Replicate Chaotic Attractors}} and {{Calculate Lyapunov Exponents}} from {{Data}}},
  author = {Ott, Edward and Pathak, Jaideep and Lu, Zhixin and Hunt, Brian R. and Girvan, Michelle},
  year = {2017},
  month = dec,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {27},
  number = {12},
  eprint = {1710.07313},
  primaryclass = {nlin},
  pages = {121102},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.5010300},
  urldate = {2023-05-30},
  abstract = {We use recent advances in the machine learning area known as 'reservoir computing' to formulate a method for model-free estimation from data of the Lyapunov exponents of a chaotic process. The technique uses a limited time series of measurements as input to a high-dimensional dynamical system called a 'reservoir'. After the reservoir's response to the data is recorded, linear regression is used to learn a large set of parameters, called the 'output weights'. The learned output weights are then used to form a modified autonomous reservoir designed to be capable of producing arbitrarily long time series whose ergodic properties approximate those of the input signal. When successful, we say that the autonomous reservoir reproduces the attractor's 'climate'. Since the reservoir equations and output weights are known, we can compute derivatives needed to determine the Lyapunov exponents of the autonomous reservoir, which we then use as estimates of the Lyapunov exponents for the original input generating system. We illustrate the effectiveness of our technique with two examples, the Lorenz system, and the Kuramoto-Sivashinsky (KS) equation. In particular, we use the Lorenz system to show that achieving climate reproduction may require tuning of the reservoir parameters. For the case of the KS equation, we note that as the system's spatial size is increased, the number of Lyapunov exponents increases, thus yielding a challenging test of our method, which we find the method successfully passes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Nonlinear Sciences - Chaotic Dynamics},
  annotation = {GSCC: 0000603},
  file = {/home/elessar/Zotero/storage/JDIZANFV/Pathak et al. - 2017 - Using Machine Learning to Replicate Chaotic Attrac.pdf}
}

@article{ottUsingMachineLearning2021,
  title = {Using Machine Learning to Predict Statistical Properties of Non-Stationary Dynamical Processes: {{System}} Climate,Regime Transitions, and the Effect of Stochasticity},
  shorttitle = {Using Machine Learning to Predict Statistical Properties of Non-Stationary Dynamical Processes},
  author = {Ott, Edward and Patel, Dhruvit and Canaday, Daniel and Girvan, Michelle and Pomerance, Andrew},
  year = {2021},
  month = mar,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {31},
  number = {3},
  pages = {033149},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0042598},
  urldate = {2023-05-30},
  abstract = {We develop and test machine learning techniques for successfully using past state time series data and knowledge of a time-dependent system parameter to predict the evolution of the ``climate'' associated with the long-term behavior of a non-stationary dynamical system, where the non-stationary dynamical system is itself unknown. By the term climate, we mean the statistical properties of orbits rather than their precise trajectories in time. By the term non-stationary, we refer to systems that are, themselves, varying with time. We show that our methods perform well on test systems predicting both continuous gradual climate evolution as well as relatively sudden climate changes (which we refer to as ``regime transitions''). We consider not only noiseless (i.e., deterministic) non-stationary dynamical systems, but also climate prediction for non-stationary dynamical systems subject to stochastic forcing (i.e., dynamical noise), and we develop a method for handling this latter case. The main conclusion of this paper is that machine learning has great promise as a new and highly effective approach to accomplishing data driven prediction of non-stationary systems.},
  langid = {english},
  annotation = {GSCC: 0000059},
  file = {/home/elessar/Zotero/storage/FM533QL5/Patel et al. - 2021 - Using machine learning to predict statistical prop.pdf}
}

@article{ozturkAnalysisDesignEcho2007,
  title = {Analysis and {{Design}} of {{Echo State Networks}}},
  author = {Ozturk, Mustafa C. and Xu, Dongming and Pr{\'i}ncipe, Jos{\'e} C.},
  year = {2007},
  month = jan,
  journal = {Neural Computation},
  volume = {19},
  number = {1},
  pages = {111--138},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2007.19.1.111},
  urldate = {2025-04-21},
  abstract = {The design of echo state network (ESN) parameters relies on the selection of the maximum eigenvalue of the linearized system around zero (spectral radius). However, this procedure does not quantify in a systematic manner the performance of the ESN in terms of approximation error. This article presents a functional space approximation framework to better understand the operation of ESNs and proposes an information-theoretic metric, the average entropy of echo states, to assess the richness of the ESN dynamics. Furthermore, it provides an interpretation of the ESN dynamics rooted in system theory as families of coupled linearized systems whose poles move according to the input signal dynamics. With this interpretation, a design methodology for functional approximation is put forward where ESNs are designed with uniform pole distributions covering the frequency spectrum to abide by the richness metric, irrespective of the spectral radius. A single bias parameter at the ESN input, adapted with the modeling error, configures the ESN spectral radius to the input-output joint space. Function approximation examples compare the proposed design methodology versus the conventional design.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/VFSGLGUV/Ozturk et al. - 2007 - Analysis and Design of Echo State Networks.pdf}
}

@article{packardGeometryTimeSeries1980,
  title = {Geometry from a {{Time Series}}},
  author = {Packard, N. H. and Crutchfield, J. P. and Farmer, J. D. and Shaw, R. S.},
  year = {1980},
  month = sep,
  journal = {Physical Review Letters},
  volume = {45},
  number = {9},
  pages = {712--716},
  publisher = {American Physical Society (APS)},
  issn = {0031-9007},
  doi = {10.1103/physrevlett.45.712},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/N9AHAEU7/Packard et al. - 1980 - Geometry from a Time Series.pdf}
}

@misc{palEstimationRenyiEntropy2010,
  title = {Estimation of {{R{\'e}nyi Entropy}} and {{Mutual Information Based}} on {{Generalized Nearest-Neighbor Graphs}}},
  author = {P{\'a}l, D{\'a}vid and P{\'o}czos, Barnab{\'a}s and Szepesv{\'a}ri, Csaba},
  year = {2010},
  month = oct,
  number = {arXiv:1003.1954},
  eprint = {1003.1954},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1003.1954},
  urldate = {2025-07-25},
  abstract = {We present simple and computationally efficient nonparametric estimators of Re{\textasciiacute}nyi entropy and mutual information based on an i.i.d. sample drawn from an unknown, absolutely continuous distribution over Rd. The estimators are calculated as the sum of p-th powers of the Euclidean lengths of the edges of the `generalized nearest-neighbor' graph of the sample and the empirical copula of the sample respectively. For the first time, we prove the almost sure consistency of these estimators and upper bounds on their rates of convergence, the latter of which under the assumption that the density underlying the sample is Lipschitz continuous. Experiments demonstrate their usefulness in independent subspace analysis.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Statistics - Machine Learning},
  file = {/home/elessar/Zotero/storage/TMK6G9A6/Pál et al. - 2010 - Estimation of Rényi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs.pdf}
}

@article{panahiAdaptableReservoirComputing2024,
  title = {Adaptable Reservoir Computing: {{A}} Paradigm for Model-Free Data-Driven Prediction of Critical Transitions in Nonlinear Dynamical Systems},
  shorttitle = {Adaptable Reservoir Computing},
  author = {Panahi, Shirin and Lai, Ying-Cheng},
  year = {2024},
  month = may,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {34},
  number = {5},
  pages = {051501},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0200898},
  urldate = {2025-08-12},
  abstract = {A problem in nonlinear and complex dynamical systems with broad applications is forecasting the occurrence of a critical transition based solely on data without knowledge about the system equations. When such a transition leads to system collapse, as often is the case, all the available data are from the pre-critical regime where the system still functions normally, making the prediction problem challenging. In recent years, a machine-learning based approach tailored to solving this difficult prediction problem, adaptable reservoir computing, has been articulated. This Perspective introduces the basics of this machine-learning scheme and describes representative results. The general setting is that the system dynamics live on a normal attractor with oscillatory dynamics at the present time and, as a bifurcation parameter changes into the future, a critical transition can occur after which the system switches to a completely different attractor, signifying system collapse. To predict a critical transition, it is essential that the reservoir computer not only learns the dynamical ``climate'' of the system of interest at some specific parameter value but, more importantly, discovers how the system dynamics changes with the bifurcation parameter. It is demonstrated that this capability can be endowed into the machine through a training process with time series from a small number of distinct, pre-critical parameter values, thereby enabling accurate and reliable prediction of the catastrophic critical transition. Three applications are presented: predicting crisis, forecasting amplitude death, and creating digital twins of nonlinear dynamical systems. Limitations and future perspectives are discussed.},
  langid = {english}
}

@article{paninskiEstimationEntropyMutual2003,
  title = {Estimation of {{Entropy}} and {{Mutual Information}}},
  author = {Paninski, Liam},
  year = {2003},
  month = jun,
  journal = {Neural Computation},
  volume = {15},
  number = {6},
  pages = {1191--1253},
  publisher = {MIT Press - Journals},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976603321780272},
  urldate = {2025-07-25},
  abstract = {We present some new results on the nonparametric estimation of entropy and mutual information. First, we use an exact local expansion of the entropy function to prove almost sure consistency and central limit theorems for three of the most commonly used discretized information estimators. The setup is related to Grenander's method of sieves and places no assumptions on the underlying probability measure generating the data. Second, we prove a converse to these consistency theorems, demonstrating that a misapplication of the most common estimation techniques leads to an arbitrarily poor estimate of the true information, even given unlimited data. This ``inconsistency'' theorem leads to an analytical approximation of the bias, valid in surprisingly small sample regimes and more accurate than the usual [Formula: see text] formula of Miller and Madow over a large region of parameter space. The two most practical implications of these results are negative: (1) information estimates in a certain data regime are likely contaminated by bias, even if ``bias-corrected'' estimators are used, and (2) confidence intervals calculated by standard techniques drastically underestimate the error of the most common estimation methods.  Finally, we note a very useful connection between the bias of entropy estimators and a certain polynomial approximation problem. By casting bias calculation problems in this approximation theory framework, we obtain the best possible generalization of known asymptotic bias results. More interesting, this framework leads to an estimator with some nice properties: the estimator comes equipped with rigorous bounds on the maximum error over all possible underlying probability distributions, and this maximum error turns out to be surprisingly small. We demonstrate the application of this new estimator on both real and simulated data.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/GZXI87GC/Paninski - 2003 - Estimation of Entropy and Mutual Information.pdf}
}

@article{panLookingAllPalindromes,
  title = {Looking for {{All Palindromes}} in a {{String}}},
  author = {Pan, Shih Jang and Lee, R C T},
  abstract = {A palindrome is a string of the form {$\alpha\alpha$}', where {$\alpha$} and {$\alpha$}' are also strings and reverse to each other. The problem of the paper is defined as follows: given a string S of length n, find all palindromes occurring in the given string S. In the paper, we present an algorithm based on suffix trees to find palindromes.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/DJM95YHQ/Pan and Lee - Looking for All Palindromes in a String.pdf}
}

@article{patanApproximationStatespaceTrajectories2008,
  title = {Approximation of State-Space Trajectories by Locally Recurrent Globally Feed-Forward Neural Networks},
  author = {Patan, Krzysztof},
  year = {2008},
  month = jan,
  journal = {Neural Networks},
  volume = {21},
  number = {1},
  pages = {59--64},
  issn = {08936080},
  doi = {10.1016/j.neunet.2007.10.004},
  urldate = {2023-05-30},
  abstract = {The paper deals with investigating approximation abilities of a special class of discrete-time dynamic neural networks. The networks considered are called locally recurrent globally feed-forward, because they are designed with dynamic neuron models which contain inner feedbacks, but interconnections between neurons are strict feed-forward ones like in the well-known multi-layer perceptron. The paper presents analytical results showing that a locally recurrent network with two hidden layers is able to approximate a state-space trajectory produced by any Lipschitz continuous function with arbitrary accuracy. Moreover, based on these results, the network can be simplified and transformed into a more practical structure needed in real world applications.},
  langid = {english},
  annotation = {GSCC: 0000023},
  file = {/home/elessar/Zotero/storage/KUFYQ8JZ/Patan - 2008 - Approximation of state-space trajectories by local.pdf}
}

@article{patelUsingMachineLearning2023,
  title = {Using Machine Learning to Anticipate Tipping Points and Extrapolate to Post-Tipping Dynamics of Non-Stationary Dynamical Systems},
  author = {Patel, Dhruvit and Ott, Edward},
  year = {2023},
  month = feb,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {33},
  number = {2},
  pages = {023143},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0131787},
  urldate = {2025-08-12},
  abstract = {The ability of machine learning (ML) models to ``extrapolate'' to situations outside of the range spanned by their training data is crucial for predicting the long-term behavior of non-stationary dynamical systems (e.g., prediction of terrestrial climate change), since the future trajectories of such systems may (perhaps after crossing a tipping point) explore regions of state space which were not explored in past time-series measurements used as training data. We investigate the extent to which ML methods can yield useful results by extrapolation of such training data in the task of forecasting non-stationary dynamics, as well as conditions under which such methods fail. In general, we find that ML can be surprisingly effective even in situations that might appear to be extremely challenging, but do (as one would expect) fail when ``too much'' extrapolation is required. For the latter case, we show that good results can potentially be obtained by combining the ML approach with an available inaccurate conventional model based on scientific knowledge.},
  langid = {english}
}

@article{pathakModelFreePredictionLarge2018,
  title = {Model-{{Free Prediction}} of {{Large Spatiotemporally Chaotic Systems}} from {{Data}}: {{A Reservoir Computing Approach}}},
  shorttitle = {Model-{{Free Prediction}} of {{Large Spatiotemporally Chaotic Systems}} from {{Data}}},
  author = {Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Lu, Zhixin and Ott, Edward},
  year = {2018},
  month = jan,
  journal = {Physical Review Letters},
  volume = {120},
  number = {2},
  pages = {024102},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.120.024102},
  urldate = {2025-03-17},
  langid = {english},
  file = {/home/elessar/Zotero/storage/2JM7DAL8/Pathak et al. - 2018 - Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data A Reservoir Computing App.pdf;/home/elessar/Zotero/storage/85T8VAAF/aps_supplement.pdf}
}

@article{pedersenContinuousTransitionsCellular,
  title = {Continuous {{Transitions}} of {{Cellular Automata}}},
  author = {Pedersen, John},
  abstract = {A method of continuously deforming one cellular automaton rule into another is presented. Transitions between behaviors under this model are found to share some characteristics of the dynamics of iter ated self-maps.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/59FU578V/Pedersen - Continuous Transitions of Cellular Automata.pdf}
}

@book{percusComputationalComplexityStatistical2006,
  title = {Computational Complexity and Statistical Physics},
  editor = {Percus, Allon and Istrate, Gabriel and Moore, Cristopher},
  year = {2006},
  series = {Santa {{Fe Institute}} Studies in the Sciences of Complexity},
  publisher = {Oxford University Press},
  address = {Oxford New York},
  isbn = {978-0-19-517737-4 978-0-19-976056-5},
  langid = {english},
  file = {/home/elessar/Zotero/storage/VRBJ3HFS/Percus et al. - 2006 - Computational complexity and statistical physics.pdf}
}

@misc{petzEntropyNeumannNeumann2001,
  title = {Entropy, von {{Neumann}} and the von {{Neumann}} Entropy},
  author = {Petz, D.},
  year = {2001},
  month = feb,
  number = {arXiv:math-ph/0102013},
  eprint = {math-ph/0102013},
  publisher = {arXiv},
  doi = {10.48550/arXiv.math-ph/0102013},
  urldate = {2025-07-25},
  abstract = {This paper is an introduction to the von Neumann entropy in a historic approach. Von Neumann's gedanken experiment is repeated, which led him to the formula of thermodynamic entropy of a statistical operator. In the analysis of his ideas we stress the role of superselection sectors and summarize von Neumann's knowledge about quantum mechanical entropy. The final part of the paper is devoted to important developments discovered long after von Neumann's work. Subadditivity and the interpretation of the von Neumann entropy as channel capacity are among those.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Mathematical Physics,Mathematics - Mathematical Physics},
  file = {/home/elessar/Zotero/storage/HAYAJ8CS/Petz - 2001 - Entropy, von Neumann and the von Neumann entropy.pdf}
}

@misc{pfanderSamplingOperators2010,
  title = {Sampling of Operators},
  author = {Pfander, G{\"o}tz E.},
  year = {2010},
  month = oct,
  number = {arXiv:1010.6165},
  eprint = {1010.6165},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1010.6165},
  urldate = {2025-07-15},
  abstract = {Sampling and reconstruction of functions is a central tool in science. A key result is given by the sampling theorem for bandlimited functions attributed to Whittaker, Shannon, Nyquist, and Kotelnikov. We develop an analogous sampling theory for operators which we call bandlimited if their Kohn-Nirenberg symbols are bandlimited. We prove sampling theorems for such operators and show that they are extensions of the classical sampling theorem.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Mathematics - Classical Analysis and ODEs,Mathematics - Functional Analysis,Mathematics - Information Theory},
  file = {/home/elessar/Zotero/storage/D7G6XETV/Pfander - 2010 - Sampling of operators.pdf}
}

@article{piaKneadingTheoryCircle1986,
  title = {Kneading Theory of the Circle Map},
  author = {Pia, E.},
  year = {1986},
  month = jul,
  journal = {Physical Review A},
  volume = {34},
  number = {1},
  pages = {574--581},
  issn = {0556-2791},
  doi = {10.1103/PhysRevA.34.574},
  urldate = {2025-07-29},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/BUWUH7AT/Pia - 1986 - Kneading theory of the circle map.pdf}
}

@article{pikovskyNewTypeIntermittent1983,
  title = {A New Type of Intermittent Transition to Chaos},
  author = {Pikovsky, A S},
  year = {1983},
  month = mar,
  journal = {Journal of Physics A: Mathematical and General},
  volume = {16},
  number = {4},
  pages = {L109-L112},
  publisher = {IOP Publishing},
  issn = {0305-4470, 1361-6447},
  doi = {10.1088/0305-4470/16/4/002},
  urldate = {2025-07-15},
  abstract = {An intermittent transition to chaos in the presence of symmetry is investigated. Statistical properties of intermittency are found, including the case of external noise.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/99TXE9NY/Pikovsky - 1983 - A new type of intermittent transition to chaos.pdf}
}

@article{plappMultiscaleRandomWalkAlgorithm2000,
  title = {Multiscale {{Random-Walk Algorithm}} for {{Simulating Interfacial Pattern Formation}}},
  author = {Plapp, Mathis and Karma, Alain},
  year = {2000},
  month = feb,
  journal = {Physical Review Letters},
  volume = {84},
  number = {8},
  pages = {1740--1743},
  publisher = {American Physical Society (APS)},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/physrevlett.84.1740},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/QJWABWQ4/Plapp and Karma - 2000 - Multiscale Random-Walk Algorithm for Simulating Interfacial Pattern Formation.PDF}
}

@article{plattConstrainingChaosEnforcing2023,
  title = {Constraining Chaos: {{Enforcing}} Dynamical Invariants in the Training of Reservoir Computers},
  shorttitle = {Constraining Chaos},
  author = {Platt, Jason A. and Penny, Stephen G. and Smith, Timothy A. and Chen, Tse-Chun and Abarbanel, Henry D. I.},
  year = {2023},
  month = oct,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {33},
  number = {10},
  pages = {103107},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0156999},
  urldate = {2025-08-12},
  abstract = {Drawing on ergodic theory, we introduce a novel training method for machine learning based forecasting methods for chaotic dynamical systems. The training enforces dynamical invariants---such as the Lyapunov exponent spectrum and the fractal dimension---in the systems of interest, enabling longer and more stable forecasts when operating with limited data. The technique is demonstrated in detail using reservoir computing, a specific kind of recurrent neural network. Results are given for the Lorenz 1996 chaotic dynamical system and a spectral quasi-geostrophic model of the atmosphere, both typical test cases for numerical weather prediction.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/MUDCTHDR/Platt et al. - 2023 - Constraining chaos Enforcing dynamical invariants in the training of reservoir computers.pdf}
}

@misc{plattForecastingUsingReservoir2021,
  title = {Forecasting {{Using Reservoir Computing}}: {{The Role}} of {{Generalized Synchronization}}},
  shorttitle = {Forecasting {{Using Reservoir Computing}}},
  author = {Platt, Jason A. and Wong, Adrian and Clark, Randall and Penny, Stephen G. and Abarbanel, Henry D. I.},
  year = {2021},
  month = apr,
  number = {arXiv:2102.08930},
  eprint = {2102.08930},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-30},
  abstract = {Reservoir computers (RC) are a form of recurrent neural network (RNN) used for forecasting time series data. As with all RNNs, selecting the hyperparameters presents a challenge when training on new inputs. We present a method based on generalized synchronization (GS) that gives direction in designing and evaluating the architecture and hyperparameters of a RC. The 'auxiliary method' for detecting GS provides a pre-training test that guides hyperparameter selection. Furthermore, we provide a metric for a "well trained" RC using the reproduction of the input system's Lyapunov exponents.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation = {GSCC: 0000025},
  file = {/home/elessar/Zotero/storage/NULQI38J/Platt et al. - 2021 - Forecasting Using Reservoir Computing The Role of.pdf}
}

@article{plattRobustForecastingUsing2021,
  title = {Robust Forecasting Using Predictive Generalized Synchronization in Reservoir Computing},
  author = {Platt, Jason A. and Wong, Adrian and Clark, Randall and Penny, Stephen G. and Abarbanel, Henry D. I.},
  year = {2021},
  month = dec,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {31},
  number = {12},
  pages = {123118},
  issn = {1054-1500},
  doi = {10.1063/5.0066013},
  urldate = {2025-08-22},
  abstract = {Reservoir computers (RCs) are a class of recurrent neural networks (RNNs) that can be used for forecasting the future of observed time series data. As with all RNNs, selecting the hyperparameters in the network to yield excellent forecasting presents a challenge when training on new inputs. We analyze a method based on predictive generalized synchronization (PGS) that gives direction in designing and evaluating the architecture and hyperparameters of an RC. To determine the occurrences of PGS, we rely on the auxiliary method to provide a computationally efficient pre-training test that guides hyperparameter selection. We provide a metric for evaluating the RC using the reproduction of the input system's Lyapunov exponents that demonstrates robustness in prediction.},
  file = {/home/elessar/Zotero/storage/ZXP6S3W6/Platt et al. - 2021 - Robust forecasting using predictive generalized synchronization in reservoir computing.pdf;/home/elessar/Zotero/storage/JL999LL5/5.html}
}

@article{plotnikUpperBoundsProbability,
  title = {Upper {{Bounds}} on the {{Probability}} of {{Sequences Emitted}} by {{Finite-State Sources}} and on the {{Redundancy}} of the {{Lempel- Ziv Algorithm}}},
  author = {Plotnik, Eli and Weinberger, J and Ziv, Jacob},
  abstract = {An upper bound on the probability of a sequence drawn from a finite-state source is derived. The bound is given in terms of the number of phrases obtained by parsing the sequence according to the Lempel-Ziv (L-Z) incremental parsing rule, and is universal in the sense that it does not depend on the statistical parameters that characterize the source. This bound is used to derive an upper bound on the redundancy of the L-Z universal data compression algorithm applied to finite-state sources, that depends on the length N of the sequence, on the number K of states of the source, and, eventually, on the source entropy. A variation of the L-Z algorithm is presented, and an upper bound on its redundancy is derived for finite-state sources. A method to derive tighter implicit upper bounds on the redundancy of both algorithms is also given, and it is shown that for the proposed variation this bound is smaller than for the original L-Z algorithm, or every value of N and K .},
  langid = {english},
  file = {/home/elessar/Zotero/storage/SAV4L3Q3/Plotnik et al. - Upper Bounds on the Probability of Sequences Emitted by Finite-State Sources and on the Redundancy o.pdf}
}

@article{polandCodingTheoremEnumerable2004,
  title = {A Coding Theorem for {{Enumerable Output Machines}}},
  author = {Poland, Jan},
  year = {2004},
  month = aug,
  journal = {Information Processing Letters},
  volume = {91},
  number = {4},
  pages = {157--161},
  publisher = {Elsevier BV},
  issn = {0020-0190},
  doi = {10.1016/j.ipl.2004.05.002},
  urldate = {2025-07-15},
  abstract = {Recently, Schmidhuber proposed a new concept of generalized algorithmic complexity. It allows for the description of both finite and infinite sequences. The resulting distributions are true probabilities rather than semimeasures. We clarify some points for this setting, concentrating on Enumerable Output Machines. As our main result, we prove a strong coding theorem (without logarithmic correction terms), which was left as an open problem. To this purpose, we introduce a more natural definition of generalized complexity.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/JIIQEH5U/Poland - 2004 - A coding theorem for Enumerable Output Machines.pdf}
}

@misc{prater-bennetteRandomnessIsometriesEcho2018,
  title = {Randomness and Isometries in Echo State Networks and Compressed Sensing},
  author = {{Prater-Bennette}, Ashley},
  year = {2018},
  month = feb,
  number = {arXiv:1802.01381},
  eprint = {1802.01381},
  primaryclass = {cs, eess, math},
  publisher = {arXiv},
  urldate = {2023-05-30},
  abstract = {Although largely different concepts, echo state networks and compressed sensing models both rely on collections of random weights; as the reservoir dynamics for echo state networks, and the sensing coefficients in compressed sensing. Several methods for generating the random matrices and metrics to indicate desirable performance are well-studied in compressed sensing, but less so for echo state networks. This work explores any overlap in these compressed sensing methods and metrics for application to echo state networks. Several methods for generating the random reservoir weights are considered, and a new metric, inspired by the restricted isometry property for compressed sensing, is proposed for echo state networks. The methods and metrics are investigated theoretically and experimentally, with results suggesting that the same types of random matrices work well for both echo state network and compressed sensing scenarios, and that echo state network classification accuracy is improved when the proposed restricted isometry-like constants are close to 1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Electrical Engineering and Systems Science - Signal Processing},
  annotation = {GSCC: 0000001},
  file = {/home/elessar/Zotero/storage/P789H2IY/Prater-Bennette - 2018 - Randomness and isometries in echo state networks a.pdf;/home/elessar/Zotero/storage/UAPJ6SKG/Hyperparameter Tuning in Echo State Networks _ Enhanced Reader.pdf}
}

@article{prinzAdvancedEstimationMethods,
  title = {{Advanced estimation methods for  Markov models of dynamical systems}},
  author = {Prinz, Jan-Hendrik and Berlin, Freie Universit{\"a}t and Mathematik, Fachbereich},
  langid = {ngerman},
  file = {/home/elessar/Zotero/storage/EU38N2JK/Prinz et al. - Advanced estimation methods for  Markov models of dynamical systems.pdf}
}

@article{qianRelativeEntropyFree2001,
  title = {Relative Entropy:{$\quad$}{{Free}} Energy Associated with Equilibrium Fluctuations and Nonequilibrium Deviations},
  shorttitle = {Relative Entropy},
  author = {Qian, Hong},
  year = {2001},
  month = mar,
  journal = {Physical Review E},
  volume = {63},
  number = {4},
  publisher = {American Physical Society (APS)},
  issn = {1063-651X, 1095-3787},
  doi = {10.1103/physreve.63.042103},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/J6BZ4XVQ/Qian - 2001 - Relative entropy Free energy associated with equilibrium fluctuations and nonequilibrium deviations.pdf}
}

@article{qiaoGrowingEchoStateNetwork2017,
  title = {Growing {{Echo-State Network With Multiple Subreservoirs}}},
  author = {Qiao, Junfei and Li, Fanjun and Han, Honggui and Li, Wenjing},
  year = {2017},
  month = feb,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {28},
  number = {2},
  pages = {391--404},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2016.2514275},
  urldate = {2025-03-11},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  file = {/home/elessar/Zotero/storage/6QNREUHD/Qiao et al. - 2017 - Growing Echo-State Network With Multiple Subreservoirs.pdf}
}

@article{quillenPHY411LectureNotes,
  title = {{{PHY411 Lecture}} Notes {{Part}} 6},
  author = {Quillen, Alice},
  langid = {english},
  file = {/home/elessar/Zotero/storage/RFC7PXAH/Quillen - PHY411 Lecture notes Part 6.pdf}
}

@article{raccaRobustOptimizationValidation2021,
  title = {Robust {{Optimization}} and {{Validation}} of {{Echo State Networks}} for Learning Chaotic Dynamics},
  author = {Racca, Alberto and Magri, Luca},
  year = {2021},
  month = oct,
  journal = {Neural Networks},
  volume = {142},
  pages = {252--268},
  issn = {08936080},
  doi = {10.1016/j.neunet.2021.05.004},
  urldate = {2025-03-12},
  abstract = {An approach to the time-accurate prediction of chaotic solutions is by learning temporal patterns from data. Echo State Networks (ESNs), which are a class of Reservoir Computing, can accurately predict the chaotic dynamics well beyond the predictability time. Existing studies, however, also showed that small changes in the hyperparameters may markedly affect the network's performance. The overarching aim of this paper is to improve the robustness in the selection of hyperparameters in Echo State Networks for the time-accurate prediction of chaotic solutions. We define the robustness of a validation strategy as its ability to select hyperparameters that perform consistently between validation and test sets. The goal is three-fold. First, we investigate routinely used validation strategies. Second, we propose the Recycle Validation, and the chaotic versions of existing validation strategies, to specifically tackle the forecasting of chaotic systems. Third, we compare Bayesian optimization with the traditional grid search for optimal hyperparameter selection. Numerical tests are performed on prototypical nonlinear systems that have chaotic and quasiperiodic solutions, such as the Lorenz and Lorenz-96 systems, and the Kuznetsov oscillator. Both model-free and model-informed Echo State Networks are analysed. By comparing the networks' performance in learning chaotic (unpredictable) versus quasiperiodic (predictable) solutions, we highlight fundamental challenges in learning chaotic solutions. The proposed validation strategies, which are based on the dynamical systems properties of chaotic time series, are shown to outperform the state-of-the-art validation strategies. Because the strategies are principled -- they are based on chaos theory such as the Lyapunov time -- they can be applied to other Recurrent Neural Networks architectures with little modification. This work opens up new possibilities for the robust design and application of Echo State Networks, and Recurrent Neural Networks, to the time-accurate prediction of chaotic systems.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/3LUL6YR7/Racca and Magri - 2021 - Robust Optimization and Validation of Echo State Networks for learning chaotic dynamics.pdf}
}

@inproceedings{rackauckasComposingModelingSimulation2022,
  title = {Composing {{Modeling And Simulation With Machine Learning In Julia}}},
  booktitle = {2022 {{Annual Modeling}} and {{Simulation Conference}} ({{ANNSIM}})},
  author = {Rackauckas, Chris and Gwozdz, Maja and Jain, Anand and Ma, Yingbo and Martinuzzi, Francesco and Rajput, Utkarsh and Saba, Elliot and Shah, Viral B. and Anantharaman, Ranjan and Edelman, Alan and Gowda, Shashi and Pal, Avik and Laughman, Chris},
  year = {2022},
  month = jul,
  pages = {1--17},
  publisher = {IEEE},
  address = {San Diego, CA, USA},
  doi = {10.23919/ANNSIM55834.2022.9859453},
  urldate = {2025-08-12},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-7138-5288-9}
}

@article{rackauckasDifferentialEquationsjlPerformantFeatureRich2017,
  title = {{{DifferentialEquations}}.Jl -- {{A Performant}} and {{Feature-Rich Ecosystem}} for {{Solving Differential Equations}} in {{Julia}}},
  author = {Rackauckas, Christopher and Nie, Qing},
  year = {2017},
  month = may,
  journal = {Journal of Open Research Software},
  volume = {5},
  number = {1},
  pages = {15},
  issn = {2049-9647},
  doi = {10.5334/jors.151},
  urldate = {2025-08-12},
  copyright = {http://creativecommons.org/licenses/by/4.0}
}

@article{rajkovicQuantifyingComplexityMinority2003,
  title = {Quantifying Complexity in the Minority Game},
  author = {Rajkovi{\'c}, Milan and Mihailovi{\'c}, Zoran},
  year = {2003},
  month = jul,
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {325},
  number = {1-2},
  pages = {40--47},
  publisher = {Elsevier BV},
  issn = {0378-4371},
  doi = {10.1016/s0378-4371(03)00181-x},
  urldate = {2025-07-15},
  abstract = {A Lempel--Ziv complexity measure is introduced into the theory ofa minority game in order to capture some features that volatility, one ofthe central quantities in this model ofinteracting agents, is not able to. Extracted solely from the binary string ofoutcomes ofthe game complexity o0ers new and valuable information on collective behavior of players. Also, we show that an expression for volatility may be included in the analytical expression for complexity. c{\copyright} 2003 Elsevier Science B.V. All rights reserved.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/VEZNDWP6/Rajković and Mihailović - 2003 - Quantifying complexity in the minority game.pdf}
}

@book{randDynamicalSystemsTurbulence1981,
  title = {Dynamical {{Systems}} and {{Turbulence}}, {{Warwick}} 1980},
  editor = {Rand, David and Young, Lai-Sang},
  year = {1981},
  series = {Lecture {{Notes}} in {{Mathematics}}},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  issn = {0075-8434, 1617-9692},
  doi = {10.1007/bfb0091903},
  urldate = {2025-07-15},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-540-11171-9 978-3-540-38945-3},
  file = {/home/elessar/Zotero/storage/IF2MSZSQ/Rand and Young - 1981 - Dynamical Systems and Turbulence, Warwick 1980.pdf}
}

@book{ranjanUnderstandingDeepLearning,
  title = {Understanding {{Deep Learning Application}} in {{Rare Event Prediction}}},
  author = {Ranjan, Chitta},
  langid = {english},
  annotation = {GSCC: 0000035},
  file = {/home/elessar/Zotero/storage/RA323YJP/Ranjan - Understanding Deep Learning Application in Rare Ev.pdf}
}

@article{raoCumulativeResidualEntropy2004,
  title = {Cumulative {{Residual Entropy}}: {{A New Measure}} of {{Information}}},
  shorttitle = {Cumulative {{Residual Entropy}}},
  author = {Rao, M. and Chen, Y. and Vemuri, B.C. and Wang, F.},
  year = {2004},
  month = jun,
  journal = {IEEE Transactions on Information Theory},
  volume = {50},
  number = {6},
  pages = {1220--1228},
  issn = {0018-9448},
  doi = {10.1109/TIT.2004.828057},
  urldate = {2025-07-29},
  abstract = {In this paper, we use the cumulative distribution of a random variable to define its information content and thereby develop an alternative measure of uncertainty that extends Shannon entropy to random variables with continuous distributions. We call this measure cumulative residual entropy (CRE). The salient features of CRE are as follows: 1) it is more general than the Shannon entropy in that its definition is valid in the continuous and discrete domains, 2) it possesses more general mathematical properties than the Shannon entropy, and 3) it can be easily computed from sample data and these computations aymptotically converge to the true values. The properties of CRE and a precise formula relating CRE and Shannon entropy are given in the paper. Finally, we present some applications of CRE to reliability engineering and computer vision.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/64EI3URE/Rao et al. - 2004 - Cumulative Residual Entropy A New Measure of Information.pdf}
}

@inproceedings{raokosarajuEntropicBoundsLempelZiv1997,
  title = {Some Entropic Bounds for {{Lempel-Ziv}} Algorithms},
  booktitle = {Proceedings {{DCC}} '97. {{Data Compression Conference}}},
  author = {Rao Kosaraju, S. and Manzini, G.},
  year = {1997},
  pages = {446},
  publisher = {IEEE},
  address = {Snowbird, UT, USA},
  doi = {10.1109/dcc.1997.582106},
  urldate = {2025-07-15},
  langid = {english},
  file = {/home/elessar/Zotero/storage/9K9RD3QE/Rao Kosaraju and Manzini - 1997 - Some entropic bounds for Lempel-Ziv algorithms.pdf}
}

@article{rappEffectiveNormalizationComplexity2001,
  title = {Effective Normalization of Complexity Measurements for Epoch Length and Sampling Frequency},
  author = {Rapp, P. E. and Cellucci, C. J. and Korslund, K. E. and Watanabe, T. A. A. and {Jim{\'e}nez-Monta{\~n}o}, M. A.},
  year = {2001},
  month = jun,
  journal = {Physical Review E},
  volume = {64},
  number = {1},
  publisher = {American Physical Society (APS)},
  issn = {1063-651X, 1095-3787},
  doi = {10.1103/physreve.64.016209},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/RWTI3BEN/Rapp et al. - 2001 - Effective normalization of complexity measurements for epoch length and sampling frequency.pdf}
}

@misc{razenshteynCommonInformationRevisited2012,
  title = {Common Information Revisited},
  author = {Razenshteyn, Ilya},
  year = {2012},
  month = jun,
  number = {arXiv:1104.3207},
  eprint = {1104.3207},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1104.3207},
  urldate = {2025-07-25},
  abstract = {One of the main notions of information theory is the notion of mutual information in two messages (two random variables in Shannon information theory or two binary strings in algorithmic information theory). The mutual information in x and y measures how much the transmission of x can be simplified if both the sender and the recipient know y in advance. Ga{\textasciiacute}cs and Ko{\textasciidieresis}rner gave an example where mutual information cannot be presented as common information (a third message easily extractable from both x and y). Then this question was studied in the framework of algorithmic information theory by An. Muchnik and A. Romashchenko who found many other examples of this type. K. Makarychev and Yu. Makarychev found a new proof of Ga{\textasciiacute}cs--Ko{\textasciidieresis}rner results by means of conditionally independent random variables. The question about the difference between mutual and common information can be studied quantitatively: for a given x and y we look for three messages a, b, c such that a and c are enough to reconstruct x, while b and c are enough to reconstruct y.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Discrete Mathematics,Computer Science - Information Theory,Mathematics - Combinatorics,Mathematics - Information Theory},
  file = {/home/elessar/Zotero/storage/TBHHFB8U/Razenshteyn - 2012 - Common information revisited.pdf}
}

@article{renPerformanceImprovementChaotic2020,
  title = {Performance {{Improvement}} of {{Chaotic Baseband Wireless Communication Using Echo State Network}}},
  author = {Ren, Hai-Peng and Yin, Hui-Ping and Bai, Chao and Yao, Jun-Liang},
  year = {2020},
  month = oct,
  journal = {IEEE Transactions on Communications},
  volume = {68},
  number = {10},
  pages = {6525--6536},
  issn = {0090-6778, 1558-0857},
  doi = {10.1109/TCOMM.2020.3007757},
  urldate = {2025-03-12},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  file = {/home/elessar/Zotero/storage/7LSVFHTX/Ren et al. - 2020 - Performance Improvement of Chaotic Baseband Wireless Communication Using Echo State Network.pdf}
}

@article{restivoFinitelyGeneratedSofic1989,
  title = {Finitely Generated Sofic Systems},
  author = {Restivo, Antonio},
  year = {1989},
  month = jun,
  journal = {Theoretical Computer Science},
  volume = {65},
  number = {2},
  pages = {265--270},
  publisher = {Elsevier BV},
  issn = {0304-3975},
  doi = {10.1016/0304-3975(89)90049-2},
  urldate = {2025-07-15},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/UBIEGBLQ/Restivo - 1989 - Finitely generated sofic systems.pdf}
}

@misc{riechersPairwiseCorrelationsLayered2014,
  title = {Pairwise {{Correlations}} in {{Layered Close-Packed Structures}}},
  author = {Riechers, P. M. and Varn, D. P. and Crutchfield, J. P.},
  year = {2014},
  month = jul,
  number = {arXiv:1407.7159},
  eprint = {1407.7159},
  primaryclass = {cond-mat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1407.7159},
  urldate = {2025-07-15},
  abstract = {Given a description of the stacking statistics of layered close-packed structures in the form of a hidden Markov model, we develop analytical expressions for the pairwise correlation functions between the layers. These may be calculated analytically as explicit functions of model parameters or the expressions may be used as a fast, accurate, and efficient way to obtain numerical values. We present several examples, finding agreement with previous work as well as deriving new relations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Materials Science},
  file = {/home/elessar/Zotero/storage/FYKDRTCM/Riechers et al. - 2014 - Pairwise Correlations in Layered Close-Packed Structures.pdf}
}

@article{rigamontiEnsembleOptimizedEcho2018,
  title = {Ensemble of Optimized Echo State Networks for Remaining Useful Life Prediction},
  author = {Rigamonti, Marco and Baraldi, Piero and Zio, Enrico and Roychoudhury, Indranil and Goebel, Kai and Poll, Scott},
  year = {2018},
  month = mar,
  journal = {Neurocomputing},
  volume = {281},
  pages = {121--138},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2017.11.062},
  urldate = {2024-11-26},
  abstract = {The use of Echo State Networks (ESNs) for the prediction of the Remaining Useful Life (RUL) of industrial components, i.e. the time left before the equipment will stop fulfilling its functions, is attractive because of their capability of handling the system dynamic behavior, the measurement noise, and the stochasticity of the degradation process. In particular, in this paper we originally resort to an ensemble of ESNs, for enhancing the performances of individual ESNs and providing also an estimation of the uncertainty affecting the RUL prediction. The main methodological novelties in our use of ESNs for RUL prediction are: i) the use of the individual ESN memory capacity within the dynamic procedure for aggregating of the ESNs outcomes; ii) the use of an additional ESN for estimating the RUL uncertainty, within the Mean Variance Estimation (MVE) approach. With these novelties, the developed approach outperforms a static ensemble and a standard MVE approach for uncertainty estimation in tests performed on a synthetic and two industrial datasets.},
  keywords = {Differential Evolution,Echo state networks,Ensembles,Prediction Intervals,Prediction uncertainty,Recurrent neural networks},
  file = {/home/elessar/Zotero/storage/4YAEF7PC/Rigamonti et al. - 2018 - Ensemble of optimized echo state networks for remaining useful life prediction.pdf}
}

@article{rissanenComplexityStringsClass1986,
  title = {Complexity of Strings in the Class of {{Markov}} Sources},
  author = {Rissanen, J.},
  year = {1986},
  month = jul,
  journal = {IEEE Transactions on Information Theory},
  volume = {32},
  number = {4},
  pages = {526--532},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/tit.1986.1057210},
  urldate = {2025-07-25},
  abstract = {Shannon's self-information of a string is generalized to its complexity relative to the class of finite-state-machine (FSM) defined sources. Unlike an earlier generalization, the new one is valid for both short and long strings. The definition is justified in part by a theorem stating that, asymptotically, the mean complexity provides a tight lower bound for the mean length of all so-called regular codes. This also generalizes Shannon's noiseless coding theorem. For a large subclass of FSM sources a simple algorithm is described for computing the complexity.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/8DX64W26/Rissanen - 1986 - Complexity of strings in the class of Markov sources.pdf}
}

@article{rivalsOptimalRepresentationAverage1998,
  title = {Optimal Representation in Average Using {{Kolmogorov}} Complexity},
  author = {Rivals, E. and Delahaye, J.P.},
  year = {1998},
  month = jun,
  journal = {Theoretical Computer Science},
  volume = {200},
  number = {1-2},
  pages = {261--287},
  publisher = {Elsevier BV},
  issn = {0304-3975},
  doi = {10.1016/s0304-3975(97)00275-2},
  urldate = {2025-07-25},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/H7APLH7S/Rivals and Delahaye - 1998 - Optimal representation in average using Kolmogorov complexity.pdf}
}

@article{robertsContinuoustimeEchoState2022,
  title = {Continuous-Time Echo State Networks for Predicting Power System Dynamics},
  author = {Roberts, Ciaran and Lara, Jos{\'e} Daniel and {Henriquez-Auba}, Rodrigo and Bossart, Matthew and Anantharaman, Ranjan and Rackauckas, Chris and Hodge, Bri-Mathias and Callaway, Duncan S.},
  year = {2022},
  month = nov,
  journal = {Electric Power Systems Research},
  volume = {212},
  pages = {108562},
  issn = {03787796},
  doi = {10.1016/j.epsr.2022.108562},
  urldate = {2025-08-12},
  langid = {english}
}

@article{robinsonLocalEntropyStructure2011,
  title = {Local Entropy and Structure in a Two-Dimensional Frustrated System},
  author = {Robinson, Matthew D. and Feldman, David P. and McKay, Susan R.},
  year = {2011},
  month = sep,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {21},
  number = {3},
  publisher = {AIP Publishing},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.3608120},
  urldate = {2025-07-25},
  abstract = {We calculate the local contributions to the Shannon entropy and excess entropy and use these information theoretic measures as quantitative probes of the order arising from quenched disorder in the diluted Ising antiferromagnet on a triangular lattice. When one sublattice is sufficiently diluted, the system undergoes a temperature-driven phase transition, with the other two sublattices developing magnetizations of equal magnitude and opposite sign as the system is cooled.1 The diluted sublattice has no net magnetization but exhibits spin glass ordering. The distribution of local entropies shows a dramatic broadening at low temperatures; this indicates that the system's total entropy is not shared equally across the lattice. The entropy contributions from some regions exhibit local reentrance, although the entropy of the system decreases monotonically as expected. The average excess entropy shows a sharp peak at the critical temperature, showing that the excess entropy is sensitive to the structural changes that occur as a result of the spin glass ordering.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/BL6QGZBJ/Robinson et al. - 2011 - Local entropy and structure in a two-dimensional frustrated system.pdf}
}

@article{robisonFastComputationAdditive,
  title = {Fast {{Computation}} of {{Additive Cellular Automata}}},
  author = {Robison, Arch D},
  langid = {english},
  file = {/home/elessar/Zotero/storage/9THYZJMP/Robison - Fast Computation of Additive Cellular Automata.pdf}
}

@article{rodanMinimumComplexityEcho2011,
  title = {Minimum {{Complexity Echo State Network}}},
  author = {Rodan, Ali and Tino, Peter},
  year = {2011},
  month = jan,
  journal = {IEEE Transactions on Neural Networks},
  volume = {22},
  number = {1},
  pages = {131--144},
  issn = {1941-0093},
  doi = {10.1109/TNN.2010.2089641},
  urldate = {2024-11-26},
  abstract = {Reservoir computing (RC) refers to a new class of state-space models with a fixed state transition structure (the reservoir) and an adaptable readout form the state space. The reservoir is supposed to be sufficiently complex so as to capture a large number of features of the input stream that can be exploited by the reservoir-to-output readout mapping. The field of RC has been growing rapidly with many successful applications. However, RC has been criticized for not being principled enough. Reservoir construction is largely driven by a series of randomized model-building stages, with both researchers and practitioners having to rely on a series of trials and errors. To initialize a systematic study of the field, we concentrate on one of the most popular classes of RC methods, namely echo state network, and ask: What is the minimal complexity of reservoir construction for obtaining competitive models and what is the memory capacity (MC) of such simplified reservoirs? On a number of widely used time series benchmarks of different origin and characteristics, as well as by conducting a theoretical analysis we show that a simple deterministically constructed cycle reservoir is comparable to the standard echo state network methodology. The (short-term) MC of linear cyclic reservoirs can be made arbitrarily close to the proved optimal value.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  keywords = {Communication channels,Computational modeling,Echo state networks,memory capability,neural networks,reservoir computing,Reservoirs,simple recurrent time-series prediction,Thyristors,Time series analysis,Topology,Training},
  file = {/home/elessar/Zotero/storage/HZVWT75U/Rodan and Tino - 2011 - Minimum Complexity Echo State Network.pdf;/home/elessar/Zotero/storage/XK6EN4K3/5629375.html}
}

@article{rodanSimpleDeterministicallyConstructed2012,
  title = {Simple {{Deterministically Constructed Cycle Reservoirs}} with {{Regular Jumps}}},
  author = {Rodan, Ali and Ti{\v n}o, Peter},
  year = {2012},
  month = jul,
  journal = {Neural Computation},
  volume = {24},
  number = {7},
  pages = {1822--1852},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00297},
  urldate = {2025-08-12},
  abstract = {A new class of state-space models, reservoir models, with a fixed state transition structure (the ``reservoir'') and an adaptable readout from the state space, has recently emerged as a way for time series processing and modeling. Echo state network (ESN) is one of the simplest, yet powerful, reservoir models. ESN models are generally constructed in a randomized manner. In our previous study (Rodan \& Ti{\v n}o, 2011 ), we showed that a very simple, cyclic, deterministically generated reservoir can yield performance competitive with standard ESN. In this contribution, we extend our previous study in three aspects. First, we introduce a novel simple deterministic reservoir model, cycle reservoir with jumps (CRJ), with highly constrained weight values, that has superior performance to standard ESN on a variety of temporal tasks of different origin and characteristics. Second, we elaborate on the possible link between reservoir characterizations, such as eigenvalue distribution of the reservoir matrix or pseudo-Lyapunov exponent of the input-driven reservoir dynamics, and the model performance. It has been suggested that a uniform coverage of the unit disk by such eigenvalues can lead to superior model performance. We show that despite highly constrained eigenvalue distribution, CRJ consistently outperforms ESN (which has much more uniform eigenvalue coverage of the unit disk). Also, unlike in the case of ESN, pseudo-Lyapunov exponents of the selected optimal CRJ models are consistently negative. Third, we present a new framework for determining the short-term memory capacity of linear reservoir models to a high degree of precision. Using the framework, we study the effect of shortcut connections in the CRJ reservoir topology on its memory capacity.},
  langid = {english}
}

@article{rohmModelfreeInferenceUnseen2021,
  title = {Model-Free Inference of Unseen Attractors: {{Reconstructing}} Phase Space Features from a Single Noisy Trajectory Using Reservoir Computing},
  shorttitle = {Model-Free Inference of Unseen Attractors},
  author = {R{\"o}hm, Andr{\'e} and Gauthier, Daniel J. and Fischer, Ingo},
  year = {2021},
  month = oct,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {31},
  number = {10},
  eprint = {2108.04074},
  primaryclass = {cs},
  pages = {103127},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0065813},
  urldate = {2025-05-08},
  abstract = {Reservoir computers are powerful tools for chaotic time series prediction. They can be trained to approximate phase space flows and can thus both predict future values to a high accuracy, as well as reconstruct the general properties of a chaotic attractor without requiring a model. In this work, we show that the ability to learn the dynamics of a complex system can be extended to systems with co-existing attractors, here a 4-dimensional extension of the well-known Lorenz chaotic system. We demonstrate that a reservoir computer can infer entirely unexplored parts of the phase space: a properly trained reservoir computer can predict the existence of attractors that were never approached during training and therefore are labelled as unseen. We provide examples where attractor inference is achieved after training solely on a single noisy trajectory.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Nonlinear Sciences - Adaptation and Self-Organizing Systems},
  file = {/home/elessar/Zotero/storage/FKU444HA/Röhm et al. - 2021 - Model-free inference of unseen attractors Reconstructing phase space features from a single noisy t.pdf;/home/elessar/Zotero/storage/PAE9UP5A/Röhm et al. - 2021 - Model-free inference of unseen attractors Reconstructing phase space features from a single noisy t.pdf;/home/elessar/Zotero/storage/J2UEWHXV/2108.html}
}

@article{romashchenkoCombinatorialInterpretationKolmogorov,
  title = {Combinatorial Interpretation of {{Kolmogorov}} Complexity},
  author = {Romashchenko, A and Shen, A and Vereshchagin, N},
  abstract = {Kolmogorov's very +rst paper on algorithmic information theory (Kolmogorov, Problemy peredachi informatsii 1(1) (1965) 3) was entitled ``Three approaches to the de+nition of the quantity of information''. These three approaches were called combinatorial, probabilistic and algorithmic. Trying to establish formal connections between combinatorial and algorithmic approaches, we prove that every linear inequality including Kolmogorov complexities could be translated into an equivalent combinatorial statement. (Note that the same linear inequalities are true for Kolmogorov complexity and Shannon entropy, see Hammer et al., (Proceedings of CCC'97, Ulm).) Entropy (complexity) proofs of combinatorial inequalities given in Llewellyn and Radhakrishnan (Personal Communication) and Hammer and Shen (Theory Comput. Syst. 31 (1998) 1) can be considered as special cases (and natural starting points) for this translation. c{\copyright} 2002 Published by Elsevier Science B.V.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/WPHJ5D6T/Romashchenko et al. - Combinatorial interpretation of Kolmogorov complexity.pdf}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  title = {The Perceptron: {{A}} Probabilistic Model for Information Storage and Organization in the Brain.},
  shorttitle = {The Perceptron},
  author = {Rosenblatt, F.},
  year = {1958},
  journal = {Psychological Review},
  volume = {65},
  number = {6},
  pages = {386--408},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/h0042519},
  urldate = {2025-07-28},
  langid = {english},
  file = {/home/elessar/Zotero/storage/LLZPIUF3/Rosenblatt - 1958 - The perceptron A probabilistic model for information storage and organization in the brain..pdf}
}

@article{ruhunusiriSynchronizationMechanismArnold2012,
  title = {Synchronization Mechanism and {{Arnold}} Tongues for Dust Density Waves},
  author = {Ruhunusiri, W. D. Suranga and Goree, J.},
  year = {2012},
  month = apr,
  journal = {Physical Review E},
  volume = {85},
  number = {4},
  pages = {046401},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.85.046401},
  urldate = {2025-07-29},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/CMZAK6SS/Ruhunusiri and Goree - 2012 - Synchronization mechanism and Arnold tongues for dust density waves.pdf}
}

@book{russellArtificialIntelligenceModern2010,
  title = {Artificial Intelligence: A Modern Approach},
  shorttitle = {Artificial Intelligence},
  author = {Russell, Stuart J. and Norvig, Peter and Davis, Ernest},
  year = {2010},
  series = {Prentice {{Hall}} Series in Artificial Intelligence},
  edition = {3rd ed},
  publisher = {Prentice Hall},
  address = {Upper Saddle River},
  isbn = {978-0-13-604259-4},
  langid = {english},
  lccn = {Q335 .R86 2010},
  keywords = {Artificial intelligence},
  file = {/home/elessar/Zotero/storage/7PI48UNQ/Russell et al. - 2010 - Artificial intelligence a modern approach.pdf}
}

@article{russellDimensionStrangeAttractors1980,
  title = {Dimension of {{Strange Attractors}}},
  author = {Russell, David A. and Hanson, James D. and Ott, Edward},
  year = {1980},
  month = oct,
  journal = {Physical Review Letters},
  volume = {45},
  number = {14},
  pages = {1175--1178},
  publisher = {American Physical Society (APS)},
  issn = {0031-9007},
  doi = {10.1103/physrevlett.45.1175},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/G8SVK6MM/Russell et al. - 1980 - Dimension of Strange Attractors.pdf}
}

@article{ryabkoUsingInformationTheory2005,
  title = {Using Information Theory Approach to Randomness Testing},
  author = {Ryabko, {\relax B.Ya}. and Monarev, V.A.},
  year = {2005},
  month = jul,
  journal = {Journal of Statistical Planning and Inference},
  volume = {133},
  number = {1},
  pages = {95--110},
  publisher = {Elsevier BV},
  issn = {0378-3758},
  doi = {10.1016/j.jspi.2004.02.010},
  urldate = {2025-07-15},
  abstract = {We address the problem of detecting deviations of binary sequence from randomness,which is very important for random number (RNG) and pseudorandom number generators (PRNG). Namely, we consider a null hypothesis H0 that a given bit sequence is generated by Bernoulli source with equal probabilities of 0 and 1 and the alternative hypothesis H1 that the sequence is generated by a stationary and ergodic source which differs from the source under H0. We show that data compression methods can be used as a basis for such testing and describe two new tests for randomness, which are based on ideas of universal coding. Known statistical tests and suggested ones are applied for testing PRNGs. Those experiments show that the power of the new tests is greater than of many known algorithms.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/56BYPGLM/Ryabko and Monarev - 2005 - Using information theory approach to randomness testing.pdf}
}

@misc{sadathApproximatingStrangeAttractors2018,
  title = {Approximating Strange Attractors and {{Lyapunov}} Exponents of Delay Differential Equations Using {{Galerkin}} Projections},
  author = {Sadath, Anwar and Uchida, Thomas K. and Vyasarayani, C. P.},
  year = {2018},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1810.01016},
  urldate = {2025-03-20},
  abstract = {Delay differential equations (DDEs) are infinite-dimensional systems, so even a scalar, unforced nonlinear DDE can exhibit chaos. Lyapunov exponents are indicators of chaos and can be computed by comparing the evolution of infinitesimally close trajectories. We convert DDEs into partial differential equations with nonlinear boundary conditions, then into ordinary differential equations (ODEs) using the Galerkin projection. The solution of the resulting ODEs approximates that of the original DDE system; for smooth solutions, the error decreases exponentially as the number of terms used in the Galerkin approximation increases. Examples demonstrate that the strange attractors and Lyapunov exponents of chaotic DDE solutions can be reliably approximated by a smaller number of ODEs using the proposed approach compared to the standard method-of-lines approach, leading to faster convergence and improved computational efficiency.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  langid = {english},
  keywords = {Chaotic Dynamics (nlin.CD),Computational Physics (physics.comp-ph),FOS: Physical sciences},
  file = {/home/elessar/Zotero/storage/U8TV34FP/Sadath et al. - 2018 - Approximating strange attractors and Lyapunov exponents of delay differential equations using Galerk.pdf}
}

@article{sadikuRESERVOIRCOMPUTINGPRIMER2018,
  title = {{{RESERVOIR}}  {{COMPUTING}}: {{A PRIMER}}},
  author = {Sadiku, Matthew},
  year = {2018},
  volume = {5},
  abstract = {Reservoir computing (RC) is a new computing paradigm that allows harnessing the dynamics of a reservoir (or compute core) to perform computations. RC is a new training concept for recurrent neural networks. It initially emerged as a software-only technique and used as an algorithmic way of processing temporal data. It has been successfully used in pattern classification problems such as like speech and image recognition and time series prediction. This paper provides a brief primer to basic concepts, implementations, and applications of reservoir computing.},
  langid = {english},
  annotation = {GSCC: 0000002},
  file = {/home/elessar/Zotero/storage/WNK35IMX/Sadiku - 2018 - RESERVOIR  COMPUTING A PRIMER.pdf}
}

@article{sagawaGeometricalExpressionExcess2011,
  title = {Geometrical Expression of Excess Entropy Production},
  author = {Sagawa, Takahiro and Hayakawa, Hisao},
  year = {2011},
  month = nov,
  journal = {Physical Review E},
  volume = {84},
  number = {5},
  publisher = {American Physical Society (APS)},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/physreve.84.051110},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/YKHP4E38/Sagawa and Hayakawa - 2011 - Geometrical expression of excess entropy production.pdf}
}

@article{sakemiLearningReservoirDynamics2024,
  title = {Learning Reservoir Dynamics with Temporal Self-Modulation},
  author = {Sakemi, Yusuke and Nobukawa, Sou and Matsuki, Toshitaka and Morie, Takashi and Aihara, Kazuyuki},
  year = {2024},
  month = jan,
  journal = {Communications Physics},
  volume = {7},
  number = {1},
  pages = {29},
  issn = {2399-3650},
  doi = {10.1038/s42005-023-01500-w},
  urldate = {2025-08-12},
  abstract = {Abstract             Reservoir computing (RC) can efficiently process time-series data by mapping the input signal into a high-dimensional space via randomly connected recurrent neural networks (RNNs), which are referred to as a reservoir. The high-dimensional representation of time-series data in the reservoir simplifies subsequent learning tasks. Although this simple architecture allows fast learning and facile physical implementation, the learning performance is inferior to that of other state-of-the-art RNN models. In this study, to improve the learning ability of RC, we propose self-modulated RC (SM-RC) that extends RC by adding a self-modulation mechanism. SM-RC can perform attention tasks where input information is retained or discarded depending on the input signal. We find that a chaotic state can emerge as a result of learning in SM-RC. Furthermore, we demonstrate that SM-RC outperforms RC in NARMA and Lorenz model tasks. Because the SM-RC architecture only requires two additional gates, it is physically implementable as RC, thereby providing a direction for realizing edge artificial intelligence.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/54Z98LIL/Sakemi et al. - 2024 - Learning reservoir dynamics with temporal self-modulation.pdf}
}

@book{sanayeiISCS2014Interdisciplinary2015,
  title = {{{ISCS}} 2014: {{Interdisciplinary Symposium}} on {{Complex Systems}}},
  shorttitle = {{{ISCS}} 2014},
  editor = {Sanayei, Ali and E. R{\"o}ssler, Otto and Zelinka, Ivan},
  year = {2015},
  series = {Emergence, {{Complexity}} and {{Computation}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  issn = {2194-7287, 2194-7295},
  doi = {10.1007/978-3-319-10759-2},
  urldate = {2025-07-25},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-3-319-10758-5 978-3-319-10759-2},
  langid = {english},
  file = {/home/elessar/Zotero/storage/8F3SPJRV/Sanayei et al. - 2015 - ISCS 2014 Interdisciplinary Symposium on Complex Systems.pdf}
}

@article{sanchezMethodDiscernComplexity2005,
  title = {A Method to Discern Complexity in Two-Dimensional Patterns Generated by Coupled Map Lattices},
  author = {Sanchez, Juan and {Lopez-Ruiz}, Ricardo},
  year = {2005},
  month = sep,
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {355},
  number = {2-4},
  eprint = {nlin/0410062},
  pages = {633--640},
  issn = {0378-4371},
  doi = {10.1016/j.physa.2005.02.058},
  urldate = {2025-07-15},
  abstract = {Complex patterns generated by the time evolution of a one-dimensional digitalized coupled map lattice are quantitatively analyzed. A method for discerning complexity among the different patterns is implemented. The quantitative results indicate two zones in parameter space where the dynamics shows the most complex patterns. These zones are located on the two edges of an absorbent region where the system displays spatio-temporal intermittency.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computational Geometry,Condensed Matter - Disordered Systems and Neural Networks,Mathematics - Dynamical Systems,Nonlinear Sciences - Chaotic Dynamics,Nonlinear Sciences - Pattern Formation and Solitons,Quantitative Biology - Quantitative Methods},
  file = {/home/elessar/Zotero/storage/GI5WHZRP/Sanchez and Lopez-Ruiz - 2005 - A method to discern complexity in two-dimensional patterns generated by coupled map lattices.pdf}
}

@article{sanjuanArtificialIntelligenceChaos2021,
  title = {Artificial {{Intelligence}}, {{Chaos}}, {{Prediction}} and {{Understanding}} in {{Science}}},
  author = {Sanjuan, Miguel A. F.},
  year = {2021},
  month = sep,
  journal = {International Journal of Bifurcation and Chaos},
  volume = {31},
  number = {11},
  eprint = {2003.01771},
  primaryclass = {cs},
  pages = {2150173},
  issn = {0218-1274, 1793-6551},
  doi = {10.1142/S021812742150173X},
  urldate = {2023-05-30},
  abstract = {Machine learning and deep learning techniques are contributing much to the advancement of science. Their powerful predictive capabilities appear in numerous disciplines, including chaotic dynamics, but they miss understanding. The main thesis here is that prediction and understanding are two very different and important ideas that should guide us about the progress of science. Furthermore, it is emphasized the important role played by nonlinear dynamical systems for the process of understanding. The path of the future of science will be marked by a constructive dialogue between big data and big theory, without which we cannot understand.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - General Literature},
  annotation = {GSCC: 0000106},
  file = {/home/elessar/Zotero/storage/7JXIRDZU/Sanjuan - 2021 - Artificial Intelligence, Chaos, Prediction and Und.pdf}
}

@article{sauerSpuriousLyapunovExponents1998,
  title = {Spurious {{Lyapunov Exponents}} in {{Attractor Reconstruction}}},
  author = {Sauer, Timothy D. and Tempkin, Joshua A. and Yorke, James A.},
  year = {1998},
  month = nov,
  journal = {Physical Review Letters},
  volume = {81},
  number = {20},
  pages = {4341--4344},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.81.4341},
  urldate = {2023-05-30},
  langid = {english},
  annotation = {GSCC: 0000058},
  file = {/home/elessar/Zotero/storage/HP39EKI6/Sauer et al. - 1998 - Spurious Lyapunov Exponents in Attractor Reconstru.pdf}
}

@article{sauerwaldLecture2Spectra,
  title = {Lecture 2: {{Spectra}} of {{Graphs}}},
  author = {Sauerwald, Thomas and Sun, He},
  langid = {english},
  annotation = {GSCC: 0002801},
  file = {/home/elessar/Zotero/storage/LZ7VSCQN/Sauerwald and Sun - Lecture 2 Spectra of Graphs.pdf}
}

@article{savariRedundancyLempelZivIncremental1997,
  title = {Redundancy of the {{Lempel-Ziv}} Incremental Parsing Rule},
  author = {Savari, S.A.},
  year = {1997},
  journal = {IEEE Transactions on Information Theory},
  volume = {43},
  number = {1},
  pages = {9--21},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9448},
  doi = {10.1109/18.567642},
  urldate = {2025-07-25},
  abstract = {The Lempel-Ziv codes are universal variable-tofixed length codes that have become virtually standard in practical lossless data compression. For any given source output string from a Markov or unifilar source, we upper-bound the difference between the number of binary digits needed to encode the string and the self-information of the string. We use this result to demonstrate that for unifilar or Markov sources, the redundancy of encoding the first n letters of the source output with the Lempel-Ziv incremental parsing rule (LZ'78), the Welch modification (LZW), or a new variant is O((lnn)-'), and we upper-bound the exact form of convergence. We conclude by considering the relationship between the code length and the empirical entropy associated with a string.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/NKDKKI6G/Savari - 1997 - Redundancy of the Lempel-Ziv incremental parsing rule.pdf}
}

@article{scardapaneDecentralizedTrainingAlgorithm2016,
  title = {A Decentralized Training Algorithm for {{Echo State Networks}} in Distributed Big Data Applications},
  author = {Scardapane, Simone and Wang, Dianhui and Panella, Massimo},
  year = {2016},
  month = jun,
  journal = {Neural Networks},
  volume = {78},
  pages = {65--74},
  issn = {08936080},
  doi = {10.1016/j.neunet.2015.07.006},
  urldate = {2025-03-12},
  abstract = {The current big data deluge requires innovative solutions for performing efficient inference on large, heterogeneous amounts of information. Apart from the known challenges deriving from high volume and velocity, real-world big data applications may impose additional technological constraints, including the need for a fully decentralized training architecture. While several alternatives exist for training feedforward neural networks in such a distributed setting, less attention has been devoted to the case of decentralized training of recurrent neural networks (RNNs). In this paper, we propose such an algorithm for a class of RNNs known as Echo State Networks. The algorithm is based on the well-known Alternating Direction Method of Multipliers optimization procedure. It is formulated only in terms of local exchanges between neighboring agents, without reliance on a coordinating node. Additionally, it does not require the communication of training patterns, which is a crucial component in realistic big data implementations. Experimental results on large scale artificial datasets show that it compares favorably with a fully centralized implementation, in terms of speed, efficiency and generalization accuracy.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/WZXJFTD2/Scardapane et al. - 2016 - A decentralized training algorithm for Echo State Networks in distributed big data applications.pdf}
}

@article{schmidhuberTrainingRecurrentNetworks2007,
  title = {Training {{Recurrent Networks}} by {{Evolino}}},
  author = {Schmidhuber, J{\"u}rgen and Wierstra, Daan and Gagliolo, Matteo and Gomez, Faustino},
  year = {2007},
  month = mar,
  journal = {Neural Computation},
  volume = {19},
  number = {3},
  pages = {757--779},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2007.19.3.757},
  urldate = {2023-05-30},
  abstract = {In recent years, gradient-based LSTM recurrent neural networks (RNNs) solved many previously RNN-unlearnable tasks. Sometimes, however, gradient information is of little use for training RNNs, due to numerous local minima. For such cases we present a novel method, namely, EVOlution of systems with LINear Outputs (Evolino). Evolino evolves weights to the nonlinear, hidden nodes of RNNs while computing optimal linear mappings from hidden state to output, using methods such as pseudo-inverse-based linear regression. If we instead use quadratic programming to maximize the margin, we obtain the first evolutionary recurrent Support Vector Machines. We show that Evolino-based LSTM can solve tasks that Echo State nets [15] cannot, and achieves higher accuracy in certain continuous function generation tasks than conventional gradient descent RNNs, including gradient-based LSTM.},
  langid = {english},
  annotation = {GSCC: 0000328},
  file = {/home/elessar/Zotero/storage/92XK4FG6/Schmidhuber et al. - 2007 - Training Recurrent Networks by Evolino.pdf}
}

@article{schrauwenOverviewReservoirComputing,
  title = {An Overview of Reservoir Computing: Theory, Applications and Implementations},
  author = {Schrauwen, Benjamin},
  abstract = {Training recurrent neural networks is hard. Recently it has however been discovered that it is possible to just construct a random recurrent topology, and only train a single linear readout layer. State-ofthe-art performance can easily be achieved with this setup, called Reservoir Computing. The idea can even be broadened by stating that any high dimensional, driven dynamic system, operated in the correct dynamic regime can be used as a temporal `kernel' which makes it possible to solve complex tasks using just linear post-processing techniques.},
  langid = {english},
  annotation = {GSCC: 0000665},
  file = {/home/elessar/Zotero/storage/CJNYFT66/Schrauwen - An overview of reservoir computing theory, applic.pdf}
}

@article{schurmannBiasAnalysisEntropy2004,
  title = {Bias {{Analysis}} in {{Entropy Estimation}}},
  author = {Sch{\"u}rmann, Thomas},
  year = {2004},
  month = jul,
  journal = {Journal of Physics A: Mathematical and General},
  volume = {37},
  number = {27},
  eprint = {cond-mat/0403192},
  pages = {L295-L301},
  issn = {0305-4470, 1361-6447},
  doi = {10.1088/0305-4470/37/27/L02},
  urldate = {2025-07-15},
  abstract = {We consider the problem of finite sample corrections for entropy estimation. New estimates of the Shannon entropy are proposed and their systematic error (the bias) is computed analytically. We find that our results cover correction formulas of current entropy estimates recently discussed in literature. The trade-off between bias reduction and the increase of the corresponding statistical error is analyzed.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Statistical Mechanics,Mathematical Physics,Mathematics - Mathematical Physics,Physics - Data Analysis Statistics and Probability},
  file = {/home/elessar/Zotero/storage/WMQNAQUX/Schürmann - 2004 - Bias Analysis in Entropy Estimation.pdf}
}

@article{schurmannEntropyEstimationSymbol1996,
  title = {Entropy Estimation of Symbol Sequences},
  author = {Sch{\"u}rmann, Thomas and Grassberger, Peter},
  year = {1996},
  month = sep,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {6},
  number = {3},
  pages = {414--427},
  publisher = {AIP Publishing},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.166191},
  urldate = {2025-07-25},
  abstract = {We discuss algorithms for estimating the Shannon entropy h of finite symbol sequences with long range correlations. In particular, we consider algorithms which estimate h from the code lengths produced by some compression algorithm. Our interest is in describing their convergence with sequence length, assuming no limits for the space and time complexities of the compression algorithms. A scaling law is proposed for extrapolation from finite sample lengths. This is applied to sequences of dynamical systems in non-trivial chaotic regimes, a 1-D cellular automaton, and to written English texts.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/8ZMN3RH7/Schürmann and Grassberger - 1996 - Entropy estimation of symbol sequences.pdf}
}

@article{schurmannPredictabilityLettersWritten1996,
  title = {The Predictability of Letters in Written English},
  author = {Sch{\"u}rmann, Thomas and Grassberger, Peter},
  year = {1996},
  month = mar,
  journal = {Fractals},
  volume = {04},
  number = {01},
  eprint = {0710.4516},
  primaryclass = {physics},
  pages = {1--5},
  issn = {0218-348X, 1793-6543},
  doi = {10.1142/S0218348X96000029},
  urldate = {2025-07-25},
  abstract = {We show that the predictability of letters in written English texts depends strongly on their position in the word. The first letters are usually the least easy to predict. This agrees with the intuitive notion that words are well defined subunits in written languages, with much weaker correlations across these units than within them. It implies that the average entropy of a letter deep inside a word is roughly 4 times smaller than the entropy of the first letter.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Physics - Physics and Society,Statistics - Machine Learning},
  file = {/home/elessar/Zotero/storage/NUUB6XCG/Schürmann and Grassberger - 1996 - The predictability of letters in written english.pdf}
}

@article{scullyMeasuringChaosLorenz2021,
  title = {Measuring Chaos in the {{Lorenz}} and {{R{\"o}ssler}} Models: {{Fidelity}} Tests for Reservoir Computing},
  shorttitle = {Measuring Chaos in the {{Lorenz}} and {{R{\"o}ssler}} Models},
  author = {Scully, James J. and Neiman, Alexander B. and Shilnikov, Andrey L.},
  year = {2021},
  month = sep,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {31},
  number = {9},
  pages = {093121},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0065044},
  urldate = {2025-08-12},
  abstract = {This study focuses on the qualitative and quantitative characterization of chaotic systems with the use of a symbolic description. We consider two famous systems, Lorenz and R{\"o}ssler models with their iconic attractors, and demonstrate that with adequately chosen symbolic partition, three measures of complexity, such as the Shannon source entropy, the Lempel--Ziv complexity, and the Markov transition matrix, work remarkably well for characterizing the degree of chaoticity and precise detecting stability windows in the parameter space. The second message of this study is to showcase the utility of symbolic dynamics with the introduction of a fidelity test for reservoir computing for simulating the properties of the chaos in both models' replicas. The results of these measures are validated by the comparison approach based on one-dimensional return maps and the complexity measures.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/U7S5T3VQ/Scully et al. - 2021 - Measuring chaos in the Lorenz and Rössler models Fidelity tests for reservoir computing.pdf}
}

@article{seroussiUniversalTypes2006,
  title = {On Universal Types},
  author = {Seroussi, G.},
  year = {2006},
  month = jan,
  journal = {IEEE Transactions on Information Theory},
  volume = {52},
  number = {1},
  pages = {171--189},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9448},
  doi = {10.1109/tit.2005.860437},
  urldate = {2025-07-15},
  abstract = {The universal type class of a sequence is defined, in analogy to the notion underlying the classical method of types. Two sequences of the same length are said to be of the same universal (LZ) type if and only if they yield the same dictionary (or, equivalently, parsing tree) in the incremental parsing of Ziv and Lempel (1978). It is shown that for any finite order , the variational distance between the th-order empirical probability distributions of two sequences of the same universal type vanishes as the sequence length tends to infinity. Consequently, for any and any th-order probability assignment, the difference between the normalized logarithms of the probabilities assigned to two sequences of the same universal type also vanishes asymptotically. The size of a universal type class is studied, and it is shown that its asymptotic behavior parallels that of the conventional counterpart, with the LZ78 code length playing the role of the empirical entropy. The number of universal types for sequences of length is estimated, and shown to be of the form exp((1 + (1)) log ) for a well characterized constant . Algorithms for enumerating the sequences in a universal type class, and for drawing a sequence from the class with uniform probability are described. As an application, the problem of universal simulation of individual sequences is considered. A sequence drawn with uniform probability from the universal type class of is an optimal simulation of in a well defined mathematical sense.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/UD3A5BBZ/Seroussi - 2006 - On universal types.pdf}
}

@article{sethnaEntropyOrderParameters2006,
  title = {Entropy, {{Order Parameters}}, and {{Complexity}}},
  author = {Sethna, James P},
  year = {2006},
  langid = {english},
  file = {/home/elessar/Zotero/storage/AYNZBNPN/Sethna - 2006 - Entropy, Order Parameters, and Complexity.pdf}
}

@article{shahiPredictionChaoticTime2022,
  title = {Prediction of Chaotic Time Series Using Recurrent Neural Networks and Reservoir Computing Techniques: {{A}} Comparative Study},
  shorttitle = {Prediction of Chaotic Time Series Using Recurrent Neural Networks and Reservoir Computing Techniques},
  author = {Shahi, Shahrokh and Fenton, Flavio H. and Cherry, Elizabeth M.},
  year = {2022},
  month = jun,
  journal = {Machine Learning with Applications},
  volume = {8},
  pages = {100300},
  issn = {26668270},
  doi = {10.1016/j.mlwa.2022.100300},
  urldate = {2023-05-30},
  abstract = {In recent years, machine-learning techniques, particularly deep learning, have outperformed traditional timeseries forecasting approaches in many contexts, including univariate and multivariate predictions. This study aims to investigate the capability of (i) gated recurrent neural networks, including long short-term memory (LSTM) and gated recurrent unit (GRU) networks, (ii) reservoir computing (RC) techniques, such as echo state networks (ESNs) and hybrid physics-informed ESNs, and (iii) the nonlinear vector autoregression (NVAR) approach, which has recently been introduced as the next generation RC, for the prediction of chaotic time series and to compare their performance in terms of accuracy, efficiency, and robustness. We apply the methods to predict time series obtained from two widely used chaotic benchmarks, the Mackey--Glass and Lorenz-63 models, as well as two other chaotic datasets representing a bursting neuron and the dynamics of the El Ni{\~n}o Southern Oscillation, and to one experimental dataset representing a time series of cardiac voltage with complex dynamics. We find that even though gated RNN techniques have been successful in forecasting time series generally, they can fall short in predicting chaotic time series for the methods, datasets, and ranges of hyperparameter values considered here. In contrast, for the chaotic datasets studied, we found that reservoir computing and NVAR techniques are more computationally efficient and offer more promise in long-term prediction of chaotic time series.},
  langid = {english},
  annotation = {GSCC: 0000070},
  file = {/home/elessar/Zotero/storage/MLMAAI3M/Shahi et al. - 2022 - Prediction of chaotic time series using recurrent .pdf;/home/elessar/Zotero/storage/NCK38B3Z/Shahi et al. - 2022 - Prediction of chaotic time series using recurrent .pdf}
}

@misc{shaliziAlgorithmPatternDiscovery2002,
  title = {An {{Algorithm}} for {{Pattern Discovery}} in {{Time Series}}},
  author = {Shalizi, Cosma Rohilla and Shalizi, Kristina Lisa and Crutchfield, James P.},
  year = {2002},
  month = nov,
  number = {arXiv:cs/0210025},
  eprint = {cs/0210025},
  publisher = {arXiv},
  doi = {10.48550/arXiv.cs/0210025},
  urldate = {2025-07-15},
  abstract = {We present a new algorithm for discovering patterns in time series and other sequential data. We exhibit a reliable procedure for building the minimal set of hidden, Markovian states that is statistically capable of producing the behavior exhibited in the data --- the underlying process's causal states. Unlike conventional methods for fitting hidden Markov models (HMMs) to data, our algorithm makes no assumptions about the process's causal architecture (the number of hidden states and their transition structure), but rather infers it from the data. It starts with assumptions of minimal structure and introduces complexity only when the data demand it. Moreover, the causal states it infers have important predictive optimality properties that conventional HMM states lack. We introduce the algorithm, review the theory behind it, prove its asymptotic reliability, use large deviation theory to estimate its rate of convergence, and compare it to other algorithms which also construct HMMs from data. We also illustrate its behavior on an example process, and report selected numerical results from an implementation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/elessar/Zotero/storage/3LR65WIM/Shalizi et al. - 2002 - An Algorithm for Pattern Discovery in Time Series.pdf}
}

@article{shanazReservoirComputingUsing2020,
  title = {Reservoir {{Computing Using Complex Systems}}},
  author = {Shanaz, N. Rasha and Murali, K. and Muruganandam, P.},
  year = {2020},
  month = dec,
  journal = {Indian Academy of Sciences Conference Series},
  volume = {3},
  number = {1},
  doi = {10.29195/iascs.03.01.0018},
  urldate = {2023-05-30},
  abstract = {Reservoir computing (RC) is an emerging machine learning framework which is a versatile option for utilizing physical systems for computation. In this paper, we demonstrate how a single node reservoir, made of a simple electronic circuit, can be employed for computation and explore the available options to improve the computational capability of the physical reservoirs. We build a RC system using a memristive chaotic oscillator as the reservoir. We choose two of the available hyperparameters to find the optimal working regime for the reservoir, resulting in two reservoir versions. We compare the performance of both the reservoirs in a set of three nontemporal tasks: approximating two non-chaotic polynomials and a chaotic trajectory of the Lorenz time series. We also demonstrate how the dynamics of the physical system plays a direct role in the reservoir's hyperparameters and hence in the reservoir's prediction ability.},
  langid = {english},
  annotation = {GSCC: 0000273},
  file = {/home/elessar/Zotero/storage/TR2YVV8Y/Shanaz et al. - 2020 - Reservoir Computing Using Complex Systems.pdf}
}

@article{shannonMathematicalTheoryCommunication,
  title = {A {{Mathematical Theory}} of {{Communication}}},
  author = {Shannon, C E},
  langid = {english},
  file = {/home/elessar/Zotero/storage/75UDTX9D/Shannon - A Mathematical Theory of Communication.pdf}
}

@article{sharmaFastPrincipalComponent2007,
  title = {Fast Principal Component Analysis Using Fixed-Point Algorithm},
  author = {Sharma, Alok and Paliwal, Kuldip K.},
  year = {2007},
  month = jul,
  journal = {Pattern Recognition Letters},
  volume = {28},
  number = {10},
  pages = {1151--1155},
  issn = {01678655},
  doi = {10.1016/j.patrec.2007.01.012},
  urldate = {2025-03-08},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/7I5TF3PR/Sharma and Paliwal - 2007 - Fast principal component analysis using fixed-point algorithm.pdf}
}

@inproceedings{sheikhaniAnalysisQuantitativeElectroencephalogram2007,
  title = {Analysis of Quantitative {{Electroencephalogram}} Background Activity in {{Autism}} Disease Patients with {{Lempel-Ziv}} Complexity and {{Short Time Fourier Transform}} Measure},
  booktitle = {2007 4th {{IEEE}}/{{EMBS International Summer School}} and {{Symposium}} on {{Medical Devices}} and {{Biosensors}}},
  author = {Sheikhani, Ali and Behnam, Hamid and Mohammadi, Mohammad Reza and Noroozian, Maryam and Golabi, Pari},
  year = {2007},
  month = aug,
  pages = {111--114},
  publisher = {IEEE},
  address = {Cambridge, UK},
  doi = {10.1109/issmdbs.2007.4338305},
  urldate = {2025-07-15},
  abstract = {Electroencephalography (EEG) is an essential tool ASDs, the exact frequency of electroencephalogram (EEG) for the evaluation and treatment of neurophysiologic disorders. abnormalities in an ASD population that has not had clinical Careful analysis of the EEG records can provide insight and seizures or prior abnormal EEGs is unknown [6].},
  langid = {english},
  file = {/home/elessar/Zotero/storage/GK2XJJ3W/Sheikhani et al. - 2007 - Analysis of quantitative Electroencephalogram background activity in Autism disease patients with Le.pdf}
}

@article{shenLogicalOperationsKolmogorov2002,
  title = {Logical Operations and {{Kolmogorov}} Complexity},
  author = {Shen, Alexander and Vereshchagin, Nikolai},
  year = {2002},
  month = jan,
  journal = {Theoretical Computer Science},
  volume = {271},
  number = {1-2},
  pages = {125--129},
  publisher = {Elsevier BV},
  issn = {0304-3975},
  doi = {10.1016/s0304-3975(01)00035-4},
  urldate = {2025-07-25},
  abstract = {Conditional Kolmogorov complexity K(x{\textbar}y) can be understood as the complexity of the problem ``Y {$\rightarrow$} X '', where X is the problem ``construct x'' and Y is the problem ``construct y''. Other logical operations ({$\wedge$}; {$\vee$}; {$\leftrightarrow$}) can be interpreted in a similar way, extendingKolmoogrov interpretation of intuitionistic logic and Kleene realizability. This leads to interesting problems in algorithmic information theory. Some of these questions are discussed. c{\copyright} 2002 Elsevier Science B.V. All rights reserved.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/NB7UIV6Z/Shen and Vereshchagin - 2002 - Logical operations and Kolmogorov complexity.pdf}
}

@incollection{shimomuraReservoirComputingBased2024,
  title = {Reservoir {{Computing Based}} on {{Iterative Function Systems}}},
  booktitle = {Photonic {{Neural Networks}} with {{Spatiotemporal Dynamics}}},
  author = {Shimomura, Suguru},
  editor = {Suzuki, Hideyuki and Tanida, Jun and Hashimoto, Masanori},
  year = {2024},
  pages = {227--243},
  publisher = {Springer Nature Singapore},
  address = {Singapore},
  doi = {10.1007/978-981-99-5072-0_11},
  urldate = {2025-03-18},
  abstract = {Various approaches have been proposed to construct reservoir computing systems. However, the network structure and information processing capacity of these systems are often tied to their individual implementations, which typically become difficult to modify after physical setup. This limitation can hinder performance when the system is required to handle a wide spectrum of prediction tasks. To address this limitation, it is crucial to develop tunable systems that can adapt to a wide range of problem domains. This chapter presents a tunable optical computing method based on the iterative function system (IFS). The tuning capability of IFS provides adjustment of the network structure and optimizes the performance of the optical system. Numerical and experimental results show the tuning capability of the IFS reservoir computing. The relationship between tuning parameters and reservoir properties is discussed. We further investigate the impact of optical feedback on the reservoir properties and present the prediction results.},
  isbn = {978-981-99-5071-3 978-981-99-5072-0},
  langid = {english},
  file = {/home/elessar/Zotero/storage/VB3XGHMW/Shimomura - 2024 - Reservoir Computing Based on Iterative Function Systems.pdf}
}

@article{shmulevichMeasuresTemporalPattern2000,
  title = {Measures of {{Temporal Pattern Complexity}}},
  author = {Shmulevich, I. and Povel, D.-J.},
  year = {2000},
  month = mar,
  journal = {Journal of New Music Research},
  volume = {29},
  number = {1},
  pages = {61--69},
  publisher = {Informa UK Limited},
  issn = {0929-8215},
  doi = {10.1076/0929-8215(200003)29:01;1-p;ft061},
  urldate = {2025-07-25},
  langid = {english},
  file = {/home/elessar/Zotero/storage/D49QRJLA/Shmulevich and Povel - 2000 - Measures of Temporal Pattern Complexity.pdf}
}

@article{shortEntropyGeneralPhysical2010,
  title = {Entropy in General Physical Theories},
  author = {Short, Anthony J and Wehner, Stephanie},
  year = {2010},
  month = mar,
  journal = {New Journal of Physics},
  volume = {12},
  number = {3},
  pages = {033023},
  publisher = {IOP Publishing},
  issn = {1367-2630},
  doi = {10.1088/1367-2630/12/3/033023},
  urldate = {2025-07-25},
  abstract = {Information plays an important role in our understanding of the physical world. Hence we propose an entropic measure of information for any physical theory that admits systems, states and measurements. In the quantum and classical worlds, our measure reduces to the von Neumann and Shannon entropies, respectively. It can even be used in a quantum or classical setting where we are only allowed to perform a limited set of operations. In a world that admits superstrong correlations in the form of non-local boxes, our measure can be used to analyze protocols such as superstrong random access encodings and the violation of `information causality'. However, we also show that in such a world no entropic measure can exhibit all the properties we commonly accept in a quantum setting. For example, there exists no `reasonable' measure of conditional entropy that is subadditive. Finally, we prove a coding theorem for some theories that is analogous to the quantum and classical settings, providing us with an appealing operational interpretation.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/BVBCT3NG/Short and Wehner - 2010 - Entropy in general physical theories.pdf}
}

@article{siegelmannComputationalComplexityContinuous1999,
  title = {Computational {{Complexity}} for {{Continuous Time Dynamics}}},
  author = {Siegelmann, Hava T and {Ben-Hur}, Asa and Fishman, Shmuel},
  year = {1999},
  journal = {P H Y S I C A L R E V I E W L E T T E R S},
  volume = {83},
  number = {7},
  langid = {english},
  file = {/home/elessar/Zotero/storage/8K6NWKE6/Siegelmann et al. - 1999 - Computational Complexity for Continuous Time Dynamics.pdf}
}

@article{siekMatrixTemplateLibrary1999,
  title = {The {{Matrix Template Library}}: Generic Components for High-Performance Scientific Computing},
  shorttitle = {The {{Matrix Template Library}}},
  author = {Siek, J.G. and Lumsdaine, A.},
  year = {1999},
  journal = {Computing in Science \& Engineering},
  volume = {1},
  number = {6},
  pages = {70--71},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {1521-9615},
  doi = {10.1109/5992.805137},
  urldate = {2025-07-25},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/YH5GMGNR/Siek and Lumsdaine - 1999 - The Matrix Template Library generic components for high-performance scientific computing.PDF}
}

@article{silverMasteringGameGo2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and Van Den Driessche, George and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  month = oct,
  journal = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature24270},
  urldate = {2025-08-21},
  langid = {english},
  file = {/home/elessar/Zotero/storage/99Q7CPVB/Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf}
}

@article{simonfongLinearSimpleCycle2025,
  title = {Linear Simple Cycle Reservoirs at the Edge of Stability Perform {{Fourier}} Decomposition of the Input Driving Signals},
  author = {Simon Fong, Robert and Li, Boyu and Ti{\v n}o, Peter},
  year = {2025},
  month = apr,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {35},
  number = {4},
  pages = {043109},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0251154},
  urldate = {2025-08-12},
  abstract = {This paper explores the representational structure of linear Simple Cycle Reservoirs (SCRs) operating at the edge of stability. We view SCR as providing in their state-space feature representations of the input-driving time series. By endowing the state space with the canonical dot-product, we ``reverse engineer'' the corresponding kernel (inner product) operating in the original time series space. The action of this time series kernel is fully characterized by the eigenspace of the corresponding metric tensor. We demonstrate that when linear SCRs are constructed at the edge of stability, the eigenvectors of the time series kernel align with the Fourier basis. This theoretical insight is supported by numerical experiments.},
  langid = {english}
}

@article{singhNoteSolvingFourthorder2018,
  title = {A Note on Solving the Fourth-Order {{Kuramoto-Sivashinsky}} Equation by the Compact Finite Difference Scheme},
  author = {Singh, Brajesh Kumar and Arora, Geeta and Kumar, Pramod},
  year = {2018},
  month = dec,
  journal = {Ain Shams Engineering Journal},
  volume = {9},
  number = {4},
  pages = {1581--1589},
  issn = {20904479},
  doi = {10.1016/j.asej.2016.11.008},
  urldate = {2023-05-30},
  abstract = {The present article is concerned with the implementation of the compact finite difference scheme, in the space and the optimal four-stage, order three strong stability-preserving time-stepping Runge-Kutta (SSP-RK43) scheme, in time for computation of one dimensional Kuramoto-Sivashinsky equation (KSE), arises in the study of flame front propagation, phase turbulence in reaction-diffusion system and in many other biological and chemical processes. The efficiency of proposed scheme is confirmed by six test problems with known exact solutions. The numerical results demonstrate the reliability and efficiency of the algorithm developed.},
  langid = {english},
  annotation = {GSCC: 0000028},
  file = {/home/elessar/Zotero/storage/KYUDE5AR/Singh et al. - 2018 - A note on solving the fourth-order Kuramoto-Sivash.pdf}
}

@article{skantzosMagnetizationEnumeratorRealvalued2003,
  title = {Magnetization Enumerator of Real-Valued Symmetric Channels in {{Gallager}} Error-Correcting Codes},
  author = {Skantzos, N. S. and Van Mourik, J. and Saad, D.},
  year = {2003},
  month = mar,
  journal = {Physical Review E},
  volume = {67},
  number = {3},
  publisher = {American Physical Society (APS)},
  issn = {1063-651X, 1095-3787},
  doi = {10.1103/physreve.67.037101},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/Q4YPRL4I/Skantzos et al. - 2003 - Magnetization enumerator of real-valued symmetric channels in Gallager error-correcting codes.pdf}
}

@article{smithLearningContinuousChaotic2022,
  title = {Learning Continuous Chaotic Attractors with a Reservoir Computer},
  author = {Smith, Lindsay M. and Kim, Jason Z. and Lu, Zhixin and Bassett, Dani S.},
  year = {2022},
  month = jan,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {32},
  number = {1},
  pages = {011101},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0075572},
  urldate = {2025-08-12},
  abstract = {Neural systems are well known for their ability to learn and store information as memories. Even more impressive is their ability to abstract these memories to create complex internal representations, enabling advanced functions such as the spatial manipulation of mental representations. While recurrent neural networks (RNNs) are capable of representing complex information, the exact mechanisms of how dynamical neural systems perform abstraction are still not well-understood, thereby hindering the development of more advanced functions. Here, we train a 1000-neuron RNN---a reservoir computer (RC)---to abstract a continuous dynamical attractor memory from isolated examples of dynamical attractor memories. Furthermore, we explain the abstraction mechanism with a new theory. By training the RC on isolated and shifted examples of either stable limit cycles or chaotic Lorenz attractors, the RC learns a continuum of attractors as quantified by an extra Lyapunov exponent equal to zero. We propose a theoretical mechanism of this abstraction by combining ideas from differentiable generalized synchronization and feedback dynamics. Our results quantify abstraction in simple neural systems, enabling us to design artificial RNNs for abstraction and leading us toward a neural basis of abstraction.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/ZIIHWISR/Smith et al. - 2022 - Learning continuous chaotic attractors with a reservoir computer.pdf}
}

@misc{sohl-dicksteinBoundaryNeuralNetwork2024,
  title = {The Boundary of Neural Network Trainability Is Fractal},
  author = {{Sohl-Dickstein}, Jascha},
  year = {2024},
  month = feb,
  number = {arXiv:2402.06184},
  eprint = {2402.06184},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.06184},
  urldate = {2025-01-07},
  abstract = {Some fractals -- for instance those associated with the Mandelbrot and quadratic Julia sets -- are computed by iterating a function, and identifying the boundary between hyperparameters for which the resulting series diverges or remains bounded. Neural network training similarly involves iterating an update function (e.g. repeated steps of gradient descent), can result in convergent or divergent behavior, and can be extremely sensitive to small changes in hyperparameters. Motivated by these similarities, we experimentally examine the boundary between neural network hyperparameters that lead to stable and divergent training. We find that this boundary is fractal over more than ten decades of scale in all tested configurations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Nonlinear Sciences - Chaotic Dynamics},
  file = {/home/elessar/Zotero/storage/93QRUH5S/Sohl-Dickstein - 2024 - The boundary of neural network trainability is fractal.pdf;/home/elessar/Zotero/storage/8PQX5RK3/2402.html}
}

@article{songExploringNonlinearDynamics2023,
  title = {Exploring Nonlinear Dynamics and Network Structures in {{Kuramoto}} Systems Using Machine Learning Approaches},
  author = {Song, Je Ung and Choi, Kwangjong and Oh, Soo Min and Kahng, B.},
  year = {2023},
  month = jul,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {33},
  number = {7},
  pages = {073148},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0153229},
  urldate = {2023-08-09},
  abstract = {Recent advances in machine learning (ML) have facilitated its application to a wide range of systems, from complex to quantum. Reservoir computing algorithms have proven particularly effective for studying nonlinear dynamical systems that exhibit collective behaviors, such as synchronizations and chaotic phenomena, some of which still remain unclear. Here, we apply ML approaches to the Kuramoto model to address several intriguing problems, including identifying the transition point and criticality of a hybrid synchronization transition, predicting future chaotic behaviors, and understanding network structures from chaotic patterns. Our proposed method also has further implications, such as inferring the structure of neural networks from electroencephalogram signals. This study, finally, highlights the potential of ML approaches for advancing our understanding of complex systems.},
  langid = {english},
  annotation = {GSCC: 0000006},
  file = {/home/elessar/Zotero/storage/HPPVV329/Song et al. - 2023 - Exploring nonlinear dynamics and network structure.pdf;/home/elessar/Zotero/storage/R3TKZT2E/Song et al. - 2023 - Exploring nonlinear dynamics and network structure.pdf}
}

@article{sotoStatisticalTestingRandom,
  title = {Statistical {{Testing}} of {{Random Number Generators}}},
  author = {Soto, Juan},
  abstract = {Random Number Generators1 (RNGs) are an important building block for algorithms and protocols in cryptography. They are paramount in the construction of encryption keys and other cryptographic algorithm parameters. In practice, statistical testing is employed to gather evidence that a generator indeed produces numbers that appear to be random. Few resources are readily available to researchers in academia and industry who wish to analyze their newly developed RNG. To address this problem, NIST has developed new metrics that may be employed to investigate the randomness of cryptographic RNGs. In this paper, issues such as statistical test suites, evaluation frameworks, and the interpretation of results are addressed.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/42AMXUXT/Soto - Statistical Testing of Random Number Generators.pdf}
}

@inproceedings{speidelAnalyticUpperBound2012,
  title = {An Analytic Upper Bound on {{T-complexity}}},
  booktitle = {2012 {{IEEE International Symposium}} on {{Information Theory Proceedings}}},
  author = {Speidel, Ulrich and Gulliver, T. Aaron},
  year = {2012},
  month = jul,
  pages = {2706--2710},
  publisher = {IEEE},
  address = {Cambridge, MA, USA},
  doi = {10.1109/isit.2012.6284014},
  urldate = {2025-07-15},
  abstract = {The Titchener T-complexity CT of a string has applications in, e.g., randomness testing, event detection and similarity comparison. Like the Lempel-Ziv production complexity, the upper bound of CT is demonstrably not a linear function of the string length. Knowledge of the bound for a given length is however required in order to convert CT into a measure with linear upper bound such as Titchener's T-information. For this reason, the upper bound of CT has been investigated before by several authors, with various asymptotic solutions proposed. We present a new analytic closed-form asymptotic upper bound for CT based on the Hurwitz-Lerch zeta function.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/N3MLVRYC/Speidel and Gulliver - 2012 - An analytic upper bound on T-complexity.pdf}
}

@inproceedings{speidelNoteEstimationString2009,
  title = {A Note on the Estimation of String Complexity for Short Strings},
  booktitle = {2009 7th {{International Conference}} on {{Information}}, {{Communications}} and {{Signal Processing}} ({{ICICS}})},
  author = {Speidel, Ulrich},
  year = {2009},
  month = dec,
  pages = {1--5},
  publisher = {IEEE},
  address = {Macau, China},
  doi = {10.1109/icics.2009.5397536},
  urldate = {2025-07-15},
  abstract = {While Kolmogorov's well-known results show that absolute string complexity is not computable, its estimation is nevertheless of importance in fields such as randomness testing, data compression evaluation, similarity measurement and event detection. Several complexity estimators have been developed over the years, with the Lempel-Ziv parsers being the most prominent. The estimators' asymptotic behaviour for long strings is generally compatible, but they differ considerably in the domain of short strings. Short strings are however exactly the kind of data encountered in many of the practical application areas. This paper proposes a method for evaluating the comparative performance of such estimators and presents experimental results for Lempel-Ziv parsers and a more recent estimator, the Tcomplexity.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/FMZENN6B/Speidel - 2009 - A note on the estimation of string complexity for short strings.pdf}
}

@inproceedings{spinnatoEnhancingEchoState2024,
  title = {Enhancing {{Echo State Networks}} with {{Gradient-based Explainability Methods}}},
  booktitle = {{{ESANN}} 2024 Proceesdings},
  author = {Spinnato, Francesco and Cossu, Andrea and Guidotti, Riccardo and Ceni, Andrea and Gallicchio, Claudio and Bacciu, Davide},
  year = {2024},
  pages = {17--22},
  publisher = {Ciaco - i6doc.com},
  address = {Bruges (Belgium) and online},
  doi = {10.14428/esann/2024.ES2024-78},
  urldate = {2025-03-07},
  abstract = {Recurrent Neural Networks are effective for analyzing temporal data, such as time series, but they often require costly and time-intensive training. Echo State Networks simplify the training process by using a fixed recurrent layer, the reservoir, and a trainable output layer, the readout. In sequence classification problems, the readout typically receives only the final state of the reservoir. However, averaging all states can sometimes be beneficial. In this work, we assess whether a weighted average of hidden states can enhance the Echo State Network performance. To this end, we propose a gradient-based, explainable technique to guide the contribution of each hidden state towards the final prediction. We show that our approach outperforms the naive average, as well as other baselines, in time series classification, particularly on noisy data.},
  isbn = {978-2-87587-090-2},
  langid = {english},
  file = {/home/elessar/Zotero/storage/QRLDYQJ2/Spinnato et al. - 2024 - Enhancing Echo State Networks with Gradient-based Explainability Methods.pdf}
}

@inproceedings{spracklinFilteringSpamUsing2007,
  title = {Filtering {{Spam Using Kolmogorov Complexity Estimates}}},
  booktitle = {21st {{International Conference}} on {{Advanced Information Networking}} and {{Applications Workshops}} ({{AINAW}}'07)},
  author = {Spracklin, L.M. and Saxton, L.V.},
  year = {2007},
  pages = {321--328},
  publisher = {IEEE},
  address = {Niagara Falls, ON, Canada},
  doi = {10.1109/ainaw.2007.184},
  urldate = {2025-07-25},
  abstract = {This paper introduces an adaptive filter which filters spam email based on Kolmogorov complexity estimates. The complexity filter is first trained exactly like a Bayesian filter. Each email is mapped to a string representation in which the tokens or words are represented by either 0 or 1. Tokens associated with spam are represented by 1 whereas those associated with non-spam, or ham, are represented by 0. Common tokens are ignored.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/8FTWZ3BJ/Spracklin and Saxton - 2007 - Filtering Spam Using Kolmogorov Complexity Estimates.pdf}
}

@article{sprottAlgebraicallySimpleChaotic2000,
  title = {Algebraically {{Simple Chaotic Flows}}},
  author = {Sprott, J C},
  year = {2000},
  abstract = {It came as a surprise to most scientists when Lorenz in 1963 discovered chaos in a simple system of three autonomous ordinary differential equations with two quadratic nonlinearities. This paper reviews efforts over the subsequent years to discover even simpler examples of chaotic flows. There is reason to believe that the algebraically simplest examples of chaotic flows with quadratic and piecewise linear nonlinearities have now been identified. The properties of these and other simple systems will be described.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/ZHWJ8GBW/Sprott - 2000 - Algebraically Simple Chaotic Flows.pdf}
}

@book{sprottChaosTimeseriesAnalysis2006,
  title = {Chaos and Time-Series Analysis},
  author = {Sprott, Julien C.},
  year = {2006},
  edition = {2. Repr},
  publisher = {Oxford Univ. Press},
  address = {Oxford},
  isbn = {978-0-19-850840-3 978-0-19-850839-7},
  langid = {english},
  file = {/home/elessar/Zotero/storage/D53Y8QFU/Sprott - 2006 - Chaos and time-series analysis.pdf}
}

@article{srinivasanParallelMachineLearning2022,
  title = {Parallel {{Machine Learning}} for {{Forecasting}} the {{Dynamics}} of {{Complex Networks}}},
  author = {Srinivasan, Keshav and Coble, Nolan and Hamlin, Joy and Antonsen, Thomas and Ott, Edward and Girvan, Michelle},
  year = {2022},
  month = apr,
  journal = {Physical Review Letters},
  volume = {128},
  number = {16},
  pages = {164101},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.128.164101},
  urldate = {2024-01-10},
  langid = {english},
  annotation = {GSCC: 0000049},
  file = {/home/elessar/Zotero/storage/U8TW7QH4/Srinivasan et al. - 2022 - Parallel Machine Learning for Forecasting the Dyna.pdf}
}

@article{staigerConstructiveDimensionEquals2005,
  title = {Constructive Dimension Equals {{Kolmogorov}} Complexity},
  author = {Staiger, Ludwig},
  year = {2005},
  month = feb,
  journal = {Information Processing Letters},
  volume = {93},
  number = {3},
  pages = {149--153},
  publisher = {Elsevier BV},
  issn = {0020-0190},
  doi = {10.1016/j.ipl.2004.09.023},
  urldate = {2025-07-25},
  abstract = {We derive the coincidence of Lutz's constructive dimension and Kolmogorov complexity for sets of infinite strings from Levin's early result on the existence of an optimal left computable cylindrical semi-measure M via simple calculations.  2004 Elsevier B.V. All rights reserved.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/P3MWGFTX/Staiger - 2005 - Constructive dimension equals Kolmogorov complexity.pdf}
}

@book{starikovskayaComputingLempelZivFactorization2012,
  title = {Computing {{Lempel-Ziv Factorization Online}}},
  author = {Starikovskaya, Tatiana},
  year = {2012},
  eprint = {1202.5233},
  primaryclass = {cs},
  issn = {0302-9743, 1611-3349},
  doi = {10.1007/978-3-642-32589-2},
  urldate = {2025-07-15},
  abstract = {We present an algorithm which computes the Lempel-Ziv factorization of a word W of length n online in the following sense: it reads W starting from the left, and, after reading each r = O(log n) characters of W , updates the Lempel-Ziv factorization. The algorithm requires O(n) bits of space and O(n log2 n) time. The basis of the algorithm is a sparse suffix tree combined with wavelet trees.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/home/elessar/Zotero/storage/HBEVJRMF/Starikovskaya - 2012 - Computing Lempel-Ziv Factorization Online.pdf}
}

@article{starkObservingComplexitySeeing2000,
  title = {Observing Complexity, Seeing Simplicity},
  author = {Stark, Jaroslav},
  editor = {Thompson, J. M. T.},
  year = {2000},
  month = jan,
  journal = {Philosophical Transactions of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences},
  volume = {358},
  number = {1765},
  pages = {41--61},
  publisher = {The Royal Society},
  issn = {1364-503X, 1471-2962},
  doi = {10.1098/rsta.2000.0518},
  urldate = {2025-07-25},
  langid = {english},
  file = {/home/elessar/Zotero/storage/F8PIYZIR/Stark - 2000 - Observing complexity, seeing simplicity.pdf}
}

@inproceedings{steilBackpropagationdecorrelationOnlineRecurrent2004,
  title = {Backpropagation-Decorrelation: Online Recurrent Learning with {{O}}({{N}}) Complexity},
  shorttitle = {Backpropagation-Decorrelation},
  booktitle = {2004 {{IEEE International Joint Conference}} on {{Neural Networks}} ({{IEEE Cat}}. {{No}}.{{04CH37541}})},
  author = {Steil, J.J.},
  year = {2004},
  volume = {2},
  pages = {843--848},
  publisher = {IEEE},
  address = {Budapest, Hungary},
  doi = {10.1109/IJCNN.2004.1380039},
  urldate = {2023-05-30},
  abstract = {We introduce a new learning rule for fully recurrent neural networks which we call Backpropagation-Decorrelation rule (BPDC). It combines important principles: one-step backpropagation of errors and the usage of temporal memory in the network dynamics by means of decorrelation of activations. The BPDC rule is derived and theoretically justified from regarding learning as a constraint optimization problem and applies uniformly in discrete and continuous time. It is very easy to implement, and has a minimal complexity of 2N multiplications per time-step in the single output case. Nevertheless we obtain fast tracking and excellent performance in some benchmark problems including the Mackey-Glass time-series.},
  isbn = {978-0-7803-8359-3},
  langid = {english},
  annotation = {GSCC: 0000336},
  file = {/home/elessar/Zotero/storage/5SBB42C5/Steil - 2004 - Backpropagation-decorrelation online recurrent le.pdf}
}

@book{stevensDeepLearningPyTorch2020,
  title = {Deep Learning with {{PyTorch}}},
  author = {Stevens, Eli and Antiga, Luca and Viehmann, Thomas},
  year = {2020},
  publisher = {Manning Publications Co},
  address = {Shelter Island, NY},
  isbn = {978-1-61729-526-3},
  langid = {english},
  lccn = {QA76.87 .S745 2020},
  keywords = {Artificial intelligence,Machine learning,Neural networks (Computer science),Python (Computer program language)},
  annotation = {GSCC: 0000298},
  file = {/home/elessar/Zotero/storage/4NNLNPPR/Stevens et al. - 2020 - Deep learning with PyTorch.pdf}
}

@article{stottparkerMonteCarloArithmetic2000,
  title = {Monte {{Carlo}} Arithmetic: How to Gamble with Floating Point and Win},
  shorttitle = {Monte {{Carlo}} Arithmetic},
  author = {Stott Parker, D. and Pierce, B. and Eggert, P.R.},
  year = {2000},
  journal = {Computing in Science \& Engineering},
  volume = {2},
  number = {4},
  pages = {58--68},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {1521-9615},
  doi = {10.1109/5992.852391},
  urldate = {2025-07-25},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/WAZBFL8H/Stott Parker et al. - 2000 - Monte Carlo arithmetic how to gamble with floating point and win.PDF}
}

@article{straussDesignStrategiesWeight2012,
  title = {Design {{Strategies}} for {{Weight Matrices}} of {{Echo State Networks}}},
  author = {Strauss, Tobias and Wustlich, Welf and Labahn, Roger},
  year = {2012},
  month = dec,
  journal = {Neural Computation},
  volume = {24},
  number = {12},
  pages = {3246--3276},
  publisher = {MIT Press - Journals},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco_a_00374},
  urldate = {2025-05-01},
  abstract = {This article develops approaches to generate dynamical reservoirs of echo state networks with desired properties reducing the amount of randomness. It is possible to create weight matrices with a predefined singular value spectrum. The procedure guarantees stability (echo state property). We prove the minimization of the impact of noise on the training process. The resulting reservoir types are strongly related to reservoirs already known in the literature. Our experiments show that well-chosen input weights can improve performance.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/8CA6TECJ/Strauss et al. - 2012 - Design Strategies for Weight Matrices of Echo State Networks.pdf}
}

@book{strogatzNonlinearDynamicsChaos2015,
  title = {Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chemistry, and Engineering},
  shorttitle = {Nonlinear Dynamics and Chaos},
  author = {Strogatz, Steven H.},
  year = {2015},
  edition = {Second edition},
  publisher = {Westview Press, a member of the Perseus Books Group},
  address = {Boulder, CO},
  isbn = {978-0-8133-4910-7},
  langid = {english},
  lccn = {Q172.5.C45 S767 2015},
  keywords = {Chaotic behavior in systems,Dynamics,Nonlinear theories},
  annotation = {GSCC: 0000024 \\
OCLC: ocn842877119},
  file = {/home/elessar/Zotero/storage/EYJ35UZB/Strogatz - 2015 - Nonlinear dynamics and chaos with applications to.pdf}
}

@book{strogatzNonlinearDynamicsChaos2018,
  title = {Nonlinear {{Dynamics}} and {{Chaos}}},
  author = {Strogatz, Steven H.},
  year = {2018},
  month = may,
  edition = {0},
  publisher = {CRC Press},
  doi = {10.1201/9780429492563},
  urldate = {2025-08-12},
  isbn = {978-0-429-96111-3},
  langid = {english}
}

@article{strongEntropyInformationNeural1998,
  title = {Entropy and {{Information}} in {{Neural Spike Trains}}},
  author = {Strong, S. P. and Koberle, Roland and De Ruyter Van Steveninck, Rob R. and Bialek, William},
  year = {1998},
  month = jan,
  journal = {Physical Review Letters},
  volume = {80},
  number = {1},
  pages = {197--200},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.80.197},
  urldate = {2025-07-29},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/NILLK9ST/Strong et al. - 1998 - Entropy and Information in Neural Spike Trains.pdf}
}

@article{suarezConnectomebasedReservoirComputing2024,
  title = {Connectome-Based Reservoir Computing with the Conn2res Toolbox},
  author = {Su{\'a}rez, Laura E. and Mihalik, Agoston and Milisav, Filip and Marshall, Kenji and Li, Mingze and V{\'e}rtes, Petra E. and Lajoie, Guillaume and Misic, Bratislav},
  year = {2024},
  month = jan,
  journal = {Nature Communications},
  volume = {15},
  number = {1},
  pages = {656},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-44900-4},
  urldate = {2024-04-30},
  abstract = {Abstract             The connection patterns of neural circuits form a complex network. How signaling in these circuits manifests as complex cognition and adaptive behaviour remains the central question in neuroscience. Concomitant advances in connectomics and artificial intelligence open fundamentally new opportunities to understand how connection patterns shape computational capacity in biological brain networks. Reservoir computing is a versatile paradigm that uses high-dimensional, nonlinear dynamical systems to perform computations and approximate cognitive functions. Here we present : an open-source Python toolbox for implementing biological neural networks as artificial neural networks.  is modular, allowing arbitrary network architecture and dynamics to be imposed. The toolbox allows researchers to input connectomes reconstructed using multiple techniques, from tract tracing to noninvasive diffusion imaging, and to impose multiple dynamical systems, from spiking neurons to memristive dynamics. The versatility of the  toolbox allows us to ask new questions at the confluence of neuroscience and artificial intelligence. By reconceptualizing function as computation,  sets the stage for a more mechanistic understanding of structure-function relationships in brain networks.},
  langid = {english},
  annotation = {GSCC: 0000009},
  file = {/home/elessar/Zotero/storage/ST727B4M/Suárez et al. - 2024 - Connectome-based reservoir computing with the conn.pdf}
}

@article{sunDeepBeliefEchostate2017,
  title = {Deep Belief Echo-State Network and Its Application to Time Series Prediction},
  author = {Sun, Xiaochuan and Li, Tao and Li, Qun and Huang, Yue and Li, Yingqi},
  year = {2017},
  month = aug,
  journal = {Knowledge-Based Systems},
  volume = {130},
  pages = {17--29},
  issn = {09507051},
  doi = {10.1016/j.knosys.2017.05.022},
  urldate = {2025-03-12},
  abstract = {Deep belief network (DBN) has attracted many attentions in time series prediction. However, the DBNbased methods fail to provide favorable prediction results due to the congenital defects of the backpropagation method, such as slow convergence and local optimum. To address the problems, we propose a deep belief echo-state network (DBEN) for time series prediction. In the new architecture, DBN is employed for feature learning in an unsupervised fashion, which can effectively extract hierarchical data features. An innovative regression layer, embedding an echo-state learning mechanism instead of the traditional back-propagation method, is built on top of DBN for supervised prediction. To our best knowledge, this is the first paper that applies the echo state network methodology to deep learning. The resulted model, combining the merits of DBN and ESN, provides a more robust alternative to conventional deep neural networks for the superior prediction capacity. Extensive experimental results show that our DBEN can achieve a significant enhancement in the prediction performance, learning speed, and short-term memory capacity.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/P3WRZ87U/Sun et al. - 2017 - Deep belief echo-state network and its application to time series prediction.pdf}
}

@article{sussilloGeneratingCoherentPatterns2009,
  title = {Generating {{Coherent Patterns}} of {{Activity}} from {{Chaotic Neural Networks}}},
  author = {Sussillo, David and Abbott, L. F.},
  year = {2009},
  month = aug,
  journal = {Neuron},
  volume = {63},
  number = {4},
  pages = {544--557},
  publisher = {Elsevier},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2009.07.018},
  urldate = {2025-02-17},
  langid = {english},
  pmid = {19709635},
  keywords = {SYSNEURO},
  file = {/home/elessar/Zotero/storage/G7VYK6GH/Sussillo and Abbott - 2009 - Generating Coherent Patterns of Activity from Chaotic Neural Networks.pdf}
}

@article{susskindSusskindQuantumMechanics,
  title = {Susskind {{Quantum Mechanics}}},
  author = {Susskind, Leonard and Friedman, Art},
  langid = {english},
  file = {/home/elessar/Zotero/storage/VI8NBN4U/Susskind and Friedman - Quantum Mechanics.pdf}
}

@article{suzukiHowDeepBrain2023,
  title = {How Deep Is the Brain? {{The}} Shallow Brain Hypothesis},
  shorttitle = {How Deep Is the Brain?},
  author = {Suzuki, Mototaka and Pennartz, Cyriel M. A. and Aru, Jaan},
  year = {2023},
  month = dec,
  journal = {Nature Reviews Neuroscience},
  volume = {24},
  number = {12},
  pages = {778--791},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/s41583-023-00756-z},
  urldate = {2024-01-10},
  langid = {english},
  annotation = {GSCC: 0000024},
  file = {/home/elessar/Zotero/storage/7F477UE5/Suzuki et al. - 2023 - How deep is the brain The shallow brain hypothesi.pdf}
}

@book{symonSymonMechanics1971,
  title = {Symon {{Mechanics}}},
  author = {Symon, Keith R.},
  year = {1971},
  series = {Addison-{{Wesley}} Series in Physics},
  edition = {3rd ed},
  publisher = {Addison-Wesley publ},
  address = {Reading (Mass.) Menlo Park (Calif.) London [etc.]},
  isbn = {978-0-201-07392-8},
  langid = {english},
  lccn = {531.015 15},
  file = {/home/elessar/Zotero/storage/EGZACGNI/Symon - 1971 - Mechanics.pdf}
}

@article{szczepanskiCharacterizingSpikeTrains2004,
  title = {Characterizing Spike Trains with {{Lempel}}--{{Ziv}} Complexity},
  author = {Szczepa{\'n}ski, J. and Amig{\'o}, J.M. and Wajnryb, E. and {Sanchez-Vives}, M.V.},
  year = {2004},
  month = jun,
  journal = {Neurocomputing},
  volume = {58--60},
  pages = {79--84},
  publisher = {Elsevier BV},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2004.01.026},
  urldate = {2025-07-15},
  abstract = {We review several applications of Lempel--Ziv complexity to the characterization of neural responses. In particular, Lempel--Ziv complexity allows to estimate the entropy of binned spike trains in an alternative way to the usual method based on the relative frequencies of words, withteh de7nitive advantage of no requiring very long registers. We also use complexity to discriminate neural responses to di8erent kinds of stimuli and to evaluate the number of states of neuronal sources. c{\copyright} 2004 Elsevier B.V. All rights reserved.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/L6689ZAL/Szczepański et al. - 2004 - Characterizing spike trains with Lempel–Ziv complexity.pdf}
}

@article{szczepanskiDistributionFunctionComplexity2009,
  title = {On the Distribution Function of the Complexity of Finite Sequences},
  author = {Szczepanski, Janusz},
  year = {2009},
  month = apr,
  journal = {Information Sciences},
  volume = {179},
  number = {9},
  pages = {1217--1220},
  publisher = {Elsevier BV},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2008.12.019},
  urldate = {2025-07-25},
  abstract = {Investigations of complexity of sequences lead to important applications such as effective data compression, testing of randomness, discriminating between information sources and many others. In this paper we establish formulae describing the distribution functions of random variables representing the complexity of finite sequences introduced by Lempel and Ziv in 1976. It is known that this quantity can be used as an estimator of entropy. We show that the distribution functions depend affinely on the probabilities of the so-called ``exact'' sequences.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/TESJBXV8/Szczepanski - 2009 - On the distribution function of the complexity of finite sequences.pdf}
}

@incollection{takensDetectingStrangeAttractors1981,
  title = {Detecting Strange Attractors in Turbulence},
  booktitle = {Dynamical {{Systems}} and {{Turbulence}}, {{Warwick}} 1980},
  author = {Takens, Floris},
  editor = {Rand, David and Young, Lai-Sang},
  year = {1981},
  volume = {898},
  pages = {366--381},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/BFb0091924},
  urldate = {2025-08-12},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-540-11171-9 978-3-540-38945-3},
  file = {/home/elessar/Zotero/storage/86W4DPBD/Takens - 1981 - Detecting strange attractors in turbulence.pdf}
}

@article{talebinejadLempelZivComplexity2011,
  title = {A {{Lempel}}--{{Ziv}} Complexity Measure for Muscle Fatigue Estimation},
  author = {Talebinejad, Mehran and Chan, Adrian D.C. and Miri, Ali},
  year = {2011},
  month = apr,
  journal = {Journal of Electromyography and Kinesiology},
  volume = {21},
  number = {2},
  pages = {236--241},
  publisher = {Elsevier BV},
  issn = {1050-6411},
  doi = {10.1016/j.jelekin.2010.12.003},
  urldate = {2025-07-15},
  abstract = {This paper presents a Lempel--Ziv complexity measure for analysis of surface electromyography signals. The Lempel--Ziv measure provides a metric for the number of distinct deterministic patterns and the rate of their creation in signals. We propose a ternary Lempel--Ziv measure, improving upon the binary Lempel--Ziv measure, and making it more suited for the analysis of biological signals. The Lempel--Ziv measure is evaluated with a muscle fatigue experiment in which participants perform static, cyclic, and random contractions. Results show this complexity measure shows a greater correlation to a steadily increasing muscle fatigue level compared to the conventional median frequency. This measure is computationally easy to compute and does not require power spectrum estimation and signal stationarity assumptions.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/R399PB5H/Talebinejad et al. - 2011 - A Lempel–Ziv complexity measure for muscle fatigue estimation.pdf}
}

@inproceedings{tamakoshiRunLengthEncoding2013,
  title = {From {{Run Length Encoding}} to {{LZ78}} and {{Back Again}}},
  booktitle = {2013 {{Data Compression Conference}}},
  author = {Tamakoshi, Y. and Tomohiro, I. and Inenaga, S. and Bannai, H. and Takeda, M.},
  year = {2013},
  month = mar,
  pages = {143--152},
  publisher = {IEEE},
  address = {Snowbird, UT},
  doi = {10.1109/dcc.2013.22},
  urldate = {2025-07-15},
  abstract = {In this paper, we present efficient algorithms for interconversion between Lempel-Ziv 78 (LZ78) encoding and run length encoding (RLE). We show how, given an RLE of size n for a string S, we can compute the corresponding LZ78 encoding of size m for S in O((n + m) log {$\sigma$}) time, where {$\sigma$} is the number of distinct characters appearing in S. We also show how, given an LZ78 encoding of size m for a string S, we can compute the corresponding RLE of size n in O(n + m) time. Both algorithms use O(m) extra working space.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/VEA2HGTQ/Tamakoshi et al. - 2013 - From Run Length Encoding to LZ78 and Back Again.pdf}
}

@article{tamuraTransferRLSMethodTransferFORCE2021,
  title = {Transfer-{{RLS}} Method and Transfer-{{FORCE}} Learning for Simple and Fast Training of Reservoir Computing Models},
  author = {Tamura, Hiroto and Tanaka, Gouhei},
  year = {2021},
  month = nov,
  journal = {Neural Networks},
  volume = {143},
  pages = {550--563},
  issn = {08936080},
  doi = {10.1016/j.neunet.2021.06.031},
  urldate = {2025-03-12},
  abstract = {Reservoir computing is a machine learning framework derived from a special type of recurrent neural network. Following recent advances in physical reservoir computing, some reservoir computing devices are thought to be promising as energy-efficient machine learning hardware for real-time information processing. To realize efficient online learning with low-power reservoir computing devices, it is beneficial to develop fast convergence learning methods with simpler operations. This study proposes a training method located in the middle between the recursive least squares (RLS) method and the least mean squares (LMS) method, which are standard online learning methods for reservoir computing models. The RLS method converges fast but requires updates of a huge matrix called a gain matrix, whereas the LMS method does not use a gain matrix but converges very slow. On the other hand, the proposed method called a transfer-RLS method does not require updates of the gain matrix in the main-training phase by updating that in advance (i.e., in a pre-training phase). As a result, the transferRLS method can work with simpler operations than the original RLS method without sacrificing much convergence speed. We numerically and analytically show that the transfer-RLS method converges much faster than the LMS method. Furthermore, we show that a modified version of the transfer-RLS method (called transfer-FORCE learning) can be applied to the first-order reduced and controlled error (FORCE) learning for a reservoir computing model with a closed-loop, which is challenging to train.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/5PARB5QI/Tamura and Tanaka - 2021 - Transfer-RLS method and transfer-FORCE learning for simple and fast training of reservoir computing.pdf}
}

@article{tanakaRecentAdvancesPhysical2019,
  title = {Recent Advances in Physical Reservoir Computing: {{A}} Review},
  shorttitle = {Recent Advances in Physical Reservoir Computing},
  author = {Tanaka, Gouhei and Yamane, Toshiyuki and H{\'e}roux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
  year = {2019},
  month = jul,
  journal = {Neural Networks},
  volume = {115},
  pages = {100--123},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.03.005},
  urldate = {2023-05-30},
  abstract = {Reservoir computing is a computational framework suited for temporal/sequential data processing. It is derived from several recurrent neural network models, including echo state networks and liquid state machines. A reservoir computing system consists of a reservoir for mapping inputs into a high-dimensional space and a readout for pattern analysis from the high-dimensional states in the reservoir. The reservoir is fixed and only the readout is trained with a simple method such as linear regression and classification. Thus, the major advantage of reservoir computing compared to other recurrent neural networks is fast learning, resulting in low training cost. Another advantage is that the reservoir without adaptive updating is amenable to hardware implementation using a variety of physical systems, substrates, and devices. In fact, such physical reservoir computing has attracted increasing attention in diverse fields of research. The purpose of this review is to provide an overview of recent advances in physical reservoir computing by classifying them according to the type of the reservoir. We discuss the current issues and perspectives related to physical reservoir computing, in order to further expand its practical applications and develop next-generation machine learning systems.},
  langid = {english},
  annotation = {GSCC: 0001676},
  file = {/home/elessar/Zotero/storage/4WNJKLVB/Tanaka et al. - 2019 - Recent advances in physical reservoir computing A.pdf}
}

@article{tangEnsemblePreTrainingApproach2024,
  title = {Ensemble and {{Pre-Training Approach}} for {{Echo State Network}} and {{Extreme Learning Machine Models}}},
  author = {Tang, Lingyu and Wang, Jun and Wang, Mengyao and Zhao, Chunyu},
  year = {2024},
  month = feb,
  journal = {Entropy},
  volume = {26},
  number = {3},
  pages = {215},
  issn = {1099-4300},
  doi = {10.3390/e26030215},
  urldate = {2024-11-25},
  abstract = {The echo state network (ESN) is a recurrent neural network that has yielded state-of-the-art results in many areas owing to its rapid learning ability and the fact that the weights of input neurons and hidden neurons are fixed throughout the learning process. However, the setting procedure for initializing the ESN's recurrent structure may lead to difficulties in designing a sound reservoir that matches a specific task. This paper proposes an improved pre-training method to adjust the model's parameters and topology to obtain an adaptive reservoir for a given application. Two strategies, namely global random selection and ensemble training, are introduced to pre-train the randomly initialized ESN model. Specifically, particle swarm optimization is applied to optimize chosen fixed and global weight values within the network, and the reliability and stability of the pre-trained model are enhanced by employing the ensemble training strategy. In addition, we test the feasibility of the model for time series prediction on six benchmarks and two real-life datasets. The experimental results show a clear enhancement in the ESN learning results. Furthermore, the proposed global random selection and ensemble training strategies are also applied to pre-train the extreme learning machine (ELM), which has a similar training process to the ESN model. Numerical experiments are subsequently carried out on the above-mentioned eight datasets. The experimental findings consistently show that the performance of the proposed pre-trained ELM model is also improved significantly. The suggested two strategies can thus enhance the ESN and ELM models' prediction accuracy and adaptability.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  pmcid = {PMC10968746},
  pmid = {38539726},
  file = {/home/elessar/Zotero/storage/JAIJNCPB/Tang et al. - 2024 - Ensemble and Pre-Training Approach for Echo State Network and Extreme Learning Machine Models.pdf}
}

@article{thiedeGradientBasedHyperparameter2019,
  title = {Gradient Based Hyperparameter Optimization in {{Echo State Networks}}},
  author = {Thiede, Luca Anthony and Parlitz, Ulrich},
  year = {2019},
  month = jul,
  journal = {Neural Networks},
  volume = {115},
  pages = {23--29},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.02.001},
  urldate = {2025-03-12},
  abstract = {Like most machine learning algorithms, Echo State Networks possess several hyperparameters that have to be carefully tuned for achieving best performance. For minimizing the error on a specific task, we present a gradient based optimization algorithm, for the input scaling, the spectral radius, the leaking rate, and the regularization parameter.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/GAC7PYY3/Thiede and Parlitz - 2019 - Gradient based hyperparameter optimization in Echo State Networks.pdf}
}

@article{thomsenStructureSoficShift2004,
  title = {On the Structure of a Sofic Shift Space},
  author = {Thomsen, Klaus},
  year = {2004},
  month = jan,
  journal = {Transactions of the American Mathematical Society},
  volume = {356},
  number = {9},
  pages = {3557--3619},
  publisher = {American Mathematical Society (AMS)},
  issn = {0002-9947, 1088-6850},
  doi = {10.1090/s0002-9947-04-03437-3},
  urldate = {2025-07-15},
  abstract = {The structure of a sofic shift space is investigated, and Krieger's embedding theorem and Boyle's factor theorem are generalized to a large class of sofic shifts.},
  copyright = {https://www.ams.org/publications/copyright-and-permissions},
  langid = {english},
  file = {/home/elessar/Zotero/storage/CXZN9968/Thomsen - 2004 - On the structure of a sofic shift space.pdf}
}

@article{tinoDynamicalSystemsTemporal,
  title = {Dynamical {{Systems}} as {{Temporal Feature Spaces}}},
  author = {Tino, Peter and Tino, P},
  abstract = {Parametrised state space models in the form of recurrent networks are often used in machine learning to learn from data streams exhibiting temporal dependencies. To break the black box nature of such models it is important to understand the dynamical features of the inputdriving time series that are formed in the state space. We propose a framework for rigorous analysis of such state representations in vanishing memory state space models such as echo state networks (ESN). In particular, we consider the state space a temporal feature space and the readout mapping from the state space a kernel machine operating in that feature space. We show that: (1) The usual ESN strategy of randomly generating input-to-state, as well as state coupling leads to shallow memory time series representations, corresponding to cross-correlation operator with fast exponentially decaying coefficients; (2) Imposing symmetry on dynamic coupling yields a constrained dynamic kernel matching the input time series with straightforward exponentially decaying motifs or exponentially decaying motifs of the highest frequency; (3) Simple ring (cycle) high-dimensional reservoir topology specified only through two free parameters can implement deep memory dynamic kernels with a rich variety of matching motifs. We quantify richness of feature representations imposed by dynamic kernels and demonstrate that for dynamic kernel associated with cycle reservoir topology, the kernel richness undergoes a phase transition close to the edge of stability.},
  langid = {english},
  annotation = {GSCC: 0000031},
  file = {/home/elessar/Zotero/storage/HPVILRXX/Tino and Tino - Dynamical Systems as Temporal Feature Spaces.pdf}
}

@inproceedings{titchenerCompressorPerformanceAbsolutely,
  title = {Compressor Performance, Absolutely!},
  booktitle = {Proceedings {{DCC}} 2002. {{Data Compression Conference}}},
  author = {Titchener, M.R.},
  pages = {474},
  publisher = {IEEE Comput. Soc},
  address = {Snowbird, UT, USA},
  doi = {10.1109/dcc.2002.1000017},
  urldate = {2025-07-15},
  langid = {english},
  file = {/home/elessar/Zotero/storage/GXNE2WUK/Titchener - Compressor performance, absolutely!.pdf}
}

@inproceedings{titchenerDeterministicComputationComplexity,
  title = {Deterministic Computation of Complexity, Information and Entropy},
  booktitle = {Proceedings. 1998 {{IEEE International Symposium}} on {{Information Theory}} ({{Cat}}. {{No}}.{{98CH36252}})},
  author = {Titchener, M.R.},
  pages = {326},
  publisher = {IEEE},
  address = {Cambridge, MA, USA},
  doi = {10.1109/isit.1998.708931},
  urldate = {2025-07-15},
  abstract = {A new measure of string complexity [3]for finite strings is presented based on a specific recursive hierarchical string production process (c.f. [ 2 ] ) .From the maximal bound we deduce a relationship between complexity and total information content.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/VE5JCP7R/Titchener - Deterministic computation of complexity, information and entropy.pdf}
}

@article{titchenerDigitalEncodingMeans1984,
  title = {Digital Encoding by Means of New {{T-codes}} to Provide Improved Data Synchronisation and Message Integrity},
  author = {Titchener, M.R.},
  year = {1984},
  journal = {IEE Proceedings E Computers and Digital Techniques},
  volume = {131},
  number = {4},
  pages = {151},
  publisher = {{Institution of Engineering and Technology (IET)}},
  issn = {0143-7062},
  doi = {10.1049/ip-e.1984.0028},
  urldate = {2025-07-15},
  abstract = {A new class of variable-length synchronisable codes offers improved data integrity in the presence of additive and timing errors. Character synchronisation occurs automatically during normal decoding procedures. The estimated improvements obtainable using these codes in the transmission of ASCII text over a noisy channel are shown in a comparison with conventional serial code systems.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/9UW8SE5D/Titchener - 1984 - Digital encoding by means of new T-codes to provide improved data synchronisation and message integr.pdf}
}

@inproceedings{titchenerMeasureInformation,
  title = {A Measure of Information},
  booktitle = {Proceedings {{DCC}} 2000. {{Data Compression Conference}}},
  author = {Titchener, M.R.},
  pages = {353--362},
  publisher = {IEEE Comput. Soc},
  address = {Snowbird, UT, USA},
  doi = {10.1109/dcc.2000.838175},
  urldate = {2025-07-15},
  abstract = {Modern information theory is founded on the ideas of Hartley and Shannon, amongst others. From a practitioners standpoint, Shannon's probabilistic framework carries certain impediments for the practical measurement of information, such as requiring a{\textasciiacute} priori knowledge of a source's characteristics. Moreover, such a statistical formulation of entropy is an asymptotic limit, meaningful only within the context of an ensemble of messages. It thus fails to address the notion of an individual string having information content in of itself. However, in 1953, Cherry [1] demonstrated that Shannon's entropy could be viewed equivalently as a measure of the average number of selections required to identify each message symbol from the alphabet. Here the terminology contrasts with Shannon's probabilistic formulation, with the process of counting selection steps appearing to be meaningful for individual, isolated, finite strings. We explore this alternative approach in the context of a recursive hierarchical pattern copying (RHPC) algorithm. We use to measure the complexity of finite strings, in terms of the number of steps required to recursively construct the string from its alphabet. From this we compute an effective rate of steps-per-symbol required for linearly constructing the string. By Cherry's interpretation of Shannon's entropy, we infer this as giving asymptotic equivalence between the two approaches, but perhaps the real significance of this new way to measure information, is its applicability and usefulness in respect of evaluating individual finite strings.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/ZLDYSU56/Titchener - A measure of information.pdf}
}

@inproceedings{titchenerRealtimeMeasurementInformation2008,
  title = {Towards Real-Time Measurement of Information in a Scientific Setting},
  booktitle = {2008 6th {{International Symposium}} on {{Communication Systems}}, {{Networks}} and {{Digital Signal Processing}}},
  author = {Titchener, M.R.},
  year = {2008},
  month = jul,
  pages = {316--320},
  publisher = {IEEE},
  address = {Graz, Austria},
  doi = {10.1109/csndsp.2008.4610767},
  urldate = {2025-07-15},
  abstract = {Quantifying information has long been the domain of communications, data compression and encryption. However, developments over the last 50 years in the field of non-linear dynamics have shown information interchange to be intrinsic to physical dynamical systems and by implication, biological processes. The practical need to effectively characterise complex dynamical processes serves as motivation for the development of tools capable of measuring information and this paper describes an evolving toolset directed to quantifying information in time-series. Such data series may correspond to signals on a communications channel, or medical signals in diagnostic patient monitoring, neurological pulse trains, seismic data, or even industrial process measurements. Our tools provide a complementary approach to conventional time-series analysis and may be used to sensitively identify state, or changes-in-state in complex systems, without necesitating a detailed understanding of underlying process mechanisms. Our emphasis is on achieving real-time performance in measurement and visualisation, for diagnostic purposes.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/23TIE5AD/Titchener - 2008 - Towards real-time measurement of information in a scientific setting.pdf}
}

@article{titchenerSynchronisationProcessVariablelength1986,
  title = {Synchronisation Process for the Variable-Length {{T-codes}}},
  author = {Titchener, M.R. and Hunter, J.J.},
  year = {1986},
  journal = {IEE Proceedings E Computers and Digital Techniques},
  volume = {133},
  number = {1},
  pages = {54},
  publisher = {{Institution of Engineering and Technology (IET)}},
  issn = {0143-7062},
  doi = {10.1049/ip-e.1986.0005},
  urldate = {2025-07-15},
  abstract = {A procedure for determining synchronisation status for a T-code decoder is developed from material presented in an earlier work. A general form for a probability-transition matrix description of the T-code synchronisation process is deduced leading to a proof in which the T-codes are shown to be statistically synchronisable. By evaluating the transition probabilities for an elementary example, numerical techniques for determining the synchronisation probabilities distribution (SPD) and the expected synchronisation delay (ESD) are illustrated. Further evaluation of the ESD for a series of eleven minimal augmented binary code sets suggests asymptotic logarithmic growth of the ESD as a function of set augmentation. Practical aspects for determining decoder synchronisation status are discussed with particular reference to the elementary example introduced in the early part of the paper.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/ZC5HXC4X/Titchener and Hunter - 1986 - Synchronisation process for the variable-length T-codes.pdf}
}

@inproceedings{torresRenyiEntropyLempelZiv2008,
  title = {R{\'e}nyi Entropy and {{Lempel-Ziv}} Complexity of Mechanomyographic Recordings of Diaphragm Muscle as Indexes of Respiratory Effort},
  booktitle = {2008 30th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}}},
  author = {Torres, Abel and Fiz, Jose A. and Jane, Raimon and Laciar, Eric and Galdiz, Juan B. and Gea, Joaquim and Morera, Josep},
  year = {2008},
  month = aug,
  pages = {2112--2115},
  publisher = {IEEE},
  address = {Vancouver, BC},
  doi = {10.1109/iembs.2008.4649610},
  urldate = {2025-07-15},
  abstract = {The study of the mechanomyographic (MMG) signals of respiratory muscles is a promising technique in order to evaluate the respiratory muscles effort. A new approach for quantifying the relationship between respiratory MMG signals and respiratory effort is presented by analyzing the spatiotemporal patterns in the MMG signal using two non-linear methods: R{\'e}nyi entropy and Lempel-Ziv (LZ) complexity analysis. Both methods are well suited to the analysis of nonstationary biomedical signals of short length. In this study, MMG signals of the diaphragm muscle acquired by means of a capacitive accelerometer applied on the costal wall were analyzed. The method was tested on an animal model (dogs), and the diaphragmatic MMG signal was recorded continuously while two non anesthetized mongrel dogs performed a spontaneous ventilation protocol with an incremental inspiratory load. The performance in discriminating high and low respiratory effort levels with these two methods was analyzed with the evaluation of the Pearson correlation coefficient between the MMG parameters and respiratory effort parameters extracted from the inspiratory pressure signal. The results obtained show an increase of the MMG signal R{\'e}nyi entropy and LZ complexity values with the increase of the respiratory effort. Compared with other parameters analyzed in previous works, both R{\'e}nyi entropy and LZ complexity indexes demonstrates better performance in all the signals analyzed. Our results suggest that these nonlinear techniques are useful to detect and quantify changes in the respiratory effort by analyzing MMG respiratory signals.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/SRG5LAN9/Torres et al. - 2008 - Rényi entropy and Lempel-Ziv complexity of mechanomyographic recordings of diaphragm muscle as index.pdf}
}

@article{traversAsymptoticSynchronizationFiniteState2011,
  title = {Asymptotic {{Synchronization}} for {{Finite-State Sources}}},
  author = {Travers, Nicholas F. and Crutchfield, James P.},
  year = {2011},
  month = dec,
  journal = {Journal of Statistical Physics},
  volume = {145},
  number = {5},
  eprint = {1011.1581},
  primaryclass = {nlin},
  pages = {1202--1223},
  issn = {0022-4715, 1572-9613},
  doi = {10.1007/s10955-011-0349-x},
  urldate = {2025-07-15},
  abstract = {We extend a recent synchronization analysis of exact finite-state sources to nonexact sources for which synchronization occurs only asymptotically. Although the proof methods are quite different, the primary results remain the same. We find that an observer's average uncertainty in the source state vanishes exponentially fast and, as a consequence, an observer's average uncertainty in predicting future output converges exponentially fast to the source entropy rate.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Mathematics - Dynamical Systems,Mathematics - Information Theory,Nonlinear Sciences - Chaotic Dynamics,Statistics - Machine Learning},
  file = {/home/elessar/Zotero/storage/YWHWUKBP/Travers and Crutchfield - 2011 - Asymptotic Synchronization for Finite-State Sources.pdf}
}

@article{traversBoundsConvergenceEntropy,
  title = {Bounds on {{Convergence}} of {{Entropy Rate Approximations}} in {{Hidden Markov Processes}}},
  author = {Travers, Nicholas F},
  langid = {english},
  file = {/home/elessar/Zotero/storage/TP6CH6PP/Travers - Bounds on Convergence of Entropy Rate Approximations in Hidden Markov Processes.pdf}
}

@misc{traversEquivalenceHistoryGenerator2012,
  title = {Equivalence of {{History}} and {{Generator Epsilon-Machines}}},
  author = {Travers, Nicholas F. and Crutchfield, James P.},
  year = {2012},
  month = dec,
  number = {arXiv:1111.4500},
  eprint = {1111.4500},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1111.4500},
  urldate = {2025-07-15},
  abstract = {Epsilon-machines are minimal, unifilar presentations of stationary stochastic processes. They were originally defined in the history machine sense, as hidden Markov models whose states are the equivalence classes of infinite pasts with the same probability distribution over futures. In analyzing synchronization, though, an alternative generator definition was given: unifilar, edge-emitting hidden Markov models with probabilistically distinct states. The key difference is that history epsilon-machines are defined by a process, whereas generator epsilon-machines define a process. We show here that these two definitions are equivalent in the finite-state case.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Condensed Matter - Statistical Mechanics,Mathematics - Information Theory,Mathematics - Probability,Nonlinear Sciences - Chaotic Dynamics,Statistics - Machine Learning},
  file = {/home/elessar/Zotero/storage/HXGFPF4M/Travers and Crutchfield - 2012 - Equivalence of History and Generator Epsilon-Machines.pdf}
}

@article{traversExactSynchronizationFiniteState2011,
  title = {Exact {{Synchronization}} for {{Finite-State Sources}}},
  author = {Travers, Nicholas F. and Crutchfield, James P.},
  year = {2011},
  month = dec,
  journal = {Journal of Statistical Physics},
  volume = {145},
  number = {5},
  eprint = {1008.4182},
  primaryclass = {nlin},
  pages = {1181--1201},
  issn = {0022-4715, 1572-9613},
  doi = {10.1007/s10955-011-0342-4},
  urldate = {2025-07-25},
  abstract = {We analyze how an observer synchronizes to the internal state of a finite-state information source, using the epsilon-machine causal representation. Here, we treat the case of exact synchronization, when it is possible for the observer to synchronize completely after a finite number of observations. The more difficult case of strictly asymptotic synchronization is treated in a sequel. In both cases, we find that an observer, on average, will synchronize to the source state exponentially fast and that, as a result, the average accuracy in an observer's predictions of the source output approaches its optimal level exponentially fast as well. Additionally, we show here how to analytically calculate the synchronization rate for exact epsilon-machines and provide an efficient polynomial-time algorithm to test epsilon-machines for exactness.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Mathematics - Dynamical Systems,Mathematics - Information Theory,Nonlinear Sciences - Chaotic Dynamics,Statistics - Machine Learning},
  file = {/home/elessar/Zotero/storage/NYWETXQ8/Travers and Crutchfield - 2011 - Exact Synchronization for Finite-State Sources.pdf}
}

@misc{traversInfiniteExcessEntropy2011,
  title = {Infinite {{Excess Entropy Processes}} with {{Countable-State Generators}}},
  author = {Travers, Nicholas F. and Crutchfield, James P.},
  year = {2011},
  month = nov,
  number = {arXiv:1111.3393},
  eprint = {1111.3393},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1111.3393},
  urldate = {2025-07-15},
  abstract = {We present two examples of finite-alphabet, infinite excess entropy processes generated by invariant hidden Markov models (HMMs) with countable state sets. The first, simpler example is not ergodic, but the second is. It appears these are the first constructions of processes of this type. Previous examples of infinite excess entropy processes over finite alphabets admit only invariant HMM presentations with uncountable state sets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Condensed Matter - Statistical Mechanics,Mathematics - Information Theory,Mathematics - Probability,Nonlinear Sciences - Chaotic Dynamics},
  file = {/home/elessar/Zotero/storage/K36FLNUX/Travers and Crutchfield - 2011 - Infinite Excess Entropy Processes with Countable-State Generators.pdf}
}

@article{triefenbachAcousticModelingHierarchical2013,
  title = {Acoustic {{Modeling With Hierarchical Reservoirs}}},
  author = {Triefenbach, Fabian and Jalalvand, Azarakhsh and Demuynck, Kris and Martens, Jean-Pierre},
  year = {2013},
  month = nov,
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {21},
  number = {11},
  pages = {2439--2450},
  issn = {1558-7916, 1558-7924},
  doi = {10.1109/TASL.2013.2280209},
  urldate = {2025-03-07},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  annotation = {GSCC: 0000081},
  file = {/home/elessar/Zotero/storage/6IJPVLYG/Triefenbach et al. - 2013 - Acoustic Modeling With Hierarchical Reservoirs.pdf}
}

@article{triefenbachLargeVocabularyContinuous2014,
  title = {Large {{Vocabulary Continuous Speech Recognition With Reservoir-Based Acoustic Models}}},
  author = {Triefenbach, Fabian and Demuynck, Kris and Martens, Jean-Pierre},
  year = {2014},
  month = mar,
  journal = {IEEE Signal Processing Letters},
  volume = {21},
  number = {3},
  pages = {311--315},
  issn = {1070-9908, 1558-2361},
  doi = {10.1109/LSP.2014.2302080},
  urldate = {2025-03-07},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  file = {/home/elessar/Zotero/storage/FYLF4GP2/Triefenbach et al. - 2014 - Large Vocabulary Continuous Speech Recognition With Reservoir-Based Acoustic Models.pdf}
}

@inproceedings{triefenbachPhonemeRecognitionLarge2010,
  title = {Phoneme {{Recognition}} with {{Large Hierarchical Reservoirs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Triefenbach, Fabian and Jalalvand, Azarakhsh and Schrauwen, Benjamin and Martens, Jean-pierre},
  year = {2010},
  volume = {23},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-03-07},
  abstract = {Automatic speech recognition has gradually improved over the years, but the reliable recognition of unconstrained speech is still not within reach. In order to achieve a breakthrough, many research groups are now investigating new methodologies that have potential to outperform the Hidden Markov Model technology that is at the core of all present commercial systems. In this paper, it is shown that the recently introduced concept of Reservoir Computing might form the basis of such a methodology. In a limited amount of time, a reservoir system that can recognize the elementary sounds of continuous speech has been built. The system already achieves a state-of-the-art performance, and there is evidence that the margin for further improvements is still significant.},
  file = {/home/elessar/Zotero/storage/USLPLB5F/Triefenbach et al. - 2010 - Phoneme Recognition with Large Hierarchical Reservoirs.pdf}
}

@incollection{trouvainReservoirPyEfficientUserFriendly2020,
  title = {{{ReservoirPy}}: {{An Efficient}} and {{User-Friendly Library}} to {{Design Echo State Networks}}},
  shorttitle = {{{ReservoirPy}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} -- {{ICANN}} 2020},
  author = {Trouvain, Nathan and Pedrelli, Luca and Dinh, Thanh Trung and Hinaut, Xavier},
  editor = {Farka{\v s}, Igor and Masulli, Paolo and Wermter, Stefan},
  year = {2020},
  volume = {12397},
  pages = {494--505},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-61616-8_40},
  urldate = {2023-05-30},
  abstract = {We present a simple user-friendly library called ReservoirPy based on Python scientific modules. It provides a flexible interface to implement efficient Reservoir Computing (RC) architectures with a particular focus on Echo State Networks (ESN). Advanced features of ReservoirPy allow to improve up to 87.9\% of computation time efficiency on a simple laptop compared to basic Python implementation. Overall, we provide tutorials for hyperparameters tuning, offline and online training, fast spectral initialization, parallel and sparse matrix computation on various tasks (MackeyGlass and audio recognition tasks). In particular, we provide graphical tools to easily explore hyperparameters using random search with the help of the hyperopt library.},
  isbn = {978-3-030-61615-1 978-3-030-61616-8},
  langid = {english},
  annotation = {GSCC: 0000054},
  file = {/home/elessar/Zotero/storage/J37N3D6C/Trouvain et al. - 2020 - ReservoirPy An Efficient and User-Friendly Librar.pdf}
}

@article{tsuchiyamaEffectTemporalResolution2023,
  title = {Effect of Temporal Resolution on the Reproduction of Chaotic Dynamics via Reservoir Computing},
  author = {Tsuchiyama, Kohei and R{\"o}hm, Andr{\'e} and Mihana, Takatomo and Horisaki, Ryoichi and Naruse, Makoto},
  year = {2023},
  month = jun,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {33},
  number = {6},
  eprint = {2302.10761},
  primaryclass = {cs},
  pages = {063145},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0143846},
  urldate = {2025-09-10},
  abstract = {Reservoir computing is a machine learning paradigm that uses a structure called a reservoir, which has nonlinearities and short-term memory. In recent years, reservoir computing has expanded to new functions such as the autonomous generation of chaotic time series, as well as time series prediction and classification. Furthermore, novel possibilities have been demonstrated, such as inferring the existence of previously unseen attractors. Sampling, in contrast, has a strong influence on such functions. Sampling is indispensable in a physical reservoir computer that uses an existing physical system as a reservoir because the use of an external digital system for the data input is usually inevitable. This study analyzes the effect of sampling on the ability of reservoir computing to autonomously regenerate chaotic time series. We found, as expected, that excessively coarse sampling degrades the system performance, but also that excessively dense sampling is unsuitable. Based on quantitative indicators that capture the local and global characteristics of attractors, we identify a suitable window of the sampling frequency and discuss its underlying mechanisms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/elessar/Zotero/storage/BBKDPSZ5/Tsuchiyama et al. - 2023 - Effect of temporal resolution on the reproduction of chaotic dynamics via reservoir computing.pdf}
}

@article{tuncayPhysicsRandomnessRegularities2008,
  title = {The Physics of Randomness and Regularities for Languages in Terms of Random Matrices},
  author = {Tuncay, {\c C}},
  year = {2008},
  month = apr,
  journal = {EPL (Europhysics Letters)},
  volume = {82},
  number = {2},
  pages = {20004},
  publisher = {IOP Publishing},
  issn = {0295-5075, 1286-4854},
  doi = {10.1209/0295-5075/82/20004},
  urldate = {2025-07-25},
  abstract = {The physics of randomness and regularities for languages (mother tongues) and their lifetimes and family trees and for the second languages (bilinguals) are studied in terms of two processes: random multiplicative noise and fragmentation. We start with various random initial worlds and come out with regularities which are all similar to the present empirical data for the above-mentioned terms.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/RZHJJNKW/Tuncay - 2008 - The physics of randomness and regularities for languages in terms of random matrices.pdf}
}

@misc{UltimateGuideFineTuning,
  title = {The {{Ultimate Guide}} to {{Fine-Tuning LLMs}} from {{Basics}} to {{Breakthroughs}}: {{An Exhaustive Review}} of {{Technologies}}, {{Research}}, {{Best Practices}}, {{Applied Research Challenges}} and {{Opportunities}} ({{Version}} 1.0)},
  urldate = {2024-10-22},
  howpublished = {https://arxiv.org/html/2408.13296v2},
  file = {/home/elessar/Zotero/storage/H4DTMJA4/2408.html}
}

@article{vansiclenInformationEntropyComplex1997,
  title = {Information Entropy of Complex Structures},
  author = {Van Siclen, Clinton DeW.},
  year = {1997},
  month = nov,
  journal = {Physical Review E},
  volume = {56},
  number = {5},
  pages = {5211--5215},
  publisher = {American Physical Society (APS)},
  issn = {1063-651X, 1095-3787},
  doi = {10.1103/physreve.56.5211},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/75IM4MM2/Van Siclen - 1997 - Information entropy of complex structures.pdf}
}

@article{varkonyiMonomonostaticBodiesAnswer2006,
  title = {Mono-Monostatic Bodies: {{The}} Answer to {{Arnold}}'s Question},
  shorttitle = {Mono-Monostatic Bodies},
  author = {V{\'a}rkonyi, P. L. and Domokos, G.},
  year = {2006},
  month = sep,
  journal = {The Mathematical Intelligencer},
  volume = {28},
  number = {4},
  pages = {34--38},
  issn = {0343-6993},
  doi = {10.1007/BF02984701},
  urldate = {2024-01-10},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  annotation = {GSCC: 0000040},
  file = {/home/elessar/Zotero/storage/2FKS4DDW/Várkonyi and Domokos - 2006 - Mono-monostatic bodies The answer to Arnold’s que.pdf}
}

@article{varnInferringPlanarDisorder2013,
  title = {Inferring Planar Disorder in Close-Packed Structures{\emph{via}}∊-Machine~Spectral Reconstruction Theory: Examples from Simulated Diffraction Patterns},
  shorttitle = {Inferring Planar Disorder in Close-Packed Structures{\emph{via}}∊-Machine~Spectral Reconstruction Theory},
  author = {Varn, D. P. and Canright, G. S. and Crutchfield, J. P.},
  year = {2013},
  month = jul,
  journal = {Acta Crystallographica Section A Foundations of Crystallography},
  volume = {69},
  number = {4},
  pages = {413--426},
  publisher = {International Union of Crystallography (IUCr)},
  issn = {0108-7673, 1600-5724},
  doi = {10.1107/s0108767313008738},
  urldate = {2025-07-25},
  copyright = {http://journals.iucr.org/services/copyrightpolicy.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/R7MFGA94/Varn et al. - 2013 - Inferring planar disorder in close-packed structuresvia∊-machine spectral reconstruction theo.pdf}
}

@article{varnMachineSpectralReconstruction2013,
  title = {∊-{{Machine}} Spectral Reconstruction Theory: A Direct Method for Inferring Planar Disorder and Structure from {{X-ray}} Diffraction Studies},
  shorttitle = {∊-{{Machine}} Spectral Reconstruction Theory},
  author = {Varn, D. P. and Canright, G. S. and Crutchfield, J. P.},
  year = {2013},
  month = mar,
  journal = {Acta Crystallographica Section A Foundations of Crystallography},
  volume = {69},
  number = {2},
  pages = {197--206},
  publisher = {International Union of Crystallography (IUCr)},
  issn = {0108-7673, 1600-5724},
  doi = {10.1107/s0108767312046582},
  urldate = {2025-07-25},
  copyright = {http://journals.iucr.org/services/copyrightpolicy.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/RPRJ949G/Varn et al. - 2013 - ∊-Machine spectral reconstruction theory a direct method for inferring planar disorder and structur.pdf;/home/elessar/Zotero/storage/VC3Q2EX6/Varn et al. - 2013 - ∊-Machine spectral reconstruction theory a direct method for inferring planar disorder and structur.pdf}
}

@article{vedralRoleRelativeEntropy2002,
  title = {The Role of Relative Entropy in Quantum Information Theory},
  author = {Vedral, V.},
  year = {2002},
  month = mar,
  journal = {Reviews of Modern Physics},
  volume = {74},
  number = {1},
  pages = {197--234},
  publisher = {American Physical Society (APS)},
  issn = {0034-6861, 1539-0756},
  doi = {10.1103/revmodphys.74.197},
  urldate = {2025-07-15},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/NGIM68EA/Vedral - 2002 - The role of relative entropy in quantum information theory.pdf}
}

@article{vereshchaginIndependentMinimumLength,
  title = {Independent Minimum Length Programs to Translate between given Strings},
  author = {Vereshchagin, Nikolai K and Vyugin, Michael V},
  abstract = {A string p is calleda program to compute y given x if U (p; x)=y, where U denotes universal programming language. Kolmogorov complexity K(y{\textbar}x) of y relative to x is de.ned as minimum length of a program to compute y given x. Let K(x) denote K(x{\textbar}empty string) (Kolmogorov complexity of x) andlet I (x : y) = K(x) + K(y) - K({\textlangle}x; y{\textrangle}) (the amount of mutual information in x; y). In the present paper, we answer in the negative the following question posedin Bennett et al., IEEE Trans. Inform. Theory 44 (4) (1998) 1407--1423. Is it true that for any strings x; y there are independent minimum length programs p; q to translate between x; y, that is, is it true that for any x; y there are p; q such that U (p; x) = y, U (q; y) = x, the length of p is K(y{\textbar}x), the length of q is K(x{\textbar}y), and I (p : q) = 0 (where the last three equalities hold up to an additive O(log(K(x{\textbar}y) + K(y{\textbar}x))) term)?. c{\copyright} 2002 Elsevier Science B.V. All rights reserved.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/G7A32TYS/Vereshchagin and Vyugin - Independent minimum length programs to translate between given strings.pdf}
}

@article{vereshchaginKolmogorovsStructureFunctions2004,
  title = {Kolmogorov's {{Structure Functions}} and {{Model Selection}}},
  author = {Vereshchagin, N.K. and Vitanyi, P.M.B.},
  year = {2004},
  month = dec,
  journal = {IEEE Transactions on Information Theory},
  volume = {50},
  number = {12},
  pages = {3265--3290},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9448},
  doi = {10.1109/tit.2004.838346},
  urldate = {2025-07-25},
  abstract = {In 1974, Kolmogorov proposed a nonprobabilistic approach to statistics and model selection. Let data be finite binary strings and models be finite sets of binary strings. Consider model classes consisting of models of given maximal (Kolmogorov) complexity. The ``structure function'' of the given data expresses the relation between the complexity level constraint on a model class and the least log-cardinality of a model in the class containing the data. We show that the structure function determines all stochastic properties of the data: for every constrained model class it determines the individual best fitting model in the class irrespective of whether the ``true'' model is in the model class considered or not. In this setting, this happens with certainty, rather than with high probability as is in the classical case. We precisely quantify the goodness-of-fit of an individual model with respect to individual data. We show that---within the obvious constraints---every graph is realized by the structure function of some data. We determine the (un)computability properties of the various functions contemplated and of the ``algorithmic minimal sufficient statistic.'' Index Terms--- Computability, constrained best fit model selection, constrained maximum likelihood (ML), constrained minimum description length (MDL), function prediction, Kolmogorov complexity, Kolmogorov structure function, lossy compression, minimal sufficient statistic, nonprobabilistic statistics, sufficient statistic.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/FSHU9R3A/Vereshchagin and Vitanyi - 2004 - Kolmogorov's Structure Functions and Model Selection.pdf}
}

@article{verstraetenExperimentalUnificationReservoir2007,
  title = {An Experimental Unification of Reservoir Computing Methods},
  author = {Verstraeten, D. and Schrauwen, B. and D'Haene, M. and Stroobandt, D.},
  year = {2007},
  month = apr,
  journal = {Neural Networks},
  volume = {20},
  number = {3},
  pages = {391--403},
  issn = {08936080},
  doi = {10.1016/j.neunet.2007.04.003},
  urldate = {2023-05-30},
  abstract = {Three different uses of a recurrent neural network (RNN) as a reservoir that is not trained but instead read out by a simple external classification layer have been described in the literature: Liquid State Machines (LSMs), Echo State Networks (ESNs) and the Backpropagation Decorrelation (BPDC) learning rule. Individual descriptions of these techniques exist, but a overview is still lacking. Here, we present a series of experimental results that compares all three implementations, and draw conclusions about the relation between a broad range of reservoir parameters and network dynamics, memory, node complexity and performance on a variety of benchmark tests with different characteristics. Next, we introduce a new measure for the reservoir dynamics based on Lyapunov exponents. Unlike previous measures in the literature, this measure is dependent on the dynamics of the reservoir in response to the inputs, and in the cases we tried, it indicates an optimal value for the global scaling of the weight matrix, irrespective of the standard measures. We also describe the Reservoir Computing Toolbox that was used for these experiments, which implements all the types of Reservoir Computing and allows the easy simulation of a wide range of reservoir topologies for a number of benchmarks.},
  langid = {english},
  annotation = {GSCC: 0001291},
  file = {/home/elessar/Zotero/storage/R8CS6EEX/Verstraeten et al. - 2007 - An experimental unification of reservoir computing.pdf}
}

@article{viehwegDeterministicReservoirComputing2025,
  title = {Deterministic Reservoir Computing for Chaotic Time Series Prediction},
  author = {Viehweg, Johannes and Poll, Constanze and M{\"a}der, Patrick},
  year = {2025},
  month = may,
  journal = {Scientific Reports},
  volume = {15},
  number = {1},
  pages = {17695},
  issn = {2045-2322},
  doi = {10.1038/s41598-025-98172-z},
  urldate = {2025-08-12},
  abstract = {Abstract                            Reservoir Computing was shown in recent years to be useful as efficient to learn networks in the field of time series tasks. Their randomized initialization, a computational benefit, results in drawbacks in theoretical analysis of large random graphs, because of which deterministic variations are still an open field of research. Building upon Next Generation Reservoir Computing and the Temporal Convolution Derived Reservoir Computing, we propose a deterministic alternative to the higher-dimensional mapping therein, TCRC-LM and TCRC-CM, utilizing the parameterized but deterministic Logistic mapping and Chebyshev maps. To further enhance the predictive capabilities in the task of time series forecasting, we propose the novel utilization of the Lobachevsky function as non-linear activation function. As a result, we observe a new, fully deterministic network being able to outperform TCRCs and classical Reservoir Computing in the form of the prominent Echo State Networks by up to                                                   \$\$99.99{\textbackslash}\%\$\$                                                               99.99                       \%                                                                                       for the non-chaotic time series and                                                   \$\$87.13{\textbackslash}\%\$\$                                                               87.13                       \%                                                                                       for the chaotic ones.},
  langid = {english}
}

@article{viehwegParameterizingEchoState2023,
  title = {Parameterizing Echo State Networks for Multi-Step Time Series Prediction},
  author = {Viehweg, Johannes and Worthmann, Karl and M{\"a}der, Patrick},
  year = {2023},
  month = feb,
  journal = {Neurocomputing},
  volume = {522},
  pages = {214--228},
  issn = {09252312},
  doi = {10.1016/j.neucom.2022.11.044},
  urldate = {2023-05-30},
  abstract = {Prediction of multi-dimensional time-series data, which may represent such diverse phenomena as climate changes or financial markets, remains a challenging task in view of inherent nonlinearities and non-periodic behavior. In contrast to other recurrent neural networks, echo state networks (ESNs) are attractive for (online) learning due to lower requirements w.r.t. training data and computational power. However, the randomly-generated reservoir renders the choice of suitable hyper-parameters as an open research topic. We systematically derive and exemplarily demonstrate design guidelines for the hyperparameter optimization of ESNs. For the evaluation, we focus on the prediction of chaotic time series, an especially challenging problem in machine learning. Our findings demonstrate the power of a hyper-parameter-tuned ESN when auto-regressively predicting time series over several hundred steps. We found that ESNs' performance improved by 85:1\% {\`A} 99:8\% over an already wisely chosen default parameter initialization. In addition, the fluctuation range is considerably reduced such that significantly worse performance becomes very unlikely across random reservoir seeds. Moreover, we report individual findings per hyper-parameter partly contradicting common knowledge to further, help researchers when training new models.},
  langid = {english},
  keywords = {auto-regressive prediction,chaotic time series,design guidelines,dynamical system,echo state network,hyper-parameter,reservoir computing},
  annotation = {GSCC: 0000017},
  file = {/home/elessar/Zotero/storage/TFVQX8YT/Viehweg et al. - 2023 - Parameterizing echo state networks for multi-step .pdf;/home/elessar/Zotero/storage/WK9MNZ9S/S0925231222014291.html}
}

@article{visweswariahUniversalCodingNonstationary2000,
  title = {Universal Coding of Nonstationary Sources},
  author = {Visweswariah, K. and Kulkarni, S.R. and Verdu, S.},
  year = {2000},
  month = jul,
  journal = {IEEE Transactions on Information Theory},
  volume = {46},
  number = {4},
  pages = {1633--1637},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9448},
  doi = {10.1109/18.850707},
  urldate = {2025-07-15},
  abstract = {In this correspondence we investigate the performance of the Lempel--Ziv incremental parsing scheme on nonstationary sources. We show that it achieves the best rate achievable by a finite-state block coder for the nonstationary source. We also show a similar result for a lossy coding scheme given by Yang and Kieffer which uses a Lempel--Ziv scheme to perform lossy coding.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/SHL4QAM7/Visweswariah et al. - 2000 - Universal coding of nonstationary sources.pdf}
}

@misc{vitanyiEmpiricalEntropy2011,
  title = {On {{Empirical Entropy}}},
  author = {Vit{\'a}nyi, Paul M. B.},
  year = {2011},
  month = mar,
  number = {arXiv:1103.5985},
  eprint = {1103.5985},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1103.5985},
  urldate = {2025-07-25},
  abstract = {We propose a compression-based version of the empirical entropy of a finite string over a finite alphabet. Whereas previously one considers the naked entropy of (possibly higher order) Markov processes, we consider the sum of the description of the random variable involved plus the entropy it induces. We assume only that the distribution involved is computable. To test the new notion we compare the Normalized Information Distance (the similarity metric) with a related measure based on Mutual Information in Shannon's framework. This way the similarities and differences of the last two concepts are exposed.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Information Theory},
  file = {/home/elessar/Zotero/storage/V4ADBTYF/Vitányi - 2011 - On Empirical Entropy.pdf}
}

@article{vlachasBackpropagationAlgorithmsReservoir2020,
  title = {Backpropagation Algorithms and {{Reservoir Computing}} in {{Recurrent Neural Networks}} for the Forecasting of Complex Spatiotemporal Dynamics},
  author = {Vlachas, P.R. and Pathak, J. and Hunt, B.R. and Sapsis, T.P. and Girvan, M. and Ott, E. and Koumoutsakos, P.},
  year = {2020},
  month = jun,
  journal = {Neural Networks},
  volume = {126},
  pages = {191--217},
  issn = {08936080},
  doi = {10.1016/j.neunet.2020.02.016},
  urldate = {2024-01-10},
  abstract = {We examine the efficiency of Recurrent Neural Networks in forecasting the spatiotemporal dynamics of high dimensional and reduced order complex systems using Reservoir Computing (RC) and Backpropagation through time (BPTT) for gated network architectures. We highlight advantages and limitations of each method and discuss their implementation for parallel computing architectures. We quantify the relative prediction accuracy of these algorithms for the longterm forecasting of chaotic systems using as benchmarks the Lorenz-96 and the KuramotoSivashinsky (KS) equations. We find that, when the full state dynamics are available for training, RC outperforms BPTT approaches in terms of predictive performance and in capturing of the long-term statistics, while at the same time requiring much less training time. However, in the case of reduced order data, large scale RC models can be unstable and more likely than the BPTT algorithms to diverge. In contrast, RNNs trained via BPTT show superior forecasting abilities and capture well the dynamics of reduced order systems. Furthermore, the present study quantifies for the first time the Lyapunov Spectrum of the KS equation with BPTT, achieving similar accuracy as RC. This study establishes that RNNs are a potent computational framework for the learning and forecasting of complex spatiotemporal systems.},
  langid = {english},
  annotation = {GSCC: 0000389},
  file = {/home/elessar/Zotero/storage/GEBCI4JS/Vlachas et al. - 2020 - Backpropagation algorithms and Reservoir Computing in Recurrent Neural Networks for the forecasting.pdf;/home/elessar/Zotero/storage/GHNHEAUM/Vlachas et al. - 2020 - Backpropagation algorithms and Reservoir Computing.pdf;/home/elessar/Zotero/storage/HKD549V9/Vlachas et al_2020_Backpropagation algorithms and Reservoir Computing in Recurrent Neural Networks.pdf}
}

@article{voorheesDivisionAlgorithmCellular,
  title = {Division {{Algorithm}} for {{Cellular Automata Rules}}},
  author = {Voorhees, Burton},
  abstract = {Given two cellu lar automata rules represented as operato rs Q and X, together with certain nat ural restrictions on their neighb orhood st ruct ures , an algor ithm is provided wh ich yie lds two other r ules, A and R , such t hat Q = AX + R. A generalized arithmetic of residues follows from this.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/LYDUDZHC/Voorhees - Division Algorithm for Cellular Automata Rules.pdf}
}

@article{vyuginComplexityEasyPredictable2002,
  title = {On {{Complexity}} of {{Easy Predictable Sequences}}},
  author = {Vyugin, Michael V and V'yugin, Vladimir V},
  year = {2002},
  month = oct,
  journal = {Information and Computation},
  volume = {178},
  number = {1},
  pages = {241--252},
  publisher = {Elsevier BV},
  issn = {0890-5401},
  doi = {10.1006/inco.2002.3164},
  urldate = {2025-07-25},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/92J6ZJJE/Vyugin and V'yugin - 2002 - On Complexity of Easy Predictable Sequences.pdf}
}

@article{vyuginErgodicTheoremsIndividual1998,
  title = {Ergodic Theorems for Individual Random Sequences},
  author = {V'yugin, V.V.},
  year = {1998},
  month = nov,
  journal = {Theoretical Computer Science},
  volume = {207},
  number = {2},
  pages = {343--361},
  publisher = {Elsevier BV},
  issn = {0304-3975},
  doi = {10.1016/s0304-3975(98)00072-3},
  urldate = {2025-07-25},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/JG5BQ6KE/V'yugin - 1998 - Ergodic theorems for individual random sequences.pdf}
}

@article{vyuginInformationDistanceConditional2002,
  title = {Information Distance and Conditional Complexities},
  author = {Vyugin, Mikhail V.},
  year = {2002},
  month = jan,
  journal = {Theoretical Computer Science},
  volume = {271},
  number = {1-2},
  pages = {145--150},
  publisher = {Elsevier BV},
  issn = {0304-3975},
  doi = {10.1016/s0304-3975(01)00037-8},
  urldate = {2025-07-25},
  abstract = {C.H. Bennett, P. G-acs, M. Li, P.M.B. Vit-anyi, and W.H. Zurek have de1ned information distance between two strings x, y as d(x; y) = max\{K(x{\textbar}y); K(y{\textbar}x)\}; where K(x{\textbar}y) is conditional Kolmogorov complexity. It is easy to see that for any string x and any integer n there is a string y such that d(x; y) = n + O(1). In this paper we prove the following (stronger) result: for any n and for any string x such that K(x) {$\geq$} 2n + O(1) there exists a string y such that both K(x{\textbar}y) and K(y{\textbar}x) are equal to n + O(1). c{\copyright} 2002 Elsevier Science B.V. All rights reserved.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/C59M8G65/Vyugin - 2002 - Information distance and conditional complexities.pdf}
}

@article{vyuginMostSequencesAre2001,
  title = {Most {{Sequences Are Stochastic}}},
  author = {V'yugin, V.V},
  year = {2001},
  month = sep,
  journal = {Information and Computation},
  volume = {169},
  number = {2},
  pages = {252--263},
  publisher = {Elsevier BV},
  issn = {0890-5401},
  doi = {10.1006/inco.2001.3041},
  urldate = {2025-07-25},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/ZZ8WD2S5/V'yugin - 2001 - Most Sequences Are Stochastic.pdf}
}

@article{walleshauserPredictingSeaSurface2022,
  title = {Predicting Sea Surface Temperatures with Coupled Reservoir Computers},
  author = {Walleshauser, Benjamin and Bollt, Erik},
  year = {2022},
  month = jul,
  journal = {Nonlinear Processes in Geophysics},
  volume = {29},
  number = {3},
  pages = {255--264},
  issn = {1607-7946},
  doi = {10.5194/npg-29-255-2022},
  urldate = {2025-08-12},
  abstract = {Abstract. Sea surface temperature (SST) is a key factor in understanding the greater climate of the Earth, and it is also an important variable when making weather predictions. Methods of machine learning have become ever more present and important in data-driven science and engineering, including in important areas for Earth science. Here, we propose an efficient framework that allows us to make global SST forecasts using a coupled reservoir computer method that we have specialized to this domain, allowing for template regions that accommodate irregular coastlines. Reservoir computing is an especially good method for forecasting spatiotemporally complex dynamical systems, as it is a machine learning method that, despite many randomly selected weights, is highly accurate and easy to train. Our approach provides the benefit of a simple and computationally efficient model that is able to predict SSTs across the entire Earth's oceans. The results are demonstrated to generally follow the actual dynamics of the system over a forecasting period of several weeks.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english}
}

@misc{wangComprehensiveSurveyContinual2024,
  title = {A {{Comprehensive Survey}} of {{Continual Learning}}: {{Theory}}, {{Method}} and {{Application}}},
  shorttitle = {A {{Comprehensive Survey}} of {{Continual Learning}}},
  author = {Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
  year = {2024},
  month = feb,
  number = {arXiv:2302.00487},
  eprint = {2302.00487},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.00487},
  urldate = {2025-03-20},
  abstract = {To cope with real-world dynamics, an intelligent system needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance degradation of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative methods address continual learning, and how they are adapted to particular challenges in realistic applications. Through an in-depth discussion of promising directions, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/elessar/Zotero/storage/EKPW5BJE/Wang et al. - 2024 - A Comprehensive Survey of Continual Learning Theory, Method and Application.pdf}
}

@article{wangConstructingHigherDimensionalDigital2021,
  title = {Constructing {{Higher-Dimensional Digital Chaotic Systems}} via {{Loop-State Contraction Algorithm}}},
  author = {Wang, Qianxue and Yu, Simin and Guyeux, Christophe and Wang, Wei},
  year = {2021},
  month = sep,
  journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
  volume = {68},
  number = {9},
  eprint = {2104.01743},
  primaryclass = {nlin},
  pages = {3794--3807},
  issn = {1549-8328, 1558-0806},
  doi = {10.1109/TCSI.2021.3091404.},
  urldate = {2024-12-16},
  abstract = {In recent years, the generation of rigorously provable chaos in finite precision digital domain has made a lot of progress in theory and practice, this article is a part of it. It aims to improve and expand the theoretical and application framework of higher-dimensional digital chaotic system (HDDCS). In this study, topological mixing for HDDCS is strictly proved theoretically at first. Topological mixing implies Devaney's definition of chaos in compact space, but not vise versa. Therefore, the proof of topological mixing improves and expands the theoretical framework of HDDCS. Then, a general design method for constructing HDDCS via loop-state contraction algorithm is given. The construction of the iterative function uncontrolled by random sequences (hereafter called iterative function) is the starting point of this research. On this basis, this article put forward a general design method to solve the problem of HDDCS construction, and a number of examples illustrate the effectiveness and feasibility of this method. The adjacency matrix corresponding to the designed HDDCS is used to construct the chaotic echo state network (ESN) for the Mackey-Glass time series prediction. The prediction accuracy is improved with the increase of the size of the reservoir and the dimension of HDDCS, indicating that higher-dimensional systems have better prediction performance than lower-dimensional systems.},
  archiveprefix = {arXiv},
  keywords = {Nonlinear Sciences - Chaotic Dynamics},
  annotation = {GSCC: 0000019},
  file = {/home/elessar/Zotero/storage/APZ4MDM9/Wang et al. - 2021 - Constructing Higher-Dimensional Digital Chaotic Systems via Loop-State Contraction Algorithm.pdf;/home/elessar/Zotero/storage/8IJRLG9T/2104.html}
}

@article{wangDeepEchoState2021,
  title = {Deep {{Echo State Network With Multiple Adaptive Reservoirs}} for {{Time Series Prediction}}},
  author = {Wang, Zhanshan and Yao, Xianshuang and Huang, Zhanjun and Liu, Lei},
  year = {2021},
  month = sep,
  journal = {IEEE Transactions on Cognitive and Developmental Systems},
  volume = {13},
  number = {3},
  pages = {693--704},
  issn = {2379-8920, 2379-8939},
  doi = {10.1109/TCDS.2021.3062177},
  urldate = {2025-03-11},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  file = {/home/elessar/Zotero/storage/7QMRGDKU/Wang et al. - 2021 - Deep Echo State Network With Multiple Adaptive Reservoirs for Time Series Prediction.pdf}
}

@article{wangEchoStateNetwork2019,
  title = {Echo State Network Based Ensemble Approach for Wind Power Forecasting},
  author = {Wang, Huaizhi and Lei, Zhenxing and Liu, Yang and Peng, Jianchun and Liu, Jing},
  year = {2019},
  month = dec,
  journal = {Energy Conversion and Management},
  volume = {201},
  pages = {112188},
  issn = {0196-8904},
  doi = {10.1016/j.enconman.2019.112188},
  urldate = {2024-11-26},
  abstract = {Accurate and reliable wind power forecasting is of great significance for the economic and safe operation of electric power and energy systems. However, due to the negative factors such as inaccurate numerical weather prediction and frequent ramping events, previous wind power prediction methods are difficult to meet actual needs for practical applications. To this end, this paper originally proposes a new wind power prediction method with high accuracy based on echo state network. This method is a hybrid of wavelet transform, echo state network and ensemble technique. Wavelet transform is used to decompose raw wind power time series data into different frequencies with better outliers and behaviors. Then, we adopt the echo state network to automatically learn the nonlinear mapping relationship between input and output in each frequency. Later, ensemble technique is applied to deal with the model misspecification and data noise problems, which are common in wind power prediction, thereby reducing the uncertainty of wind power forecasting and improving prediction accuracy. Finally, we use wind power data from real wind farms in Belgium and China to verify the feasibility and effectiveness of the proposed method. The simulation results show that the proposed method is superior to other benchmark algorithms in prediction accuracy, which indicates that the method has high potentials for practical application in real electric power and energy systems.},
  keywords = {Echo state network,Ensemble,Short-term forecast,Wavelet transform,Wind power prediction},
  file = {/home/elessar/Zotero/storage/XS5H676U/Wang et al. - 2019 - Echo state network based ensemble approach for wind power forecasting.pdf;/home/elessar/Zotero/storage/Y7RBXYUQ/S019689041931194X.html}
}

@article{wangEchoStateNetwork2022,
  title = {Echo State Network with Logistic Mapping and Bias Dropout for Time Series Prediction},
  author = {Wang, Heshan and Liu, Yuxi and Lu, Peng and Luo, Yong and Wang, Dongshu and Xu, Xiangyang},
  year = {2022},
  month = jun,
  journal = {Neurocomputing},
  volume = {489},
  pages = {196--210},
  issn = {09252312},
  doi = {10.1016/j.neucom.2022.03.018},
  urldate = {2025-03-12},
  abstract = {An echo state network (ESN) is a special structure of a recurrent neural network in which the recurrent neurons are randomly connected. ESN models that have achieved high accuracy on time series prediction tasks can be utilized as time series prediction models in many fields. Nevertheless, in most ESN models, the input weights are irregularly generated and the reservoir layer units are generally redundant, which cannot guarantee that the ESN models will always be optimal for a given task. In this paper, a novel ESN model that combines logistic mapping (LM) and bias dropout (BD) algorithms is proposed to optimize the irregular input weight matrix and generate a superior and simpler reservoir. Initially, the initial input weight matrix of ESN is replaced by an LM input weight matrix that is generated by the recurrent LM algorithm. Meanwhile, the reservoir, which can convert the input space into a high-dimensional feature space, is formed by the input signals and the LM input weight matrix. Then, the units with low activation values that are determined by a reservoir dropout probability are discarded through the BD algorithm. The dropout probability is determined by the contributions of the reservoir units to the training performance. Three multivariable benchmark tasks and four univariate real-world time series tasks indicate that the proposed LM-BD-ESN model is effective in reducing the testing time, reservoir size, and model complexity while improving the performance of the traditional ESN.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/5ZNLJ73W/Wang et al. - 2022 - Echo state network with logistic mapping and bias dropout for time series prediction.pdf}
}

@article{wangEffectiveElectricityEnergy2018,
  title = {Effective Electricity Energy Consumption Forecasting Using Echo State Network Improved by Differential Evolution Algorithm},
  author = {Wang, Lin and Hu, Huanling and Ai, Xue-Yi and Liu, Hua},
  year = {2018},
  month = jun,
  journal = {Energy},
  volume = {153},
  pages = {801--815},
  issn = {03605442},
  doi = {10.1016/j.energy.2018.04.078},
  urldate = {2025-03-12},
  abstract = {Electricity energy consumption (EEC) has great effect on the government to make reasonable energy policy and has attracted great attentions of the power generation groups with the liberalization of competition in the electricity industry. In fact, the EEC is easily affected by many factors, including the climate factor and the gross domestic product. So, the precise forecasting of electricity energy consumption is very challenging. This study aims to propose an effective and stable model named ESN-DE using an improved echo state network for forecasting electricity energy consumption. Differential evolution algorithm is used to search optimal values of the three crucial parameters of echo state network. Two comparative examples and an extended example are used to validate the applicability and accuracy of the proposed ESN-DE. Errors of the comparative examples where mean absolute percentage errors of ESN-DE are 1.516\% and 0.570\% respectively indicate that the ESN-DE outperforms the traditional echo state network and the existing best model. Mean absolute percentage error of ESN-DE is 2.156\% for Zhengzhou City's electricity energy consumption forecasting. The proposed ESN-DE is a potential candidate for effective forecasting of electricity energy consumption because of its easy implementation and stability.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/3RNTWSMV/Wang et al. - 2018 - Effective electricity energy consumption forecasting using echo state network improved by differenti.pdf}
}

@article{wangNewMethodNonlinear2022,
  title = {A New Method of Nonlinear Causality Detection: {{Reservoir}} Computing {{Granger}} Causality},
  shorttitle = {A New Method of Nonlinear Causality Detection},
  author = {Wang, Mingzhao and Fu, Zuntao},
  year = {2022},
  month = jan,
  journal = {Chaos, Solitons \& Fractals},
  volume = {154},
  pages = {111675},
  issn = {09600779},
  doi = {10.1016/j.chaos.2021.111675},
  urldate = {2025-03-12},
  abstract = {Identifying causal and interaction relationships from observational time series is a key step toward understanding complex systems and is also a challenging problem. Many methods have been developed to detect and identify the possible causal link between two variables. However, most of them often suffer from the false detection problems, including false-positive (detected causal links do not exist) and false-negative (an existing causal link fails to be detected). Compared to false-negative problem, false-positive is a more serious problem found in nonlinear systems or processes for almost all causality detection methods. In this study, a new method combining the advantages of Reservoir Computing (RC, a machine learning method) and classical Granger causality detection was developed to fully solve false-positive problems or considerably lower the false-positive rate in detecting causal links of nonlinear systems. Three typical coupled systems with known causal links are applied to show the better performance of new Reservoir Computing Granger (RCG) method over traditional nonlinear Granger causality and its extension methods, which indicates that RCG can accurately detect causal relationships in complex systems and its great potential in exploring the causal and interaction relationships from observational time series.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/XATLTGBA/Wang and Fu - 2022 - A new method of nonlinear causality detection Reservoir computing Granger causality.pdf}
}

@article{wangOptimizingEchoState2017,
  title = {Optimizing the Echo State Network Based on Mutual Information for Modeling Fed-Batch Bioprocesses},
  author = {Wang, Heshan and Ni, Chunjuan and Yan, Xuefeng},
  year = {2017},
  month = feb,
  journal = {Neurocomputing},
  volume = {225},
  pages = {111--118},
  issn = {09252312},
  doi = {10.1016/j.neucom.2016.11.007},
  urldate = {2025-03-12},
  abstract = {Echo state networks (ESNs) have become one of the most effective dynamic neural networks because of its excellent fitting performance in real-valued time series modeling tasks and simple training processes. The original ESN concept used randomly fixed created reservoirs, and this concept is considered to be one of its main advantages. However, ESNs have been criticized for its randomly created connectivity and weight parameters. Determining the appropriate weight parameters for a given task is an important problem. An optimization method based on mutual information (MI) is proposed in this study to optimize the input scaling parameters and the structure of ESN to address the aforementioned problem and improve the performance of ESN. The MI optimization method mainly consists of two parts: First, the scaling parameters of multiple inputs are adjusted based on the MI between the network inputs and outputs. Second, some output weight connections are pruned for optimization based on the MI between reservoir states. Furthermore, three MI-ESN models are proposed for a fed-batch penicillin fermentation process. Our experimental outcomes reveal that the obtained MI-ESN models outperform the ESN models without optimization and other traditional neural networks.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/SP9EI4EU/Wang et al. - 2017 - Optimizing the echo state network based on mutual information for modeling fed-batch bioprocesses.pdf}
}

@article{wangOptimizingEchoState2019,
  title = {Optimizing Echo State Network with Backtracking Search Optimization Algorithm for Time Series Forecasting},
  author = {Wang, Zhigang and Zeng, Yu-Rong and Wang, Sirui and Wang, Lin},
  year = {2019},
  month = may,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {81},
  pages = {117--132},
  issn = {09521976},
  doi = {10.1016/j.engappai.2019.02.009},
  urldate = {2025-03-20},
  abstract = {The echo state network (ESN) is a state-of-the art reservoir computing approach, which is particularly effective for time series forecasting problems because it is coupled with a time parameter. However, the linear regression algorithm commonly used to compute the output weights of ESN could usually cause the trained network overfitted and thus obtain unsatisfactory results. To overcome the problem, we present four optimized ESNs that are based on the backtracking search optimization algorithm (BSA) or its variants to improve generalizability. Concretely, we utilize BSA and its variants to determine the most appropriate output weights of ESN given that the optimization problem is complex while BSA is a novel evolutionary algorithm that effectively unscrambles optimal solutions in complex spaces. The three BSA variants, namely, adaptive population selection scheme (APSS)--BSA, adaptive mutation factor strategy (AMFS)--BSA, and APSS\&AMFS--BSA, were designed to further improve the performance of BSA. Time series forecasting experiments were performed using two real-life time series. The experimental results of the optimized ESNs were compared with those of the basic ESN without optimization, and the two other comparison approaches, as well as the other existing approaches. Experimental results showed that (a) the results of the optimized ESNs are more accurate than that of basic ESN and (b) APSS\&AMFS--BSA--ESN nearly outperforms basic ESN, the three other optimized ESNs, the two comparison approaches, and other existing optimization approaches.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/T3IIUR3X/Wang et al. - 2019 - Optimizing echo state network with backtracking search optimization algorithm for time series foreca.pdf}
}

@article{wattsCollectiveDynamicsSmallworld1998,
  title = {Collective Dynamics of `Small-World' Networks},
  author = {Watts, Duncan J and Strogatz, Steven H},
  year = {1998},
  volume = {393},
  langid = {english},
  annotation = {GSCC: 0055250},
  file = {/home/elessar/Zotero/storage/P9WGUAHD/Watts and Strogatz - 1998 - Collective dynamics of ‘small-world’ networks.pdf}
}

@inbook{weaverScienceComplexity1991,
  title = {Science and {{Complexity}}},
  booktitle = {Facets of {{Systems Science}}},
  year = {1991},
  pages = {449--456},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4899-0718-9_30},
  urldate = {2025-07-15},
  collaborator = {Weaver, Warren},
  isbn = {978-1-4899-0720-2 978-1-4899-0718-9},
  langid = {english},
  file = {/home/elessar/Zotero/storage/KMLP59XW/1991 - Science and Complexity.pdf}
}

@article{weinbergerOptimalAsymptoticPerformance1992,
  title = {On the Optimal Asymptotic Performance of Universal Ordering and of Discrimination of Individual Sequences},
  author = {Weinberger, M.J. and Ziv, J. and Lempel, A.},
  year = {1992},
  month = mar,
  journal = {IEEE Transactions on Information Theory},
  volume = {38},
  number = {2},
  pages = {380--385},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/18.119694},
  urldate = {2025-07-25},
  abstract = {We consider the problem of ordering strings of a fixed length over a discrete alphabet according to decreasing probabilities of having been emitted by an unknown finite-state source. Data compression is applied to derive a universal algorithm that solves this problem with an optimal asymptotic performance. This result is employed in the solution of the following problem: Discriminate an individual sequence as emitted by an i.i.d. random source of equally likely symbols or as a signal corrupted by noise. Tight lower and upper bounds on the asymptotic relative performance of universal and nonuniversal, but finite-state discriminators are given.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/5BC5EJW2/Weinberger et al. - 1992 - On the optimal asymptotic performance of universal ordering and of discrimination of individual sequ.pdf}
}

@article{weissInformationContentProtein2000,
  title = {Information {{Content}} of {{Protein Sequences}}},
  author = {Weiss, Olaf and {Jim{\'e}nez-Monta{\~n}o}, Miguel A and Herzel, Hanspeter},
  year = {2000},
  month = oct,
  journal = {Journal of Theoretical Biology},
  volume = {206},
  number = {3},
  pages = {379--386},
  publisher = {Elsevier BV},
  issn = {0022-5193},
  doi = {10.1006/jtbi.2000.2138},
  urldate = {2025-07-25},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/GE3N65XA/Weiss et al. - 2000 - Information Content of Protein Sequences.pdf}
}

@article{weissSubshiftsFiniteType1973,
  title = {Subshifts of Finite Type and Sofic Systems},
  author = {Weiss, Benjamin},
  year = {1973},
  month = oct,
  journal = {Monatshefte f{\dbend}r Mathematik},
  volume = {77},
  number = {5},
  pages = {462--474},
  publisher = {{Springer Science and Business Media LLC}},
  issn = {0026-9255, 1436-5081},
  doi = {10.1007/bf01295322},
  urldate = {2025-07-15},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/home/elessar/Zotero/storage/LQMAGEDC/Weiss - 1973 - Subshifts of finite type and sofic systems.pdf}
}

@article{wengSynchronizationChaoticSystems2019,
  title = {Synchronization of Chaotic Systems and Their Machine-Learning Models},
  author = {Weng, Tongfeng and Yang, Huijie and Gu, Changgui and Zhang, Jie and Small, Michael},
  year = {2019},
  month = apr,
  journal = {Physical Review E},
  volume = {99},
  number = {4},
  pages = {042203},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.99.042203},
  urldate = {2024-01-10},
  langid = {english},
  annotation = {GSCC: 0000121},
  file = {/home/elessar/Zotero/storage/DZ6PARGR/Weng et al_2019_Synchronization of chaotic systems and their machine-learning models.pdf;/home/elessar/Zotero/storage/HN3E8QFB/Weng et al. - 2019 - Synchronization of chaotic systems and their machi.pdf;/home/elessar/Zotero/storage/QMXGA46L/Weng et al. - 2019 - Synchronization of chaotic systems and their machi.pdf}
}

@article{whalenIntroductionComputationalMechanics,
  title = {An {{Introduction}} to {{Computational Mechanics}}},
  author = {Whalen, Sean H},
  langid = {english},
  file = {/home/elessar/Zotero/storage/X5FZGAQT/Whalen - An Introduction to Computational Mechanics.pdf}
}

@article{whalenSecurityApplicationsEMachine,
  title = {Security {{Applications}} of the {$\varepsilon$}-{{Machine}}},
  author = {Whalen, Sean Harrison},
  langid = {english},
  file = {/home/elessar/Zotero/storage/2GQE397Q/Whalen - Security Applications of the ε-Machine.pdf}
}

@book{wigginsIntroductionAppliedNonlinear2003,
  title = {Introduction to Applied Nonlinear Dynamical Systems and Chaos},
  author = {Wiggins, Stephen},
  year = {2003},
  series = {Texts in Applied Mathematics},
  edition = {2nd ed},
  number = {2},
  publisher = {Springer},
  address = {New York},
  isbn = {978-0-387-00177-7},
  lccn = {QA614.8 .W544 2003},
  keywords = {Chaotic behavior in systems,Differentiable dynamical systems,Nonlinear theories},
  file = {/home/elessar/Zotero/storage/3X5XU8NR/Wiggins - 2003 - Introduction to applied nonlinear dynamical systems and chaos.pdf}
}

@article{wiknerCombiningMachineLearning2020,
  title = {Combining Machine Learning with Knowledge-Based Modeling for Scalable Forecasting and Subgrid-Scale Closure of Large, Complex, Spatiotemporal Systems},
  author = {Wikner, Alexander and Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Arcomano, Troy and Szunyogh, Istvan and Pomerance, Andrew and Ott, Edward},
  year = {2020},
  month = may,
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  volume = {30},
  number = {5},
  pages = {053111},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/5.0005541},
  urldate = {2024-01-10},
  abstract = {We consider the commonly encountered situation (e.g., in weather forecast) where the goal is to predict the time evolution of a large, spatiotemporally chaotic dynamical system when we have access to both time series data of previous system states and an imperfect model of the full system dynamics. Specifically, we attempt to utilize machine learning as the essential tool for integrating the use of past data into predictions. In order to facilitate scalability to the common scenario of interest where the spatiotemporally chaotic system is very large and complex, we propose combining two approaches: (i) a parallel machine learning prediction scheme and (ii) a hybrid technique for a composite prediction system composed of a knowledge-based component and a machine learning-based component. We demonstrate that not only can this method combining (i) and (ii) be scaled to give excellent performance for very large systems but also that the length of time series data needed to train our multiple, parallel machine learning components is dramatically less than that necessary without parallelization. Furthermore, considering cases where computational realization of the knowledge-based component does not resolve subgrid-scale processes, our scheme is able to use training data to incorporate the effect of the unresolved short-scale dynamics upon the resolved longer-scale dynamics (subgrid-scale closure).},
  langid = {english},
  annotation = {GSCC: 0000080},
  file = {/home/elessar/Zotero/storage/PN7PLW9X/Wikner et al. - 2020 - Combining machine learning with knowledge-based mo.pdf}
}

@misc{wiknerStabilizingMachineLearning2022,
  title = {Stabilizing {{Machine Learning Prediction}} of {{Dynamics}}: {{Noise}} and {{Noise-inspired Regularization}}},
  shorttitle = {Stabilizing {{Machine Learning Prediction}} of {{Dynamics}}},
  author = {Wikner, Alexander and Harvey, Joseph and Girvan, Michelle and Hunt, Brian R. and Pomerance, Andrew and Antonsen, Thomas and Ott, Edward},
  year = {2022},
  month = dec,
  number = {arXiv:2211.05262},
  eprint = {2211.05262},
  primaryclass = {nlin},
  publisher = {arXiv},
  urldate = {2024-01-10},
  abstract = {Recent work has shown that machine learning (ML) models can be trained to accurately forecast the dynamics of unknown chaotic dynamical systems. Short-term predictions of the state evolution and long-term predictions of the statistical patterns of the dynamics (``climate'') can be produced by employing a feedback loop, whereby the model is trained to predict forward one time step, then the model output is used as input for multiple time steps. In the absence of mitigating techniques, however, this technique can result in artificially rapid error growth. In this article, we systematically examine the technique of adding noise to the ML model input during training to promote stability and improve prediction accuracy. Furthermore, we introduce Linearized Multi-Noise Training (LMNT), a regularization technique that deterministically approximates the effect of many small, independent noise realizations added to the model input during training. Our case study uses reservoir computing, a machine-learning method using recurrent neural networks, to predict the spatiotemporal chaotic Kuramoto-Sivashinsky equation. We find that reservoir computers trained with noise or with LMNT produce climate predictions that appear to be indefinitely stable and have a climate very similar to the true system, while reservoir computers trained without regularization are unstable. Compared with other regularization techniques that yield stability in some cases, we find that both short-term and climate predictions from reservoir computers trained with noise or with LMNT are substantially more accurate. Finally, we show that the deterministic aspect of our LMNT regularization facilitates fast hyperparameter tuning when compared to training with noise.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Nonlinear Sciences - Chaotic Dynamics},
  annotation = {GSCC: 0000002},
  file = {/home/elessar/Zotero/storage/APJW4LIN/Wikner et al. - 2022 - Stabilizing Machine Learning Prediction of Dynamic.pdf}
}

@article{wiknerStabilizingMachineLearning2024,
  title = {Stabilizing Machine Learning Prediction of Dynamics: {{Novel}} Noise-Inspired Regularization Tested with Reservoir Computing},
  shorttitle = {Stabilizing Machine Learning Prediction of Dynamics},
  author = {Wikner, Alexander and Harvey, Joseph and Girvan, Michelle and Hunt, Brian R. and Pomerance, Andrew and Antonsen, Thomas and Ott, Edward},
  year = {2024},
  month = feb,
  journal = {Neural Networks},
  volume = {170},
  pages = {94--110},
  issn = {08936080},
  doi = {10.1016/j.neunet.2023.10.054},
  urldate = {2024-01-10},
  abstract = {Recent work has shown that machine learning (ML) models can skillfully forecast the dynamics of unknown chaotic systems. Short-term predictions of the state evolution and long-term predictions of the statistical patterns of the dynamics (``climate'') can be produced by employing a feedback loop, whereby the model is trained to predict forward only one time step, then the model output is used as input for multiple time steps. In the absence of mitigating techniques, however, this feedback can result in artificially rapid error growth (``instability''). One established mitigating technique is to add noise to the ML model training input. Based on this technique, we formulate a new penalty term in the loss function for ML models with memory of past inputs that deterministically approximates the effect of many small, independent noise realizations added to the model input during training. We refer to this penalty and the resulting regularization as Linearized Multi-Noise Training (LMNT). We systematically examine the effect of LMNT, input noise, and other established regularization techniques in a case study using reservoir computing, a machine learning method using recurrent neural networks, to predict the spatiotemporal chaotic Kuramoto--Sivashinsky equation. We find that reservoir computers trained with noise or with LMNT produce climate predictions that appear to be indefinitely stable and have a climate very similar to the true system, while the short-term forecasts are substantially more accurate than those trained with other regularization techniques. Finally, we show the deterministic aspect of our LMNT regularization facilitates fast reservoir computer regularization hyperparameter tuning.},
  langid = {english},
  annotation = {GSCC: 0000010},
  file = {/home/elessar/Zotero/storage/CDWR799J/Wikner et al. - 2024 - Stabilizing machine learning prediction of dynamic.pdf}
}

@article{wilmsMutualInformationClassical2011,
  title = {Mutual Information in Classical Spin Models},
  author = {Wilms, Johannes and Troyer, Matthias and Verstraete, Frank},
  year = {2011},
  month = oct,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2011},
  number = {10},
  eprint = {1011.4421},
  primaryclass = {cond-mat},
  pages = {P10011},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/2011/10/P10011},
  urldate = {2025-07-25},
  abstract = {The total many-body correlations present in finite temperature classical spin systems are studied using the concept of mutual information. As opposed to zero-temperature quantum phase transitions, the total correlations are not maximal at the phase transition, but reach a maximum in the high temperature paramagnetic phase. The Shannon and Renyi mutual information in both Ising and Potts models in 2 dimensions are calculated numerically by combining matrix product states algorithms and Monte Carlo sampling techniques.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Condensed Matter - Statistical Mechanics,Quantum Physics},
  file = {/home/elessar/Zotero/storage/DEPBCKCU/Wilms et al. - 2011 - Mutual information in classical spin models.pdf}
}

@article{wolpertComputationalCapabilitiesPhysical2001,
  title = {Computational Capabilities of Physical Systems},
  author = {Wolpert, David H.},
  year = {2001},
  month = dec,
  journal = {Physical Review E},
  volume = {65},
  number = {1},
  publisher = {American Physical Society (APS)},
  issn = {1063-651X, 1095-3787},
  doi = {10.1103/physreve.65.016128},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/HWKDPDA2/Wolpert - 2001 - Computational capabilities of physical systems.PDF}
}

@incollection{wolpertSupervisedLearningNoFreeLunch2002,
  title = {The {{Supervised Learning No-Free-Lunch Theorems}}},
  booktitle = {Soft {{Computing}} and {{Industry}}},
  author = {Wolpert, David H.},
  editor = {Roy, Rajkumar and K{\"o}ppen, Mario and Ovaska, Seppo and Furuhashi, Takeshi and Hoffmann, Frank},
  year = {2002},
  pages = {25--42},
  publisher = {Springer London},
  address = {London},
  doi = {10.1007/978-1-4471-0123-9_3},
  urldate = {2023-05-30},
  abstract = {This paper reviews the supervised learning versions of the no-free-lunch theorems in a simpli ed form. It also discusses the signi cance of those theorems, and their relation to other aspects of supervised learning.},
  isbn = {978-1-4471-1101-6 978-1-4471-0123-9},
  langid = {english},
  annotation = {GSCC: 0000699},
  file = {/home/elessar/Zotero/storage/T9VXIJ78/Wolpert - 2002 - The Supervised Learning No-Free-Lunch Theorems.pdf}
}

@misc{wuReasoningRecitingExploring2024,
  title = {Reasoning or {{Reciting}}? {{Exploring}} the {{Capabilities}} and {{Limitations}} of {{Language Models Through Counterfactual Tasks}}},
  shorttitle = {Reasoning or {{Reciting}}?},
  author = {Wu, Zhaofeng and Qiu, Linlu and Ross, Alexis and Aky{\"u}rek, Ekin and Chen, Boyuan and Wang, Bailin and Kim, Najoung and Andreas, Jacob and Kim, Yoon},
  year = {2024},
  month = mar,
  number = {arXiv:2307.02477},
  eprint = {2307.02477},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.02477},
  urldate = {2025-03-20},
  abstract = {The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on "counterfactual" task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/elessar/Zotero/storage/LDGSVB9U/Wu et al. - 2024 - Reasoning or Reciting Exploring the Capabilities and Limitations of Language Models Through Counter.pdf}
}

@article{wuSynchronizationNonsmoothChaotic2024,
  title = {Synchronization of Non-Smooth Chaotic Systems via an Improved Reservoir Computing},
  author = {Wu, Guyue and Tang, Longkun and Liang, Jianli},
  year = {2024},
  month = jan,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {229},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-50690-4},
  urldate = {2025-07-28},
  abstract = {Abstract             The reservoir computing (RC) is increasingly used to learn the synchronization behavior of chaotic systems as well as the dynamical behavior of complex systems, but it is scarcely applied in studying synchronization of non-smooth chaotic systems likely due to its complexity leading to the unimpressive effect. Here proposes a simulated annealing-based differential evolution (SADE) algorithm for the optimal parameter selection in the reservoir, and constructs an improved RC model for synchronization, which can work well not only for non-smooth chaotic systems but for smooth ones. Extensive simulations show that the trained RC model with optimal parameters has far longer prediction time than those with empirical and random parameters. More importantly, the well-trained RC system can be well synchronized to its original chaotic system as well as its replicate RC system via one shared signal, whereas the traditional RC system with empirical or random parameters fails for some chaotic systems, particularly for some non-smooth chaotic systems.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/EQSQZ99V/Wu et al. - 2024 - Synchronization of non-smooth chaotic systems via an improved reservoir computing.pdf}
}

@article{wynerAsymptoticPropertiesEntropy1989,
  title = {Some Asymptotic Properties of the Entropy of a Stationary Ergodic Data Source with Applications to Data Compression},
  author = {Wyner, A.D. and Ziv, J.},
  year = {1989},
  journal = {IEEE Transactions on Information Theory},
  volume = {35},
  number = {6},
  pages = {1250--1258},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9448},
  doi = {10.1109/18.45281},
  urldate = {2025-07-25},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/E554JX7X/Wyner and Ziv - 1989 - Some asymptotic properties of the entropy of a stationary ergodic data source with applications to d.pdf;/home/elessar/Zotero/storage/QLHJW7ZG/Wyner and Ziv - 1989 - Some asymptotic properties of the entropy of a stationary ergodic data source with applications to d.pdf}
}

@article{xiaTimeSeriesClassification2024,
  title = {Time {{Series Classification Based}} on {{Forward Echo State Convolution Network}}},
  author = {Xia, Lei and Tang, Jianfeng and Li, Guangli and Fu, Jun and Duan, Shukai and Wang, Lidan},
  year = {2024},
  month = may,
  journal = {Neural Processing Letters},
  volume = {56},
  number = {3},
  pages = {173},
  issn = {1573-773X},
  doi = {10.1007/s11063-024-11449-8},
  urldate = {2025-03-07},
  abstract = {The Echo state network (ESN) is an efficient recurrent neural network that has achieved good results in time series prediction tasks. Still, its application in time series classification tasks has yet to develop fully. In this study, we work on the time series classification problem based on echo state networks. We propose a new framework called forward echo state convolutional network (FESCN). It consists of two parts, the encoder and the decoder, where the encoder part is composed of a forward topology echo state network (FT-ESN), and the decoder part mainly consists of a convolutional layer and a max-pooling layer. We apply the proposed network framework to the univariate time series dataset UCR and compare it with six traditional methods and four neural network models. The experimental findings demonstrate that FESCN outperforms other methods in terms of overall classification accuracy. Additionally, we investigated the impact of reservoir size on network performance and observed that the optimal classification results were obtained when the reservoir size was set to 32. Finally, we investigated the performance of the network under noise interference, and the results show that FESCN has a more stable network performance compared to EMN (echo memory network).},
  langid = {english},
  keywords = {Artificial Intelligence,Echo state network,Forward echo state convolution network,Forward topology echo state network,Time series classification},
  file = {/home/elessar/Zotero/storage/KQ96XYAD/Xia et al. - 2024 - Time Series Classification Based on Forward Echo State Convolution Network.pdf}
}

@article{xieTimeSeriesPrediction2024,
  title = {Time {{Series Prediction}} of {{ESN Based}} on {{Chebyshev Mapping}} and {{Strongly Connected Topology}}},
  author = {Xie, Minzhi and Wang, Qianxue and Yu, Simin},
  year = {2024},
  month = feb,
  journal = {Neural Processing Letters},
  volume = {56},
  number = {1},
  pages = {30},
  issn = {1573-773X},
  doi = {10.1007/s11063-024-11474-7},
  urldate = {2024-11-26},
  abstract = {This paper introduces a novel approach called Chebyshev mapping and strongly connected topology for optimization of echo state network (ESN). To enhance the predictive performance of ESNs for time series data, Chebyshev mapping is employed to optimize the irregular input weight matrix. And the reservoir of the ESN is also replaced using an adjacency matrix derived from a digital chaotic system, resulting in a reservoir with strong connectivity properties. Numerical experiments are conducted on various time series datasets, including the Mackey--Glass time series, Lorenz time series and solar sunspot numbers, validating the effectiveness of the proposed optimization methods. Compared with the traditional ESNs, the optimization method proposed in this paper has higher predictive performance, and effectively reduce the reservoir's size and model complexity.},
  langid = {english},
  keywords = {Artificial Intelligence,Chebyshev mapping,Echo state network,Strongly connected topology,Time series predicting},
  file = {/home/elessar/Zotero/storage/SG78REB3/Xie et al. - 2024 - Time Series Prediction of ESN Based on Chebyshev Mapping and Strongly Connected Topology.pdf}
}

@article{xuAdaptiveElasticEcho2016,
  title = {Adaptive {{Elastic Echo State Network}} for {{Multivariate Time Series Prediction}}},
  author = {Xu, Meiling and Han, Min},
  year = {2016},
  month = oct,
  journal = {IEEE Transactions on Cybernetics},
  volume = {46},
  number = {10},
  pages = {2173--2183},
  issn = {2168-2267, 2168-2275},
  doi = {10.1109/TCYB.2015.2467167},
  urldate = {2025-03-07},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  annotation = {GSCC: 0000129},
  file = {/home/elessar/Zotero/storage/PKPKGNDL/Xu and Han - 2016 - Adaptive Elastic Echo State Network for Multivariate Time Series Prediction.pdf}
}

@article{xueAutomaticTopologyOptimization2023,
  title = {Automatic Topology Optimization of Echo State Network Based on Particle Swarm Optimization},
  author = {Xue, Yu and Zhang, Qi and Slowik, Adam},
  year = {2023},
  month = jan,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {117},
  pages = {105574},
  issn = {09521976},
  doi = {10.1016/j.engappai.2022.105574},
  urldate = {2025-03-12},
  abstract = {The task of time series forecasting is to predict the future trend of data based on the collected historical data, providing theoretical and data support for human judgment and decision making. Randomization-based echo state networks (ESNs) are widely used in the research and application field of time series analysis for their simple structure and fast training speed. The core of the ESN is its dynamic reservoir, which the original reservoir are randomly generated and controlled only by parameter sparsity, often performing poorly on complex tasks and affecting the performance of networks. Manual design of the topology of reservoir is difficult, time-consuming and inconvenient to operate, which is not conducive to the development of ESNs. The construction of a suitable reservoir topology for practical application problems to enrich reservoir dynamics is a hot research point for researchers. In this paper, an automatic optimization method is introduced into the topology optimization of ESN (TP-ESN), and the particle swarm optimization algorithm is used to optimize the topological construction of the ESN. The connection structure between the reservoir neurons is first encoded and then iteratively optimized. The optimized structure is decoded and then the reservoir is initialized for ESN training. Prediction results on Mackey--Glass benchmark time series and two electroencephalogram (EEG) datasets demonstrate that TP-ESN method can have better adaptability, stronger prediction ability and stability than several other manually designed ESN reservoir topologies in the case of relatively complex tasks.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/DGHCJJG4/Xue et al. - 2023 - Automatic topology optimization of echo state network based on particle swarm optimization.pdf}
}

@misc{yamamotoTimeSpaceEfficient2013,
  title = {Time and {{Space Efficient Lempel-Ziv Factorization}} Based on {{Run Length Encoding}}},
  author = {Yamamoto, Jun'ichi and Bannai, Hideo and Inenaga, Shunsuke and Takeda, Masayuki},
  year = {2013},
  month = may,
  number = {arXiv:1204.5524},
  eprint = {1204.5524},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1204.5524},
  urldate = {2025-07-15},
  abstract = {We propose a new approach for calculating the Lempel-Ziv factorization of a text efficiently, based on run length encoding (RLE). We present a conceptually simple off-line algorithm based on a variant of suffix arrays, as well as an on-line algorithm based on a variant of directed acyclic word graphs (DAWGs). Both algorithms run in O(N + n log n) time and O(n) extra space, where N is the size of the text, n {$\leq$} N is the number of RLE factors. The time dependency on N is only in the conversion of the text to RLE, which can be computed very efficiently in O(N ) time and O(1) extra space. When the text is compressible via RLE, i.e., n = o(N ), our algorithms are, to the best of our knowledge, the first algorithms which require only o(N ) extra space while running in o(N log N ) time.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/home/elessar/Zotero/storage/6VVDHBPR/Yamamoto et al. - 2013 - Time and Space Efficient Lempel-Ziv Factorization based on Run Length Encoding.pdf}
}

@article{yamanoInformationTheoryBased2001,
  title = {Information Theory Based on Nonadditive Information Content},
  author = {Yamano, Takuya},
  year = {2001},
  month = mar,
  journal = {Physical Review E},
  volume = {63},
  number = {4},
  publisher = {American Physical Society (APS)},
  issn = {1063-651X, 1095-3787},
  doi = {10.1103/physreve.63.046105},
  urldate = {2025-07-25},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  langid = {english},
  file = {/home/elessar/Zotero/storage/T3DSNNYU/Yamano - 2001 - Information theory based on nonadditive information content.pdf}
}

@article{yangDesignPolynomialEcho2018,
  title = {Design of Polynomial Echo State Networks for Time Series Prediction},
  author = {Yang, Cuili},
  year = {2018},
  abstract = {Echo state networks (ESNs) have been widely used in the field of time series prediction. In conventional ESNs, the spectral radius of reservoir is always scaled to lower than 1 to satisfy the necessary condition for echo state property (ESP), while the sufficient condition is unregarded. Meanwhile, the output weights are always trained without considering the high order statistics of input signals. To solve above problems, the original ESN is extended to polynomial ESNs (PESNs) by employing the polynomial functions of input variables into output weights. Firstly, the reservoir of the PESN is built by the singular value decomposition (SVD) method. Secondly, the prime PESN (P-PESN) is implemented and its polynomial output weights are augmented by the high order statistics of inputs. Thirdly, the simplified PESN (S-PESN) is constructed by decomposing the polynomial output weights of the P-PESN into randomly generated polynomial nodes and tuned output weights. Furthermore, the regression matrix properties of P-PESN and S-PESN are theoretically analyzed, respectively. Finally, the simulation results show that the proposed PESNs obtain better performance than other methods in terms of prediction accuracy and learning speed.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/5LYRBG7B/Yang - 2018 - Design of polynomial echo state networks for time series prediction.pdf}
}

@article{yangRobustEchoState2022,
  title = {Robust Echo State Network with Sparse Online Learning},
  author = {Yang, Cuili and Nie, Kaizhe and Qiao, Junfei and Wang, Danlei},
  year = {2022},
  month = may,
  journal = {Information Sciences},
  volume = {594},
  pages = {95--117},
  issn = {00200255},
  doi = {10.1016/j.ins.2022.02.009},
  urldate = {2025-03-20},
  abstract = {Echo state network (ESN) is an effective tool for nonlinear systems modeling. To handle irregular noises or outliers in practical systems and alleviate the overfitting issue, the robust echo state network with sparse online learning (RESN-SOL) is proposed. Firstly, the e-insensitive loss function is introduced to replace the commonly used quadratic loss function, which is theoretically optimal for Gaussian noise distribution. Secondly, the online gradient descent algorithm is used to calculate the network readout. Notably, the better learning performance can be achieved by the constant learning rate rather than the decreasing step size. Based on this observation, the sparse online learning algorithm (SOL) is proposed, in which the constant step size is used. Particularly, the SOL is able to truncate the small weights in network readout to zero for achieving sparsity. Furthermore, the convergence of RESN-SOL is theoretically analyzed, which implies the tradeoff between learning performance and readout sparsity can be controlled by a predefined sparsity parameter. Finally, the proposed method is verified in two simulated benchmarks and an actual dynamical wastewater treatment system. Experimental results demonstrate that the RESN-SOL exhibits better robustness against outliers, network compactness and modeling accuracy than other existing algorithms.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/JLTAZ87M/Yang et al. - 2022 - Robust echo state network with sparse online learning.pdf}
}

@article{yaoFractionalOrderEcho2020,
  title = {Fractional {{Order Echo State Network}} for {{Time Series Prediction}}},
  author = {Yao, Xianshuang and Wang, Zhanshan},
  year = {2020},
  month = aug,
  journal = {Neural Processing Letters},
  volume = {52},
  number = {1},
  pages = {603--614},
  issn = {1573-773X},
  doi = {10.1007/s11063-020-10267-y},
  urldate = {2025-03-12},
  abstract = {In this brief, considering the infinite memory of fractional-order differential equation, a fractional-order echo state network (FESN) is given for time series prediction. For the FESN, the reservoir state differential equation is replaced with fractional-order differential equation. According to the advantages of FESN, the dynamic characteristics of a class of time series can be fully reflected. In order to improve the prediction performance of FESN, a fractional-order output weights learning method and a fractional-order parameter optimization method are given to train the output weights and optimize the reservoir parameters, respectively. Finally, two numerical examples are used to show the effectiveness of FESN.},
  langid = {english},
  keywords = {Artificial Intelligence,Echo state network,Fractional order,Parameter optimization,Time series prediction},
  file = {/home/elessar/Zotero/storage/ETV97JG8/Yao and Wang - 2020 - Fractional Order Echo State Network for Time Series Prediction.pdf}
}

@inproceedings{yinReservoirComputingEnsembles2012,
  title = {Reservoir Computing Ensembles for Multi-Object Behavior Recognition},
  booktitle = {The 2012 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Yin, Jun and Meng, Yan},
  year = {2012},
  month = jun,
  pages = {1--8},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2012.6252531},
  urldate = {2024-11-26},
  abstract = {Most available methods in computer vision can only detect one behavior at a time in a video sequence. Multi-object behavior recognition is still a very challenge problem. In this paper, we propose a novel model based on reservoir computing ensembles for multi-pattern recognition. In this new model, multiple interactive sub-reservoirs are connected to construct the reservoir model. The sub-reservoirs are competing with each other through inhibitory connections, and the internal states of all the sub-reservoirs are combined to form the output action potentials. Neurobiological studies have shown that cortical neural networks have a distinctive modular and laminar structure, which can provide powerful computational function. Therefore, cortical neural networks are employed to construct each sub-reservoir, whose parameters can be dynamically tuned by a gene regulatory network (GRN). Extensive experimental results on the MSR action dataset II have demonstrated the feasibility and efficiency of the proposed reservoir computing ensembles model on multi-object behavior recognition in video sequences.},
  keywords = {Biological neural networks,Biological system modeling,Computational modeling,cortical template,gene regulatory network,Humans,Mathematical model,multiple patterns,Neurons,reservoir computing,reservoir ensembles,Reservoirs}
}

@article{youngStoneWeierstrassTheorem,
  title = {The {{Stone-Weierstrass Theorem}}},
  author = {Young, Matt},
  langid = {english},
  annotation = {GSCC: 0000225},
  file = {/home/elessar/Zotero/storage/C9E7HHB6/Nature ESN.pdf;/home/elessar/Zotero/storage/QILC4E9X/Young - The Stone-Weierstrass Theorem.pdf}
}

@article{yuKolmogorovComplexityRandom2004,
  title = {The {{Kolmogorov}} Complexity of Random Reals},
  author = {Yu, Liang and Ding, Decheng and Downey, Rodney},
  year = {2004},
  month = oct,
  journal = {Annals of Pure and Applied Logic},
  volume = {129},
  number = {1-3},
  pages = {163--180},
  publisher = {Elsevier BV},
  issn = {0168-0072},
  doi = {10.1016/j.apal.2004.01.006},
  urldate = {2025-07-25},
  abstract = {We investigate the initial segment complexity of random reals. Let K( ) denote pre4x-free Kolmogorov complexity. A natural measure of the relative randomness of two reals and is to compare complexity K( n) and K( n). It is well-known that a real is 1-randomi7 there is a constant c such that for all n, K( n) {\textquestiondown} n - c. We ask the question, what else can be said about the initial segment complexity of random reals. Thus, we study the 4ne behaviour of K( n) for random . Following work of Downey, Hirschfeldt and LaForte, we say that 6K i7 there is a constant O(1) such that for all n, K( n) 6 K( n) + O(1). We call the equivalence classes under this measure of relative randomness K-degrees. We give proofs that there is a randomreal so that limsup n K( n) - K( n) = {$\infty$} where is Chaitin's randomreal. One is based upon (unpublished) work of Solovay, and the other exploits a new idea. Further, based on this new idea, we prove there are uncountably many K-degrees of random reals by proving that (\{ : 6K \}) = 0. As a corollary to the proof we can prove there is no largest K-degree. Finally we prove that if n = m then the initial segment complexities of the natural n- and m-randomsets (namely ∅(n-1) and ∅(m-1)) are di7erent. The techniques introduced in this paper have already found a number of other applications.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/I69FPNDR/Yu et al. - 2004 - The Kolmogorov complexity of random reals.pdf}
}

@article{yusoffModelingNeuralPlasticity2016,
  title = {Modeling Neural Plasticity in Echo State Networks for Classification and Regression},
  author = {Yusoff, Mohd-Hanif and {Chrol-Cannon}, Joseph and Jin, Yaochu},
  year = {2016},
  month = oct,
  journal = {Information Sciences},
  volume = {364--365},
  pages = {184--196},
  issn = {00200255},
  doi = {10.1016/j.ins.2015.11.017},
  urldate = {2025-03-12},
  abstract = {Echo state networks (ESNs) are one of two major neural network models belonging to the reservoir computing framework. Traditionally, only the weights connecting to the output neuron, termed read-out weights, are trained using a supervised learning algorithm, while the weights inside the reservoir of the ESN are randomly determined and remain unchanged during the training. In this paper, we investigate the influence of neural plasticity applied to the weights inside the reservoir on the learning performance of the ESN. We examine the influence of two plasticity rules, anti-Oja's learning rule and the Bienenstock--Cooper--Munro (BCM) learning rule on the prediction and classification performance when either offline or online supervised learning algorithms are employed for training the read-out connections. Empirical studies are conducted on two widely used classification tasks and two time series prediction problems. Our experimental results demonstrate that neural plasticity can more effectively enhance the learning performance when offline learning is applied. The results also indicate that the BCM rule outperforms the anti-Oja rule in improving the learning performance of the ENS in the offline learning mode.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/8CJ4AYSD/Yusoff et al. - 2016 - Modeling neural plasticity in echo state networks for classification and regression.pdf}
}

@article{zaninCanDeepLearning2022,
  title = {Can {{Deep Learning}} Distinguish Chaos from Noise? {{Numerical}} Experiments and General Considerations},
  shorttitle = {Can {{Deep Learning}} Distinguish Chaos from Noise?},
  author = {Zanin, Massimiliano},
  year = {2022},
  month = nov,
  journal = {Communications in Nonlinear Science and Numerical Simulation},
  volume = {114},
  pages = {106708},
  issn = {10075704},
  doi = {10.1016/j.cnsns.2022.106708},
  urldate = {2024-04-30},
  abstract = {Within the larger field of real-world time series analysis, one of the most important tasks is the assessment of their stochastic vs. chaotic nature, and not surprisingly, many metrics and algorithms have been proposed to this end. A still under-explored option is offered by Deep Learning, i.e. a family of machine learning algorithms that perform automatic feature extraction and (usually supervised) classification. We here propose a series of numerical experiments aimed at assessing the performance of different Deep Learning models in discriminating between stochastic and chaotic time series generated by discrete maps, and at comparing such performance with that of standard metrics in the literature. Deep Learning clearly outperforms other alternatives, both in terms of minimum time series length and resilience to observational noise, and can be used to define a new gold standard against which old and new methods can be compared. At the same time, we explore more general considerations about the use of Deep Learning, including whether such models are able to detect general chaoticity fingerprints, or only patterns associated to specific chaotic maps; and what steps ought to be taken to make Deep Learning models a feasible instrument.},
  langid = {english},
  annotation = {GSCC: 0000038},
  file = {/home/elessar/Zotero/storage/63JMWXJZ/Zanin - 2022 - Can Deep Learning distinguish chaos from noise Nu.pdf}
}

@article{zavalaniFourierSpectralMethods2014,
  title = {Fourier {{Spectral Methods}} for {{Numerical Solving}} of the {{Kuramoto-Sivashinsky Equation}}},
  author = {Zavalani, Gentian},
  year = {2014},
  abstract = {In this paper I presented a numerical technique for solving Kuramoto-Sivashinsky equation, based on spectral Fourier methods. This equation describes reaction diffusion problems, and the dynamics of viscous-fuid films flowing along walls. After we wrote the equation in Furie space, we get a system. In this case, the exponential time differencing methods integrate the system much more accurately than other methods since the exponential time differencing methods assume in their derivation that the solution varies slowly in time. When evaluating the coefficients of the exponential time differencing and the exponential time differencing Runge Kutta methods via the''Cauchy integral''. All computational work is done with Matlab package.},
  langid = {english},
  annotation = {GSCC: 0000004},
  file = {/home/elessar/Zotero/storage/G9DTXKZ6/Zavalani - 2014 - Fourier Spectral Methods for Numerical Solving of .pdf}
}

@misc{zengAreTransformersEffective2022,
  title = {Are {{Transformers Effective}} for {{Time Series Forecasting}}?},
  author = {Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  year = {2022},
  month = aug,
  number = {arXiv:2205.13504},
  eprint = {2205.13504},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-30},
  abstract = {Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the permutation-invariant self-attention mechanism inevitably results in temporal information loss.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {GSCC: 0001152},
  file = {/home/elessar/Zotero/storage/HNS9SVYF/Zeng et al. - 2022 - Are Transformers Effective for Time Series Forecas.pdf}
}

@article{zhangApplicationESNPrediction2021,
  title = {Application of {{ESN}} Prediction Model Based on Compressed Sensing in Stock Market},
  author = {Zhang, Hao and Zheng, Mingwen and Zhang, Yanping and Yu, Xiao and Li, Wenchao and Gao, Hui},
  year = {2021},
  month = oct,
  journal = {Communications in Nonlinear Science and Numerical Simulation},
  volume = {101},
  pages = {105857},
  issn = {10075704},
  doi = {10.1016/j.cnsns.2021.105857},
  urldate = {2023-05-30},
  langid = {english},
  annotation = {GSCC: 0000016},
  file = {/home/elessar/Zotero/storage/RGXI6LAH/Zhang et al. - 2021 - Application of ESN prediction model based on compr.pdf}
}

@article{zhangCatch22sReservoirComputing2023,
  title = {Catch-22s of Reservoir Computing},
  author = {Zhang, Yuanzhao and Cornelius, Sean P.},
  year = {2023},
  month = sep,
  journal = {Physical Review Research},
  volume = {5},
  number = {3},
  pages = {033213},
  issn = {2643-1564},
  doi = {10.1103/PhysRevResearch.5.033213},
  urldate = {2024-01-10},
  langid = {english},
  annotation = {GSCC: 0000015},
  file = {/home/elessar/Zotero/storage/R4LLSEFX/Zhang and Cornelius - 2023 - Catch-22s of reservoir computing.pdf}
}

@article{zhangLearningHamiltonianDynamics2021,
  title = {Learning {{Hamiltonian}} Dynamics by Reservoir Computer},
  author = {Zhang, Han and Fan, Huawei and Wang, Liang and Wang, Xingang},
  year = {2021},
  month = aug,
  journal = {Physical Review E},
  volume = {104},
  number = {2},
  eprint = {2104.14474},
  primaryclass = {nlin},
  pages = {024205},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.104.024205},
  urldate = {2023-05-30},
  abstract = {Reconstructing the KAM dynamics diagram of Hamiltonian system from the time series of a limited number of parameters is an outstanding question in nonlinear science, especially when the Hamiltonian governing the system dynamics are unknown. Here, we demonstrate that this question can be addressed by the machine learning approach knowing as reservoir computer (RC). Specifically, we show that without prior knowledge about the Hamilton's equations of motion, the trained RC is able to not only predict the short-term evolution of the system state, but also replicate the long-term ergodic properties of the system dynamics. Furthermore, by the architecture of parameter-aware RC, we also show that the RC trained by the time series acquired at a handful parameters is able to reconstruct the entire KAM dynamics diagram with a high precision by tuning a control parameter externally. The feasibility and efficiency of the learning techniques are demonstrated in two classical nonlinear Hamiltonian systems, namely the double-pendulum oscillator and the standard map. Our study indicates that, as a complex dynamical system, RC is able to learn from data the Hamiltonian.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Nonlinear Sciences - Chaotic Dynamics},
  annotation = {GSCC: 0000024},
  file = {/home/elessar/Zotero/storage/64VNFLVN/Zhang et al. - 2021 - Learning Hamiltonian dynamics by reservoir compute.pdf}
}

@article{zhangOptimalEchoState2022,
  title = {Optimal Echo State Network Parameters Based on Behavioural Spaces},
  author = {Zhang, ZhaoZhao and Zhu, YingQin and Wang, XiaoHui and Yu, Wen},
  year = {2022},
  month = sep,
  journal = {Neurocomputing},
  volume = {503},
  pages = {299--313},
  issn = {09252312},
  doi = {10.1016/j.neucom.2022.06.008},
  urldate = {2025-03-12},
  abstract = {The quantitative analysis of neural networks is a critical issue to improve performance. In this work, we extend the ideas of this adaptation method to the more commonly used behavioural spaces. The aim is to construct an echo state network (ESN) behavioural space through the generalization rank, kernel rank, and memory capacity. After deriving behavioural spaces, we investigate the influencing factors and methods of reducing the search complexity of behavioural spaces. This investigation reveals that the rule converges to the expected space distributions, even in a random ESN. We propose an optimization algorithm that adopts the novel search genetic algorithm (NSGA), which combines the novelty and quality of individuals. In the novel search process, because the time needed to construct the behaviour space is much lower than the network training time, the network optimization efficiency is greatly improved. According to the characteristics of the behaviour space distribution, we propose a method to shrink the search space to solve the problem of a large search space. This method overcomes the difficulties in traditional ESN parameter selection and the long optimization time of a genetic algorithm and provides behavioural space theory to clarify the influence of reservoir performance on tasks.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/H4BH7628/Zhang et al. - 2022 - Optimal echo state network parameters based on behavioural spaces.pdf}
}

@article{zhengResearchTimeSeries2024,
  title = {Research on Time Series Prediction of Multi-Process Based on Deep Learning},
  author = {Zheng, Huali and Cao, Yu and Sun, Dong and Wang, Mingjun and Yan, Binglong and Ye, Chunming},
  year = {2024},
  month = feb,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {3739},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-53762-1},
  urldate = {2024-04-30},
  abstract = {Abstract             Aiming at the problem of data fluctuation in multi-process production, a Soft Update Dueling Double Deep Q-learning (SU-D3QN) network combined with soft update strategy is proposed. Based on this, a time series combination forecasting model SU-D3QN-G is proposed. Firstly, based on production data, Gate Recurrent Unit (GRU) is used for prediction. Secondly, based on the model, SU-D3QN algorithm is used to learn and add bias to it, and the prediction results of GRU are corrected, so that the prediction value of each time node fits in the direction of reducing the absolute error. Thirdly, experiments were carried out on the dataset of a company. The data sets of four indicators, namely, the outlet temperature of drying silk, the loose moisture return water, the outlet temperature of feeding leaves and the inlet water of leaf silk warming and humidification, are selected, and more than 1000 real production data are divided into training set, inspection set and test set according to the ratio of 6:2:2. The experimental results show that the SU-D3QN-G combined time series prediction model has a great improvement compared with GRU, LSTM and ARIMA, and the MSE index is reduced by 0.846--23.930\%, 5.132--36.920\% and 10.606--70.714\%, respectively. The RMSE index is reduced by 0.605--10.118\%, 2.484--14.542\% and 5.314--30.659\%. The MAE index is reduced by 3.078--15.678\%, 7.94--15.974\% and 6.860--49.820\%. The MAPE index is reduced by 3.098--15.700\%, 7.98--16.395\% and 7.143--50.000\%.},
  langid = {english},
  annotation = {GSCC: 0000001},
  file = {/home/elessar/Zotero/storage/UMIG4N2G/Zheng et al. - 2024 - Research on time series prediction of multi-proces.pdf}
}

@article{zhongGeneticAlgorithmOptimized2017,
  title = {Genetic Algorithm Optimized Double-Reservoir Echo State Network for Multi-Regime Time Series Prediction},
  author = {Zhong, Shisheng and Xie, Xiaolong and Lin, Lin and Wang, Fang},
  year = {2017},
  month = may,
  journal = {Neurocomputing},
  volume = {238},
  pages = {191--204},
  issn = {09252312},
  doi = {10.1016/j.neucom.2017.01.053},
  urldate = {2025-03-20},
  abstract = {In prognostics and health management (PHM), the sensor measurement time series of equipment is collected, and predicting future sensor measurements accurately is crucial to PHM. Complex equipment is generally operated under dynamic operational conditions; thus, operational regime-switching process exists in the sensor measurement time series, which is called multi-regime time series. Different operational regimes may have various effects on time series; thus, the regime-switching process poses great challenge for multi-regime time series prediction. To predict the multi-regime time series accurately, the double-reservoir echo state network (DRESN) is adopted by modifying the conventional echo state network. The DRESN model has two input sequences: the sensor measurement sequence and regime parameter sequence, where the regime parameter reflects the operational regimes and influences sensor measurement; then, two reservoirs try to model these two sequences, respectively; last, the outputs of two reservoirs are aggregated to predict the future sensor measurement. The DRESN model not only considers previous sensor measurements but also takes the influence of regime parameters into account when predicting future sensor measurement; thus, it can improve the accuracy of multi-regime time series prediction. In addition, the training algorithm of the DRESN model is presented and only a linear regression problem needs to be solved, making the DRESN model efficient. To achieve good performance, four parameters of the DRESN model are optimized using genetic algorithm (GA) because GA is effective in solving mixed-integer problem, and the weighted cross validation is adopted in the objective function to achieve the accuracy and simplicity simultaneously. The DRESN model is applied to turbofan engine multi-regime time series and compared with other models. The results validate that the DRESN model can be accurate and stable in multi-regime time series prediction.},
  langid = {english},
  file = {/home/elessar/Zotero/storage/847UKGJU/Zhong et al. - 2017 - Genetic algorithm optimized double-reservoir echo state network for multi-regime time series predict.pdf}
}

@article{zivEstimatingNumberStates1992,
  title = {Estimating the Number of States of a Finite-State Source},
  author = {Ziv, J. and Merhav, N.},
  year = {1992},
  journal = {IEEE Transactions on Information Theory},
  volume = {38},
  number = {1},
  pages = {61--65},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {0018-9448},
  doi = {10.1109/18.108249},
  urldate = {2025-07-25},
  abstract = {The problem of estimating the number of states of a finite-alphabet, finite-state source is investigated. An estimator is developed that asymptotically attains the minimum probability of underestimating the number of states, among all estimators with a prescribed exponential decay rate of overestimation probability. The proposed estimator relies on the Lempel-Ziv data compression algorithm in an intuitively appealing manner. Index Terms-Model order estimation, finite-state sources, hidden Markov models, universal data compression, Lempel-Ziv algorithm.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/elessar/Zotero/storage/DRXDXYXS/Ziv and Merhav - 1992 - Estimating the number of states of a finite-state source.pdf}
}

@article{zozorLempelZivComplexity2005,
  title = {On {{Lempel}}--{{Ziv}} Complexity for Multidimensional Data Analysis},
  author = {Zozor, S. and Ravier, P. and Buttelli, O.},
  year = {2005},
  month = jan,
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {345},
  number = {1-2},
  pages = {285--302},
  publisher = {Elsevier BV},
  issn = {0378-4371},
  doi = {10.1016/j.physa.2004.07.025},
  urldate = {2025-07-15},
  abstract = {In this paper, a natural extension of the Lempel--Ziv complexity for several finite-time sequences, defined on finite size alphabets is proposed. Some results on the defined joint Lempel--Ziv complexity are given, as well as properties in connection with the Lempel--Ziv complexity of the individual sequences. Also, some links with Shannon entropies are exhibited and, by analogy, some derived quantities are proposed. Lastly, the potential use of the extended complexities for data analysis is illustrated on random boolean networks and on a proposed multidimensional extension of the minority game.},
  copyright = {http://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/home/elessar/Zotero/storage/2JWGHP52/On Lempel-Ziv complexity for multidimensional data analysis.pdf;/home/elessar/Zotero/storage/4EG9394I/Zozor et al. - 2005 - On Lempel–Ziv complexity for multidimensional data analysis.pdf}
}

@article{zvonkinCOMPLEXITYFINITEOBJECTS1970,
  title = {{{THE COMPLEXITY OF FINITE OBJECTS AND THE DEVELOPMENT OF THE CONCEPTS OF INFORMATION AND RANDOMNESS BY MEANS OF THE THEORY OF ALGORITHMS}}},
  author = {Zvonkin, A K and Levin, L A},
  year = {1970},
  month = dec,
  journal = {Russian Mathematical Surveys},
  volume = {25},
  number = {6},
  pages = {83--124},
  publisher = {Steklov Mathematical Institute},
  issn = {0036-0279, 1468-4829},
  doi = {10.1070/rm1970v025n06abeh001269},
  urldate = {2025-07-25},
  langid = {english},
  file = {/home/elessar/Zotero/storage/DWBZF4D7/Zvonkin and Levin - 1970 - THE COMPLEXITY OF FINITE OBJECTS AND THE DEVELOPMENT OF THE CONCEPTS OF INFORMATION AND RANDOMNESS B.pdf}
}
